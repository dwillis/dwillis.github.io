[
  {
    "objectID": "aggregates.html",
    "href": "aggregates.html",
    "title": "14  Aggregates",
    "section": "",
    "text": "R is a statistical programming language that is purpose built for data analysis.\nBase R does a lot, but there are a mountain of external libraries that do things to make R better/easier/more fully featured. We already installed the tidyverse – or you should have if you followed the instructions for the last assignment – which isn’t exactly a library, but a collection of libraries. Together, they make up the Tidyverse. Individually, they are extraordinarily useful for what they do. We can load them all at once using the tidyverse name, or we can load them individually. Let’s start with individually.\nThe two libraries we are going to need for this assignment are readr and dplyr. The library readr reads different types of data in. For this assignment, we’re going to read in csv data or Comma Separated Values data. That’s data that has a comma between each column of data.\nThen we’re going to use dplyr to analyze it.\nTo use a library, you need to import it. Good practice – one I’m going to insist on – is that you put all your library steps at the top of your notebooks.\nThat code looks like this:\n\nlibrary(readr)\n\nTo load them both, you need to do this:\n\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nBut, because those two libraries – and several others that we’re going to use over the course of this class – are so commonly used, there’s a shortcut to loading all of the libraries we’ll need:\n\nlibrary(tidyverse)\n\nYou can keep doing that for as many libraries as you need."
  },
  {
    "objectID": "aggregates.html#importing-data",
    "href": "aggregates.html#importing-data",
    "title": "14  Aggregates",
    "section": "14.2 Importing data",
    "text": "14.2 Importing data\nThe first thing we need to do is get some data to work with. We do that by reading it in. In our case, we’re going to read a datatable from an “rds” file, which is a format for storing data with R. Later in the course, we’ll more frequently work with a format called a CSV. A CSV is a stripped down version of a spreadsheet you might open in a program like Excel, in which each column is separated by a comma. RDS files are less common when getting data from other people. But reading in CSVs is less foolproof than reading in rds files, so for now we’ll work with rds.\nThe rds file we’re going to read in contains individual campaign contributions from Maryland donors via WinRed, an online fundraising platform used by conservatives. We’ll be working with a slice of the data from earlier this year.\nSo step 1 is to import the data. The code to import the data looks like this:\nmaryland_winred_contributions <- read_rds(\"maryland_winred.rds\")\nLet’s unpack that.\nThe first part – maryland_winred_contributions – is the name of a variable.\nA variable is just a name that we’ll use to refer to some more complex thing. In this case, the more complex thing is the data we’re importing into R that will be stored as a dataframe, which is one way R stores data.\nWe can call this variable whatever we want. The variable name doesn’t matter, technically. We could use any word. You could use your first name, if you like. Generally, though, we want to give variables names that are descriptive of the thing they refer to. Which is why we’re calling this one maryland_winred_contributions. Variable names, by convention are one word all lower case (or two or more words connected by an underscore). You can end a variable with a number, but you can’t start one with a number.\nThe <- bit, you’ll recall from the basics, is the variable assignment operator. It’s how we know we’re assigning something to a word. Think of the arrow as saying “Take everything on the right of this arrow and stuff it into the thing on the left.” So we’re creating an empty vessel called maryland_winred_contributions and stuffing all this data into it.\nread_rds() is a function, one that only works when we’ve loaded the tidyverse. A function is a little bit of computer code that takes in information and follows a series of pre-determined steps and spits it back out. A recipe to make pizza is a kind of function. We might call it make_pizza().\nThe function does one thing. It takes a preset collection of ingredients – flour, water, oil, cheese, tomato, salt – and passes them through each step outlined in a recipe, in order. Things like: mix flour and water and oil, knead, let it sit, roll it out, put tomato sauce and cheese on it, bake it in an oven, then take it out.\nThe output of our make pizza() function is a finished pie.\nWe’ll make use of a lot of pre-written functions from the tidyverse and other packages, and even write some of our own. Back to this line of code:\nmaryland_winred_contributions <- read_rds(\"maryland_winred.rds\")\nInside of the read_rds() function, we’ve put the name of the file we want to load. Things we put inside of function, to customize what the function does, are called arguments.\nThe easiest thing to do, if you are confused about how to find your data, is to put your data in the same folder as as your notebook (you’ll have to save that notebook first). If you do that, then you just need to put the name of the file in there (maryland_winred.rds). If you put your data in a folder called “data” that sits next to your data notebook, your function would instead look like this:\n\nmaryland_winred_contributions <- read_rds(\"data/maryland_winred.rds\")\n\nIn this data set, each row represents an individual contribution to a federal political committee, typically a candidate’s campaign account.\nAfter loading the data, it’s a good idea to get a sense of its shape. What does it look like? There are several ways we can examine it.\nBy looking in the R Studio environment window, we can see the number of rows (called “obs.”, which is short for observations), and the number of columns(called variables). We can double click on the dataframe name in the environment window, and explore it like a spreadsheet.\nThere are several useful functions for getting a sense of the dataset right in our markdown document.\nIf we run glimpse(maryland_winred_contributions), it will give us a list of the columns, the data type for each column and and the first few values for each column.\n\nglimpse(maryland_winred_contributions)\n\nRows: 54,247\nColumns: 24\n$ linenumber       <chr> \"SA11AI\", \"SA11AI\", \"SA11AI\", \"SA11AI\", \"SA11AI\", \"SA…\n$ fec_committee_id <chr> \"C00694323\", \"C00694323\", \"C00694323\", \"C00694323\", \"…\n$ tran_id          <chr> \"A9EF36684A71C4FF097A\", \"A64BB5021801F42B386B\", \"A583…\n$ flag_orgind      <chr> \"IND\", \"IND\", \"IND\", \"IND\", \"IND\", \"IND\", \"IND\", \"IND…\n$ org_name         <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ last_name        <chr> \"Hansher\", \"Leishman\", \"Bode\", \"Gallagher\", \"Bode\", \"…\n$ first_name       <chr> \"Beth\", \"Alexander\", \"John\", \"Daniel\", \"Denise\", \"Joe…\n$ middle_name      <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ prefix           <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ suffix           <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ address_one      <chr> \"7205 Arrowood Rd\", \"1021 Gadsden Ave\", \"29389 Catalp…\n$ address_two      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ city             <chr> \"Bethesda\", \"Silver Spring\", \"Easton\", \"Towson\", \"Eas…\n$ state            <chr> \"MD\", \"MD\", \"MD\", \"MD\", \"MD\", \"MD\", \"MD\", \"MD\", \"MD\",…\n$ zip              <dbl> 20817, 20905, 21601, 21204, 21601, 20815, 21502, 2081…\n$ prigen           <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ date             <date> 2022-06-01, 2022-05-25, 2022-06-16, 2022-04-05, 2022…\n$ amount           <dbl> 5800.00, 5800.00, 5800.00, 5800.00, 5800.00, 5800.00,…\n$ aggregate_amount <dbl> 5800.00, 22300.00, 11600.00, 17900.00, 17650.00, 1450…\n$ employer         <chr> \"INFORMATION REQUESTED\", \"RIVER\", \"CORN REFINERS ASSO…\n$ occupation       <chr> \"HOMEMAKER\", \"CEO\", \"ASSOCIATION EXECUTIVE\", \"CLO\", \"…\n$ memo_code        <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ memo_text        <chr> \"Earmarked for BLAKE MASTERS FOR SENATE (C00784165)\",…\n$ cycle            <dbl> 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022,…\n\n\nIf we type head(maryland_winred_contributions), it will print out the columns and the first six rows of data.\n\nhead(maryland_winred_contributions)\n\n# A tibble: 6 × 24\n  linenu…¹ fec_c…² tran_id flag_…³ org_n…⁴ last_…⁵ first…⁶ middl…⁷ prefix suffix\n  <chr>    <chr>   <chr>   <chr>   <lgl>   <chr>   <chr>   <lgl>   <lgl>  <lgl> \n1 SA11AI   C00694… A9EF36… IND     NA      Hansher Beth    NA      NA     NA    \n2 SA11AI   C00694… A64BB5… IND     NA      Leishm… Alexan… NA      NA     NA    \n3 SA11AI   C00694… A5835B… IND     NA      Bode    John    NA      NA     NA    \n4 SA11AI   C00694… AE3AF0… IND     NA      Gallag… Daniel  NA      NA     NA    \n5 SA11AI   C00694… AC5569… IND     NA      Bode    Denise  NA      NA     NA    \n6 SA11AI   C00694… A2B1EB… IND     NA      Kaplan  Joel    NA      NA     NA    \n# … with 14 more variables: address_one <chr>, address_two <chr>, city <chr>,\n#   state <chr>, zip <dbl>, prigen <lgl>, date <date>, amount <dbl>,\n#   aggregate_amount <dbl>, employer <chr>, occupation <chr>, memo_code <lgl>,\n#   memo_text <chr>, cycle <dbl>, and abbreviated variable names ¹​linenumber,\n#   ²​fec_committee_id, ³​flag_orgind, ⁴​org_name, ⁵​last_name, ⁶​first_name,\n#   ⁷​middle_name\n\n\nWe can also click on the data name in the R Studio environment window to explore it interactively."
  },
  {
    "objectID": "aggregates.html#group-by-and-count",
    "href": "aggregates.html#group-by-and-count",
    "title": "14  Aggregates",
    "section": "14.3 Group by and count",
    "text": "14.3 Group by and count\nSo what if we wanted to know how many contributions went to each recipient?\nTo do that by hand, we’d have to take each of the 54,247 individual rows (or observations or records) and sort them into a pile. We’d put them in groups – one for each recipient – and then count them.\ndplyr has a group by function in it that does just this. A massive amount of data analysis involves grouping like things together and then doing simple things like counting them, or averaging them together. So it’s a good place to start.\nSo to do this, we’ll take our dataset and we’ll introduce a new operator: %>%. The best way to read that operator, in my opinion, is to interpret that as “and then do this.”\nWe’re going to establish a pattern that will come up again and again throughout this book: data %>% function. In English: take your data set and then do this specific action to it.\nThe first step of every analysis starts with the data being used. Then we apply functions to the data.\nIn our case, the pattern that you’ll use many, many times is: data %>% group_by(COLUMN NAME) %>% summarize(VARIABLE NAME = AGGREGATE FUNCTION(COLUMN NAME))\nIn our dataset, the column with recipient information is called “memo_text”\nHere’s the code to count the number of contributions to each recipient:\n\nmaryland_winred_contributions %>%\n  group_by(memo_text) %>%\n  summarise(\n    count_contribs = n()\n  )\n\n# A tibble: 519 × 2\n   memo_text                                           count_contribs\n   <chr>                                                        <int>\n 1 Earmarked for ABRAHAM LINCOLN PAC (C00631051)                    3\n 2 Earmarked for ADRIAN SMITH FOR CONGRESS (C00412890)              2\n 3 Earmarked for AL FOR MONTANA (C00783381)                         1\n 4 Earmarked for ALAMO PAC (C00387464)                              1\n 5 Earmarked for ALAN SIMS FOR CONGRESS (C00784975)                 2\n 6 Earmarked for ALASKANS FOR DAN SULLIVAN (C00570994)              5\n 7 Earmarked for ALASKANS FOR NICK BEGICH (C00792341)               1\n 8 Earmarked for ALEK FOR OREGON (C00715854)                      121\n 9 Earmarked for ALEX FOR CONGRESS (C00771279)                      3\n10 Earmarked for ALLCORN FOR COLORADO (C00798983)                   2\n# … with 509 more rows\n\n\nSo let’s walk through that.\nWe start with our dataset – maryland_winred_contributions – and then we tell it to group the data by a given field in the data. In this case, we wanted to group together all the recipients, signified by the field name memo_text, which you could get from using the glimpse() function. After we group the data, we need to count them up.\nIn dplyr, we use the summarize() function, which can do alot more than just count things.\nInside the parentheses in summarize, we set up the summaries we want. In this case, we just want a count of the number of loans for each county grouping. The line of code count_contribs = n(), says create a new field, called count_contribs and set it equal to n(). n() is a function that counts the number of rows or records in each group. Why the letter n? The letter n is a common symbol used to denote a count of something. The number of things (or rows or observations or records) in a dataset? Statisticians call it n. There are n number of contributions in this dataset.\nWhen we run that, we get a list of recipients with a count next to them. But it’s not in any order.\nSo we’ll add another “and then do this” symbol – %>% – and use a new function called arrange(). Arrange does what you think it does – it arranges data in order. By default, it’s in ascending order – smallest to largest. But if we want to know the county with the most loans, we need to sort it in descending order. That looks like this:\n\nmaryland_winred_contributions %>%\n  group_by(memo_text) %>%\n  summarise(\n    count_contribs = n()\n  ) %>%\n  arrange(desc(count_contribs))\n\n# A tibble: 519 × 2\n   memo_text                                                          count_co…¹\n   <chr>                                                                   <int>\n 1 Earmarked for SAVE AMERICA JOINT FUNDRAISING COMMITTEE (C00770941)       7809\n 2 Earmarked for NRSC (C00027466)                                           6497\n 3 Earmarked for REPUBLICAN NATIONAL COMMITTEE (C00003418)                  4268\n 4 Earmarked for NRCC (C00075820)                                           3945\n 5 Earmarked for TEAM SCALISE (C00750521)                                   2477\n 6 Earmarked for KEVIN MCCARTHY FOR CONGRESS (C00420935)                    1461\n 7 Earmarked for MARCO RUBIO FOR SENATE (C00620518)                         1326\n 8 Earmarked for RON JOHNSON FOR SENATE, INC. (C00482984)                    974\n 9 Earmarked for JOHN KENNEDY FOR US (C00608398)                             955\n10 Earmarked for TIM SCOTT FOR SENATE (C00540302)                            915\n# … with 509 more rows, and abbreviated variable name ¹​count_contribs\n\n\nThe Save America Joint Fundraising Committee has 7,809 contributions, more than any other recipient.\nWe can, if we want, group by more than one thing.\nThe WinRed data contains a column detailing the date of the contribution: “date”.\nWe can group by “memo_text” and “date” to see how many contributions occurred on every date to every recipient. We’ll sort by recipient and then date\n\nmaryland_winred_contributions %>%\n  group_by(memo_text, date) %>%\n  summarise(\n    count_contribs = n()\n  ) %>%\n  arrange(memo_text, date)\n\n`summarise()` has grouped output by 'memo_text'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 9,194 × 3\n# Groups:   memo_text [519]\n   memo_text                                           date       count_contribs\n   <chr>                                               <date>              <int>\n 1 Earmarked for ABRAHAM LINCOLN PAC (C00631051)       2022-06-27              1\n 2 Earmarked for ABRAHAM LINCOLN PAC (C00631051)       2022-06-28              1\n 3 Earmarked for ABRAHAM LINCOLN PAC (C00631051)       2022-06-30              1\n 4 Earmarked for ADRIAN SMITH FOR CONGRESS (C00412890) 2022-04-01              1\n 5 Earmarked for ADRIAN SMITH FOR CONGRESS (C00412890) 2022-06-27              1\n 6 Earmarked for AL FOR MONTANA (C00783381)            2022-06-04              1\n 7 Earmarked for ALAMO PAC (C00387464)                 2022-05-16              1\n 8 Earmarked for ALAN SIMS FOR CONGRESS (C00784975)    2022-04-23              1\n 9 Earmarked for ALAN SIMS FOR CONGRESS (C00784975)    2022-05-23              1\n10 Earmarked for ALASKANS FOR DAN SULLIVAN (C00570994) 2022-04-22              1\n# … with 9,184 more rows"
  },
  {
    "objectID": "aggregates.html#other-summarization-methods-summing-mean-median-min-and-max",
    "href": "aggregates.html#other-summarization-methods-summing-mean-median-min-and-max",
    "title": "14  Aggregates",
    "section": "14.4 Other summarization methods: summing, mean, median, min and max",
    "text": "14.4 Other summarization methods: summing, mean, median, min and max\nIn the last example, we grouped like records together and counted them, but there’s so much more we can to summarize each group.\nLet’s say we wanted to know the total dollar amount of contributions to each recipient? For that, we could use the sum() function to add up all of the loan values in the column “amount”. We put the column we want to total – “amount” – inside the sum() function sum(amount). Note that we can simply add a new summarize function here, keeping our count_contribs field in our output table.\n\nmaryland_winred_contributions %>%\n  group_by(memo_text) %>%\n  summarise(\n    count_contribs = n(),\n    total_amount = sum(amount)\n  ) %>%\n  arrange(desc(total_amount))\n\n# A tibble: 519 × 3\n   memo_text                                                     count…¹ total…²\n   <chr>                                                           <int>   <dbl>\n 1 Earmarked for SAVE AMERICA JOINT FUNDRAISING COMMITTEE (C007…    7809 220155.\n 2 Earmarked for NRSC (C00027466)                                   6497 149673.\n 3 Earmarked for NRCC (C00075820)                                   3945 118658.\n 4 Earmarked for REPUBLICAN NATIONAL COMMITTEE (C00003418)          4268 111955.\n 5 Earmarked for NICOLEE AMBROSE FOR CONGRESS (C00812891)            150  67980.\n 6 Earmarked for TEAM SCALISE (C00750521)                           2477  51709.\n 7 Earmarked for PARROTT FOR CONGRESS (C00691931)                    346  45243.\n 8 Earmarked for MARCO RUBIO FOR SENATE (C00620518)                 1326  43279.\n 9 Earmarked for KEVIN MCCARTHY FOR CONGRESS (C00420935)            1461  29612.\n10 Earmarked for RON JOHNSON FOR SENATE, INC. (C00482984)            974  29611.\n# … with 509 more rows, and abbreviated variable names ¹​count_contribs,\n#   ²​total_amount\n\n\nWe can also calculate the average amount for each recipient – the mean – and the amount that sits at the midpoint of our data – the median.\n\nmaryland_winred_contributions %>%\n  group_by(memo_text) %>%\n  summarise(\n    count_contribs = n(),\n    total_amount = sum(amount),\n    mean_amount = mean(amount),\n    median_amount = median(amount)\n  ) %>%\n  arrange(desc(count_contribs))\n\n# A tibble: 519 × 5\n   memo_text                                     count…¹ total…² mean_…³ media…⁴\n   <chr>                                           <int>   <dbl>   <dbl>   <dbl>\n 1 Earmarked for SAVE AMERICA JOINT FUNDRAISING…    7809 220155.    28.2    20  \n 2 Earmarked for NRSC (C00027466)                   6497 149673.    23.0    12.5\n 3 Earmarked for REPUBLICAN NATIONAL COMMITTEE …    4268 111955.    26.2    18  \n 4 Earmarked for NRCC (C00075820)                   3945 118658.    30.1    25  \n 5 Earmarked for TEAM SCALISE (C00750521)           2477  51709.    20.9    10  \n 6 Earmarked for KEVIN MCCARTHY FOR CONGRESS (C…    1461  29612.    20.3    10  \n 7 Earmarked for MARCO RUBIO FOR SENATE (C00620…    1326  43279.    32.6    20.2\n 8 Earmarked for RON JOHNSON FOR SENATE, INC. (…     974  29611.    30.4    25  \n 9 Earmarked for JOHN KENNEDY FOR US (C00608398)     955  26433.    27.7    15  \n10 Earmarked for TIM SCOTT FOR SENATE (C0054030…     915  29095.    31.8    10  \n# … with 509 more rows, and abbreviated variable names ¹​count_contribs,\n#   ²​total_amount, ³​mean_amount, ⁴​median_amount\n\n\nWe see something interesting here. The mean contribution amount is higher than the median amount in most cases, but the difference isn’t huge. In some cases the mean gets skewed by larger amounts. Examining both the median – which is less sensitive to extreme values – and the mean – which is more sensitive to extreme values – gives you a clearer picture of the composition of the data.\nWhat about the highest and lowest amounts for each recipient? For that, we can use the min() and max() functions.\n\nmaryland_winred_contributions %>%\n  group_by(memo_text) %>%\n  summarise(\n    count_contribs = n(),\n    total_amount = sum(amount),\n    mean_amount = mean(amount),\n    median_amount = median(amount),\n    min_amount = min(amount),\n    max_amount = max(amount)\n  ) %>%\n  arrange(desc(max_amount))\n\n# A tibble: 519 × 7\n   memo_text                     count…¹ total…² mean_…³ media…⁴ min_a…⁵ max_a…⁶\n   <chr>                           <int>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 Earmarked for ALASKANS FOR D…       5   7304   1461.      500    1       5800\n 2 Earmarked for AMAL FOR CONGR…      30  10442.   348.       25    1       5800\n 3 Earmarked for BLAKE MASTERS …      24   6780    282.       25   10       5800\n 4 Earmarked for FRIENDS OF MIK…     358  17711.    49.5      10    0.5     5800\n 5 Earmarked for FRIENDS OF TOD…      75  14833.   198.        5    0.2     5800\n 6 Earmarked for GALLUCH FOR CO…       1   5800   5800      5800 5800       5800\n 7 Earmarked for KELLY FOR ALAS…      54   7900    146.       25    5       5800\n 8 Earmarked for LAWLER FOR CON…       1   5800   5800      5800 5800       5800\n 9 Earmarked for LAXALT FOR SEN…     334  26920.    80.6      25    0.77    5800\n10 Earmarked for MCHENRY FOR CO…       3   6800   2267.      500  500       5800\n# … with 509 more rows, and abbreviated variable names ¹​count_contribs,\n#   ²​total_amount, ³​mean_amount, ⁴​median_amount, ⁵​min_amount, ⁶​max_amount\n\n\nFrom this, we can see that some committees focus on small-dollar donors while others ask for (and get) larger amounts. This pattern isn’t random: campaigns make choices about how they will raise money.\nIt would be interesting to see what the largest donation was. To do that, we could simply take our original data set and sort it from highest to lowest on the amount.\n\nmaryland_winred_contributions %>%\n  arrange(desc(amount))\n\n# A tibble: 54,247 × 24\n   linen…¹ fec_c…² tran_id flag_…³ org_n…⁴ last_…⁵ first…⁶ middl…⁷ prefix suffix\n   <chr>   <chr>   <chr>   <chr>   <lgl>   <chr>   <chr>   <lgl>   <lgl>  <lgl> \n 1 SA11AI  C00694… A9EF36… IND     NA      Hansher Beth    NA      NA     NA    \n 2 SA11AI  C00694… A64BB5… IND     NA      Leishm… Alexan… NA      NA     NA    \n 3 SA11AI  C00694… A5835B… IND     NA      Bode    John    NA      NA     NA    \n 4 SA11AI  C00694… AE3AF0… IND     NA      Gallag… Daniel  NA      NA     NA    \n 5 SA11AI  C00694… AC5569… IND     NA      Bode    Denise  NA      NA     NA    \n 6 SA11AI  C00694… A2B1EB… IND     NA      Kaplan  Joel    NA      NA     NA    \n 7 SA11AI  C00694… A2D9CC… IND     NA      Freas   Dustin  NA      NA     NA    \n 8 SA11AI  C00694… A8F4D9… IND     NA      Monahan Tom     NA      NA     NA    \n 9 SA11AI  C00694… A3E262… IND     NA      powers  rick    NA      NA     NA    \n10 SA11AI  C00694… AD5034… IND     NA      JonesL… Brien   NA      NA     NA    \n# … with 54,237 more rows, 14 more variables: address_one <chr>,\n#   address_two <chr>, city <chr>, state <chr>, zip <dbl>, prigen <lgl>,\n#   date <date>, amount <dbl>, aggregate_amount <dbl>, employer <chr>,\n#   occupation <chr>, memo_code <lgl>, memo_text <chr>, cycle <dbl>, and\n#   abbreviated variable names ¹​linenumber, ²​fec_committee_id, ³​flag_orgind,\n#   ⁴​org_name, ⁵​last_name, ⁶​first_name, ⁷​middle_name\n\n\nLots of $5,800 contributions, which probably seems like a weirdly specific number. And it is! That’s the maximum contribution an individual can give for both a primary and a general election."
  },
  {
    "objectID": "textanalysis.html",
    "href": "textanalysis.html",
    "title": "29  An intro to text analysis",
    "section": "",
    "text": "Throughout this course, we’ve been focused on finding information in structured data. We’ve learned a lot of techniques to do that, and we’ve learned how the creative mixing and matching of those skills can find new insights.\nWhat happens when the insights are in unstructured data? Like a block of text?\nTurning unstructured text into data to analyze is a whole course in and of itself – and one worth taking if you’ve got the credit hours – but some simple stuff is in the grasp of basic data analysis.\nTo do this, we’ll need a new library – tidytext, which you can guess by the name plays very nicely with the tidyverse. So install it with install.packages(\"tidytext\") and we’ll get rolling.\nHere’s the question we’re going to go after: How did federal politicians talk about the coronavirus pandemic on Twitter?\nTo answer this question, we’ll use a dataset of tweets posted by federal politicians from both campaign and official accounts that mentioned either “COVID” or “coronavirus” beginning on Feb. 1, 2020. This dataset doesn’t include retweets, only original tweets. Let’s read in this data and examine it:\nWe can see what it looks like with head:\nWhat we want to do is to make the content column easier to analyze. Let’s say we want to find out the most commonly used words. We’ll probably want to remove URLs from the text of the tweets since they aren’t actual words. Let’s use mutate to make that happen:\nIf you are trying to create a list of unique words, R will treat differences in capitalization as unique and also will include punctuation by default, even using its unique function:\nFortunately, this is a solved problem with tidytext, which has a function called unnest_tokens that will convert the text to lowercase and remove all punctuation. The way that unnest_tokens works is that we tell it what we want to call the field we’re creating with this breaking apart, then we tell it what we’re breaking apart – what field has all the text in it. For us, that’s the content column:\nNow we can look at the top words in this dataset. Let’s limit ourselves to making a plot of the top 25 words:\nWell, that’s a bit underwhelming - a lot of very common (and short) words. This also is a solved problem in working with text data, and words like “a” and “the” are known as “stop words”. In most cases you’ll want to remove them from your analysis since they are so common. Tidytext provides a dataframe of them:\nThen we’re going to use a function we haven’t used yet called an anti_join, which filters out any matches. So we’ll anti_join the stop words and get a list of words that aren’t stop words.\nFrom there, we can get a simple word frequency by just grouping them together and counting them. We can borrow the percent code from above to get a percent of the words our top 10 words represent.\nThose seem like more relevant unique words. Now, here’s where we can start to do more interesting and meaningful analysis. Let’s create two dataframes of unique words based on time: one for all of 2020 and the other for all of 2021:\nThen we can create top 10 lists for both of them and compare:\nIn the 2021 top 10 list, “vaccine” and its variations are much more prominent, which makes sense, while “testing” drops out of the top 10 compared to 2020."
  },
  {
    "objectID": "textanalysis.html#going-beyond-a-single-word",
    "href": "textanalysis.html#going-beyond-a-single-word",
    "title": "29  An intro to text analysis",
    "section": "29.1 Going beyond a single word",
    "text": "29.1 Going beyond a single word\nThe next step in text analysis is using ngrams. An ngram is any combination of words that you specify. Two word ngrams are called bigrams (bi-grams). Three would be trigrams. And so forth.\nThe code to make ngrams is similar to what we did above, but involves some more twists.\nSo this block is is going to do the following:\n\nUse the covid_tweets data we created above, and filter for pre-2021 tweets.\nUnnest the tokens again, but instead we’re going to create a field called bigram, break apart summary, but we’re going to specify the tokens in this case are ngrams of 2.\nWe’re going to make things easier to read and split bigrams into word1 and word2.\nWe’re going to filter out stopwords again, but this time we’re going to do it in both word1 and word2 using a slightly different filtering method.\nBecause of some weirdness in calculating the percentage, we’re going to put bigram back together again, now that the stop words are gone.\nWe’ll then group by, count and create a percent just like we did above.\nWe’ll then use top_n to give us the top 10 bigrams.\n\n\ncovid_tweets %>%\n  filter(created < '2021-01-01') %>%\n  unnest_tokens(bigram, content, token = \"ngrams\", n = 2) %>%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") %>%\n  filter(!word1 %in% stop_words$word) %>%\n  filter(!word2 %in% stop_words$word) %>%\n  mutate(bigram = paste(word1, word2, sep=\" \")) %>%\n  group_by(bigram) %>%\n  tally(sort=TRUE) %>%\n  mutate(percent = (n/sum(n))*100) %>%\n  top_n(10)\n\nSelecting by percent\n\n\n# A tibble: 10 × 3\n   bigram                   n percent\n   <chr>                <int>   <dbl>\n 1 covid 19             32302   3.52 \n 2 health care           4388   0.478\n 3 public health         3908   0.426\n 4 covid19 pandemic      3206   0.349\n 5 town hall             3014   0.328\n 6 coronavirus pandemic  2915   0.318\n 7 19 pandemic           2670   0.291\n 8 covid relief          2246   0.245\n 9 relief package        2062   0.225\n10 covid ー              1832   0.200\n\n\nAnd we already have a different, more nuanced result. Health was among the top single words, and we can see that “health care” and “public health” are among the top 2-word phrases. What about after 2021?\n\ncovid_tweets %>%\n  filter(created >= '2021-01-01') %>%\n  unnest_tokens(bigram, content, token = \"ngrams\", n = 2) %>%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") %>%\n  filter(!word1 %in% stop_words$word) %>%\n  filter(!word2 %in% stop_words$word) %>%\n  mutate(bigram = paste(word1, word2, sep=\" \")) %>%\n  group_by(bigram) %>%\n  tally(sort=TRUE) %>%\n  mutate(percent = (n/sum(n))*100) %>%\n  top_n(10)\n\nSelecting by percent\n\n\n# A tibble: 10 × 3\n   bigram               n percent\n   <chr>            <int>   <dbl>\n 1 covid 19         17357   5.35 \n 2 19 vaccine        2638   0.814\n 3 covid relief      2159   0.666\n 4 19 pandemic       1495   0.461\n 5 covid19 vaccine   1227   0.379\n 6 covid vaccine     1099   0.339\n 7 health care        908   0.280\n 8 covid19 pandemic   872   0.269\n 9 public health      860   0.265\n10 19 relief          805   0.248\n\n\nWhile “covid 19” is still the leading phrase, vaccine-related phrases dominate the top 10, and “public health” and “health care” have slipped down the list.\nSo far, we’ve only looked at the entire set of tweets, not any characteristics of who posted them. Would these lists be any different for Democrats and Republicans? To find out, we just need to add to our filter.\n\ncovid_tweets %>%\n  filter(created < '2021-01-01', party == 'D') %>%\n  unnest_tokens(bigram, content, token = \"ngrams\", n = 2) %>%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") %>%\n  filter(!word1 %in% stop_words$word) %>%\n  filter(!word2 %in% stop_words$word) %>%\n  mutate(bigram = paste(word1, word2, sep=\" \")) %>%\n  group_by(bigram) %>%\n  tally(sort=TRUE) %>%\n  mutate(percent = (n/sum(n))*100) %>%\n  top_n(10)\n\nSelecting by percent\n\n\n# A tibble: 10 × 3\n   bigram                   n percent\n   <chr>                <int>   <dbl>\n 1 covid 19             22182   3.57 \n 2 health care           3398   0.547\n 3 public health         3314   0.534\n 4 town hall             2481   0.400\n 5 covid19 pandemic      2426   0.391\n 6 19 pandemic           1923   0.310\n 7 coronavirus pandemic  1903   0.307\n 8 covid relief          1525   0.246\n 9 relief package        1478   0.238\n10 social distancing     1373   0.221\n\ncovid_tweets %>%\n  filter(created < '2021-01-01', party == 'R') %>%\n  unnest_tokens(bigram, content, token = \"ngrams\", n = 2) %>%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") %>%\n  filter(!word1 %in% stop_words$word) %>%\n  filter(!word2 %in% stop_words$word) %>%\n  mutate(bigram = paste(word1, word2, sep=\" \")) %>%\n  group_by(bigram) %>%\n  tally(sort=TRUE) %>%\n  mutate(percent = (n/sum(n))*100) %>%\n  top_n(10)\n\nSelecting by percent\n\n\n# A tibble: 10 × 3\n   bigram                   n percent\n   <chr>                <int>   <dbl>\n 1 covid 19              9901   3.45 \n 2 coronavirus pandemic   966   0.336\n 3 health care            943   0.328\n 4 ー 19                  782   0.272\n 5 covid ー               782   0.272\n 6 covid19 pandemic       769   0.268\n 7 cares act              759   0.264\n 8 19 pandemic            733   0.255\n 9 american people        711   0.248\n10 covid relief           703   0.245\n\n\nNow we can begin to see some differences between the parties. We also could do the same for different kinds of accounts: the title column represents the role of the account, and if it includes “Candidate” then the tweet is from a campaign account. Let’s compare House of Representatives’ official and campaign tweets during 2020:\n\ncovid_tweets %>%\n  filter(created < '2021-01-01', title == 'House Representative') %>%\n  unnest_tokens(bigram, content, token = \"ngrams\", n = 2) %>%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") %>%\n  filter(!word1 %in% stop_words$word) %>%\n  filter(!word2 %in% stop_words$word) %>%\n  mutate(bigram = paste(word1, word2, sep=\" \")) %>%\n  group_by(bigram) %>%\n  tally(sort=TRUE) %>%\n  mutate(percent = (n/sum(n))*100) %>%\n  top_n(10)\n\nSelecting by percent\n\n\n# A tibble: 10 × 3\n   bigram                   n percent\n   <chr>                <int>   <dbl>\n 1 covid 19             15796   3.53 \n 2 town hall             2265   0.507\n 3 health care           2093   0.468\n 4 public health         2033   0.455\n 5 covid19 pandemic      1851   0.414\n 6 19 pandemic           1522   0.341\n 7 coronavirus pandemic  1457   0.326\n 8 covid relief          1108   0.248\n 9 telephone town        1051   0.235\n10 relief package        1010   0.226\n\ncovid_tweets %>%\n  filter(created < '2021-01-01', title == 'House Candidate') %>%\n  unnest_tokens(bigram, content, token = \"ngrams\", n = 2) %>%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") %>%\n  filter(!word1 %in% stop_words$word) %>%\n  filter(!word2 %in% stop_words$word) %>%\n  mutate(bigram = paste(word1, word2, sep=\" \")) %>%\n  group_by(bigram) %>%\n  tally(sort=TRUE) %>%\n  mutate(percent = (n/sum(n))*100) %>%\n  top_n(10)\n\nSelecting by percent\n\n\n# A tibble: 10 × 3\n   bigram                   n percent\n   <chr>                <int>   <dbl>\n 1 covid 19              8378   3.38 \n 2 public health          778   0.314\n 3 covid ー               722   0.291\n 4 ー 19                  721   0.291\n 5 health care            721   0.291\n 6 19 pandemic            519   0.209\n 7 social distancing      518   0.209\n 8 coronavirus pandemic   512   0.207\n 9 town hall              482   0.195\n10 covid relief           458   0.185\n\n\nThere are some differences here, too, but also some potential challenges to doing an analysis. For one, there are variations of words like “vaccine” that could probably be standardized - maybe using OpenRefine - that would give us cleaner results. There might be some words among our list of stop words that actually are meaningful in this context."
  },
  {
    "objectID": "textanalysis.html#sentiment-analysis",
    "href": "textanalysis.html#sentiment-analysis",
    "title": "29  An intro to text analysis",
    "section": "29.2 Sentiment Analysis",
    "text": "29.2 Sentiment Analysis\nAnother popular use of text analysis is to measure the sentiment of a word - whether it expresses a positive or negative idea - and tidytext has built-in tools to make that possible. We use word counts like we’ve already calculated and bring in a dataframe of words (called a lexicon) along with their sentiments using a function called get_sentiments. The most common dataframe is called “bing” which has nothing to do with the Microsoft search engine. Let’s load it:\n\nbing <- get_sentiments(\"bing\")\n\nbing_word_counts_2020 <- unique_words_2020 %>%\n  inner_join(bing) %>%\n  count(word, sentiment, sort = TRUE)\n\nJoining, by = \"word\"\n\nbing_word_counts_2021 <- unique_words_2021 %>%\n  inner_join(bing) %>%\n  count(word, sentiment, sort = TRUE)\n\nJoining, by = \"word\"\n\nView(bing_word_counts_2020)\nView(bing_word_counts_2021)\n\nGauging the sentiment of a word can be heavily dependent on the context, and as with other types of text analysis sometimes larger patterns are more meaningful than individual results. But the potential with text analysis is vast: knowing what words and phrases that public officials employ can be a way to evaluate their priorities, cohesiveness and tactics for persuading voters and their colleagues. And those words and phrases are data."
  },
  {
    "objectID": "basicstats.html",
    "href": "basicstats.html",
    "title": "30  Basic Stats: Linear Regression and The T-Test",
    "section": "",
    "text": "“Reveal’s analysis found that businesses in states that Trump won in 2016 received a far greater share of the small-business relief funds than those won by his Democratic rival, Hillary Clinton. Eight of the top 10 recipient states – ranked according to the proportion of each state’s businesses that received funding – went to Trump in 2016. Meanwhile, seven of the bottom 10 states, where the lowest proportion of businesses received funding, went to Clinton. Taken together, 32% of businesses in states that Trump won got Paycheck Protection Program dollars, we found, compared with 22% of businesses in states that went to Clinton.”\nIt continued: “The figures were so stark that they sparked concerns of political interference. Rep. Jackie Speier, a California Democrat who serves on the House Oversight and Reform Committee, said the data raise questions about whether stimulus dollars were deliberately funneled to states that voted for Trump and have Republican governors.”\nThe story didn’t present any evidence of political meddling. Instead, it offered the results of several lines of data analysis that attempted to answer this central question: did red states get a bigger slice of the PPP pie than blue states?\nMostly, it used basic descriptive statistics, calculating rates, ranking states and computing averages. But the data set it used also presents an opportunity to use two slightly more advanced statistical analysis methods to look for patterns: linear regression, to examine relationships, and a t.test, to confirm the statistical validity of an average between two groups. So, let’s do that here.\nFirst, let’s load libraries. We’re going to load janitor, the tidyverse and a new package, corrr, which will help us do linear regression a bit easier than base R.\n\nlibrary(janitor)\nlibrary(tidyverse)\nlibrary(corrr)\n\nNow let’s load the data we’ll be using. It has five fields:\n\nstate_name\nvote_2016: whether Trump or Clinton won the state’s electoral vote.\npct_trump: the percentage of the vote Trump received in the state.\nbusinesses_receiving_ppe_pct: the percentage of the state’s small businesses that received a PPP loan.\nppe_amount_per_employee: the average amount of money provided by PPP per small business employee in the state.\n\n\nreveal_data <- read_rds(\"data/reveal_data.rds\")\n\nreveal_data\n\n# A tibble: 51 × 5\n   state_name   vote_2016 pct_trump businesses_receiving_ppe_pct ppe_amount_pe…¹\n   <chr>        <chr>         <dbl>                        <dbl>           <dbl>\n 1 North Dakota Trump          63.0                           58            7928\n 2 Nebraska     Trump          58.8                           56            7244\n 3 South Dakota Trump          61.5                           53            6541\n 4 Oklahoma     Trump          65.3                           50            6499\n 5 Mississippi  Trump          57.9                           49            5674\n 6 Iowa         Trump          51.2                           48            6642\n 7 Kansas       Trump          56.6                           47            7087\n 8 Hawaii       Clinton        29.4                           47            7417\n 9 Maine        Clinton        43.5                           45            6617\n10 Arkansas     Trump          60.6                           44            5549\n# … with 41 more rows, and abbreviated variable name ¹​ppe_amount_per_employee\n\n\n\n31 Linear Regression\nLet’s start with this question: did small businesses in states that voted more strongly for Trump get loans at higher rate than small businesses in Democratic states? We can answer it by examining the relationship or correlation between two variables, pct_trump and businesses_receiving_ppe_pct. How much do they move in tandem? Do states with more Trump support see bigger average PPP loans? Do extra Trumpy states get even more? Do super blue states get the least?\nLet’s start by plotting them to get a sense of the pattern.\n\nreveal_data %>%\n  ggplot() +\n  geom_point(aes(x=pct_trump,y=businesses_receiving_ppe_pct)) +\n  geom_smooth(aes(x=pct_trump,y=businesses_receiving_ppe_pct), method=\"lm\")\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nIt’s a bit messy, but we can see something of a pattern here in the blob of dots. Generally, the dots are moving from the lower left (less Trumpy states that got loans at a lower rate) to upper right (red states that got loans at a higher rate). The blue “line of best fit” shows the general direction of the relationship.\nLet’s test another variable, the average amount of money provided by PPP per small business employee in the state.\n\nreveal_data %>%\n  ggplot() +\n  geom_point(aes(x=pct_trump,y=ppe_amount_per_employee)) +\n  geom_smooth(aes(x=pct_trump,y=ppe_amount_per_employee), method=\"lm\")\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nThis one is a bit messier. There may be a slight upward slope in this blob of dots, but it’s not quite as apparent. It seems less certain that there’s a relationship between these two variables.\nWe can be a bit more precise by calculating a statistic called the correlation coefficient, also called “r”. r is a value between 1 and -1. An r of 1 indicates a strong positive correlation.\nAn increase in air temperature and air conditioning use at home is strongly-positively correlated: the hotter it gets, the more we have to use air conditioning. If we were to plot those two variables, we might not get 1, but we’d get close to it.\nAn r of -1 indicates a strong negative correlation. An increase in temperature and home heating use is strongly negatively correlated: the hotter it gets, the less heat we use indoors. We might not hit -1, but we’d probably get close to it.\nA correlation of 0 indicates no relationship.\nAll r values will fall somewhere on this scale, and how to interpret them isn’t always straightforward. They’re best used to give general guidance when exploring patterns.\nWe can calculate r with a function from the corrr package called “correlate()”. First, we remove the non-numeric values from our reveal_data (state name and a binary vote_2016 column), then we correlate.\n\nreveal_data %>%\n  select(-state_name, -vote_2016) %>%\n  correlate() %>%\n  select(term, pct_trump)\n\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\n\n# A tibble: 3 × 2\n  term                         pct_trump\n  <chr>                            <dbl>\n1 pct_trump                       NA    \n2 businesses_receiving_ppe_pct     0.522\n3 ppe_amount_per_employee          0.221\n\n#glimpse(reveal_data)\n\nThe table this function produces generally confirms our interpretation of the two graphs above. The relationship between a state’s pct_trump and ppe_amount_per employee is positive, but at .22 (on a scale of -1 to 1), the relationship isn’t particularly strong. That’s why the second graphic above was messier than the first.\nThe relationship between businesses in a state receiving ppe and the state’s Trump vote is a bit stronger, if still moderate, .52 (on a scale of -1 to 1). Is this finding statistically valid? We can get a general sense of that by calculating the p-value of this correlation, a test of statistical significance. For that, we can use the cor.test function.\n\ncor.test(reveal_data$pct_trump, reveal_data$businesses_receiving_ppe_pct)\n\n\n    Pearson's product-moment correlation\n\ndata:  reveal_data$pct_trump and reveal_data$businesses_receiving_ppe_pct\nt = 4.2818, df = 49, p-value = 8.607e-05\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2875734 0.6971386\nsample estimates:\n      cor \n0.5218038 \n\n\nThis output is quite a bit uglier, but for our purposes there are two key pieces of information from this chunk of unfamiliar words. First, it shows the correlation calculated above: r 0.5218. Two, it shows the p-value, which is 0.00008607. That’s very low, as far as p-values go, which indicates that there’s a very slim chance that our finding is a statistical aberration.\nNow let’s test the other one, the relationship between the pct_trump and the ppe_amount_per_employee.\n\ncor.test(reveal_data$pct_trump, reveal_data$ppe_amount_per_employee)\n\n\n    Pearson's product-moment correlation\n\ndata:  reveal_data$pct_trump and reveal_data$ppe_amount_per_employee\nt = 1.5872, df = 49, p-value = 0.1189\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.05798429  0.46818515\nsample estimates:\n     cor \n0.221133 \n\n\nAgain, it shows our r value of .22, which was weaker. And the p-value here is a much larger 0.12. That indicates a higher chance of our finding being a statistical aberration, high enough that I wouldn’t rely on its validity.\np < .05 is accepted in many scientific disciplines – and by many data journalists – as the cutoff for statistical significance. But there’s heated debate about that level, and some academics question whether p-values should be relied on so heavily.\nAnd to be clear, a low p-value does not prove that we’ve found what we set out to find. There’s nothing on this graph or in the regression model output that proves that Trump’s administration tipped the scales in favor of states that voted for it. It’s entirely possible that there’s some other variable – or variables – not considered here that explain this pattern.\nAll we know is that we’ve identified a potentially promising pattern, worthy of additional reporting and analysis to flesh out.\n\n\n32 T-tests\nLet’s suppose we want to ask a related set of questions: did Trump states get higher ppp loan amounts per employee than states won by Clinton? Or did a larger percentage of businesses in states won by Trump receive, on average, a higher rate of PPP loans on average than states won by Clinton.\nWe can do this because, in our data, we have a column with two possible categorical values, Clinton or Trump, for each state.\nWe could just calculate the averages like we’re used to doing.\n\nreveal_data %>%\n  group_by(vote_2016) %>%\n  summarise(\n    mean_ppp_amount_per_employee = mean(ppe_amount_per_employee),\n    mean_businesses_receiving_ppe_pct = mean(businesses_receiving_ppe_pct)\n  )\n\n# A tibble: 2 × 3\n  vote_2016 mean_ppp_amount_per_employee mean_businesses_receiving_ppe_pct\n  <chr>                            <dbl>                             <dbl>\n1 Clinton                          5704.                              28.2\n2 Trump                            6021.                              37.2\n\n\nExamining this, it appears that in both categories there’s a difference.\nThe average amount of ppp loans per employee in Clinton states is smaller than Trump states (6,000 to 5,700). And the average percentage of businesses that got loans in Trump states was larger – 37% – than Clinton states – 28%. Should we report these as meaningful findings?\nA t-test can help us answer that question. It can tell us where there’s a statistically significant difference between the means of two groups. Have we found a real difference, or have we chanced upon a statistical aberration? Let’s see by calculating it for the average loan amount.\n\nt.test(ppe_amount_per_employee ~ vote_2016, data = reveal_data)\n\n\n    Welch Two Sample t-test\n\ndata:  ppe_amount_per_employee by vote_2016\nt = -1.2223, df = 36.089, p-value = 0.2295\nalternative hypothesis: true difference in means between group Clinton and group Trump is not equal to 0\n95 percent confidence interval:\n -843.7901  209.1329\nsample estimates:\nmean in group Clinton   mean in group Trump \n             5703.571              6020.900 \n\n\nWe see our two means, for Trump and Clinton, the same as we calculated above. The t-value is approximately 1, the p-value here is .2295, both of which should which should give us pause that we’ve identified something meaningful. More on t-tests here\nLet’s try the percentage of businesses getting ppp loans.\n\nt.test(businesses_receiving_ppe_pct ~ vote_2016, data = reveal_data)\n\n\n    Welch Two Sample t-test\n\ndata:  businesses_receiving_ppe_pct by vote_2016\nt = -3.182, df = 45.266, p-value = 0.002643\nalternative hypothesis: true difference in means between group Clinton and group Trump is not equal to 0\n95 percent confidence interval:\n -14.68807  -3.30241\nsample estimates:\nmean in group Clinton   mean in group Trump \n             28.23810              37.23333 \n\n\nThis is a bit more promising. T is much stronger – about 3 – and the p-value is .002. Both of these should give us assurance that we’ve found something statistically meaningful. Again, this doesn’t prove that Trump is stacking the deck for states. It just suggests there’s a pattern worth following up on."
  },
  {
    "objectID": "writingwithdata.html",
    "href": "writingwithdata.html",
    "title": "31  Writing with numbers",
    "section": "",
    "text": "The number one sin of all early career data journalist is to get really, really, really attached to the analysis you’ve done and include every number you find.\nDon’t do that.\nNumbers tell you what. Numbers rarely tell you why. What question has driven most people since they were three years old? Why. The very first thing to do is realize that is the purpose of reporting. You’ve done the analysis to determine the what. Now go do the reporting to do the why. Or as an old editor of mine used to say “Now go do that reporting shit you do.”\nThe trick to writing a numbers story is to frame your story around people. Sometimes, your lead can be a number, if that number is compelling. Often, your lead is a person, a person who is one of the numbers you are writing about.\nTell their story. Briefly. Then, let us hear from them. Let them speak about what it is you are writing about.\nThen come the numbers."
  },
  {
    "objectID": "writingwithdata.html#how-to-write-about-numbers-without-overwhelming-with-numbers.",
    "href": "writingwithdata.html#how-to-write-about-numbers-without-overwhelming-with-numbers.",
    "title": "31  Writing with numbers",
    "section": "31.1 How to write about numbers without overwhelming with numbers.",
    "text": "31.1 How to write about numbers without overwhelming with numbers.\nWriting complex stories is often a battle against that complexity. You don’t want to overwhelm. You want to simplify where you can. The first place you can do that is only use exact numbers where an exact number is called for.\nWhere you can, do the following:\n\nUsing ratios instead of percents\nOften, it’s better to put it in counts of 10. 6 of 10, 4 of 10. It’s easy to translate that from a percentage to a ratio.\nBut be careful when your number is 45 percent. Is that 4 in 10 or 5 in 10?\nIf a ratio doesn’t make sense, round. There’s 287,401 people in Lincoln, according to the Census Bureau. It’s easier, and no less accurate, to say there’s more than 287,000 people in Lincoln.\n\nA critical question your writing should answer: As compared to what?\nHow does this compare to the average? The state? The nation? The top? The bottom?\nOne of the most damning numbers in the series of stories Craig Pittman and I wrote that became the book Paving Paradise was comparing approvals and denials.\nWe were looking at the US Army Corps of Engineers and their permitting program. We were able to get a dataset of just a few years of permits that was relatively clean. From that, we were able to count the number of times the corps had said yes to a developer to wipe out wetlands the law protected and how many times they said no.\nThey said yes 12,000 times. They said no once.\nThat one time? Someone wanted to build an eco-lodge in the Everglades. Literally. Almost every acre of the property was wetlands. So in order to build it, the developer would have to fill in the very thing they were going to try to bring people into. The corps said no."
  },
  {
    "objectID": "writingwithdata.html#when-exact-numbers-matter",
    "href": "writingwithdata.html#when-exact-numbers-matter",
    "title": "31  Writing with numbers",
    "section": "31.2 When exact numbers matter",
    "text": "31.2 When exact numbers matter\nSometimes ratios and rounding are not appropriate.\nThis is being written in the days of the coronavirus. Case counts are where an exact number is called for. You don’t say that there are more than 70 cases in Lancaster County on the day this was written. You specify. It’s 75.\nYou don’t say almost 30 deaths. It’s 28.\nWhere this also comes into play is any time there are deaths: Do not round bodies."
  },
  {
    "objectID": "writingwithdata.html#an-example",
    "href": "writingwithdata.html#an-example",
    "title": "31  Writing with numbers",
    "section": "31.3 An example",
    "text": "31.3 An example\nRead this story from USA Today and the Arizona Republic. Notice first that the top sets up a conflict: People say one thing, and that thing is not true.\n\nNo one could have anticipated such a catastrophe, people said. The fire’s speed was unprecedented, the ferocity unimaginable, the devastation unpredictable.\n\n\nThose declarations were simply untrue. Though the toll may be impossible to predict, worst-case fires are a historic and inevitable fact.\n\nThe first voice you hear? An expert who studies wildfires.\n\nPhillip Levin, a researcher at the University of Washington and lead scientist for the Nature Conservancy in Washington, puts it this way: “Fire is natural. But the disaster happens because people didn’t know to leave, or couldn’t leave. It didn’t have to happen.”\n\nThen notice how they take what is a complex analysis using geographic information systems, raster analysis, the merging of multiple different datasets together and show that it’s quite simple – the averaging together of pixels on a 1-5 scale.\nThen, the compare what they found to a truly massive fire: The Paradise fire that burned 19,000 structures.\n\nAcross the West, 526 small communities — more than 10 percent of all places — rank higher.\n\nAnd that is how it’s done. Simplify, round, ratios: simple metrics, powerful results."
  }
]