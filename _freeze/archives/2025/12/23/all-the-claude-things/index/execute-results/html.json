{
  "hash": "4a74a8cf79d0ca9448d68f0972638d6e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"All the Claude (Code) Things\"\nauthor: \"Derek Willis\"\ndate: \"2025-12-23\"\nformat:\n  html:\n    df-print: paged\n    toc: true\nengine: knitr\n---\n\n\n\nOf nearly all of the potential uses for Large Language Models, perhaps the best and most defensible is using them to write code. During the recently-completed fall semester, I co-taught [a Computer Science class with Bill Pugh](https://www.cs.umd.edu/class/fall2025/cmsc398z/) that explored some good ways to do that. We used a lot of Claude Code for that class, and it quickly has become my favorite coding model. And then, in the middle of November, the folks at Anthropic released Claude Code on the Web and gave paid users free credits to use it. I got $250 worth, and I had a lot of ideas.\n\nI didn't use all of the $250 in credits, but I made it to about $25. I had Claude Code revive a few very old (10+ years) projects, make significant progress on some long-stalled codebases and put the finishing touches on complex problems. There were the now-familiar moments when the model got caught in unproductive loops, as well as over-engineered approaches to simpler tasks. But on the whole, a couple of weeks of Claude Code on the web proved very useful. Here's some of what I had Claude Code do.\n\n### Simple Apps\n\nOne of the sweet spots for coding models are simple HTML/CSS/JS applications - often deployed on GitHub Pages - based on some data you've got in a repository. I have a lot of data sitting around in repositories, so I had Claude Code make some apps, because not everyone wants to look at a CSV file.\n\n* [Maryland Women's Basketball Data](https://thescoop.org/umdwbb-data/). I've been collecting and storing JSON files with game-level information from women's college basketball for a few years now. But few people want to look at JSON files in a browser, so this was a great candidate for a simple web app. This one took a number of attempts at refinement, and I needed to get specific with Claude Code about some of the things I wanted, especially the details of each game. But I also asked for an assist network analysis without providing any other instructions, and what I got is pretty great. It updates every day via GitHub Actions. \n* [Post Ping](https://thescoop.org/wapo-alerts/). I've been collecting SMS alerts from the Washington Post since March 2024, and here is the entirety of my instructions: \"Create a slightly whimsical webpage that presents the alert data in several ways: a quick look at the alerts from the previous 24 hours, a searchable filter on title, another on target topic. Have a line chart showing the trend of alerts and title the page “Post Ping”. Use JS CSS HTML.\" I think we can all agree that the font is perhaps too \"whimsical\", but Claude Code did everything I asked for and produced a site I actually look at.\n* [Frederick County Fire & Rescue Incidents](https://newsappsumd.github.io/fredscanner/). Working with a Maryland newspaper,I wrote a scraper to track emergency response incidents and provide them as an RSS feed reporters could subscribe to. That wasn't really effective, but this simple searchable and browsable database that Claude Code built was more useful.\n* [UMPD Logs](https://thescoop.org/umpd-logs/build/). A former student, Rachel Marconi, [originally built this app](https://rachelmarconi.github.io/na-gov-upgrade/build/index.html) for my news apps class, and I had Claude add some visual upgrades and some topline summary data with minimal effort.\n* [Hattiesburg (Miss.) Public Notices](https://thescoop.org/mcjw25/). For a presentation at a conference on local news hosted at Southern Miss, I had Claude Code build an app that allows users to comb through public notices from the Hattiesburg American, using an LLM to provide a newsworthiness value to each announcement.\n* [WV Football Game Viewer](https://thescoop.org/wv_football/). For a 2023 Mountain State Spotlight story on transfers in West Virginia high school football, I [scraped game scores](https://thescoop.org/archives/2023/11/13/wv-football-blowouts/) to measure blowouts. Claude Code took that data and made a simple website of it.\n\n### Command-Line Utilities & Scrapers\n\nBuilding pipelines for data is a big part of my work (and teaching), and mostly that has meant writing single Python scripts do do the work of obtaining and transforming data. This set of Claude Code work led to more robust and predictable CLI tools.\n\n* [Testudo](https://github.com/dwillis/testudo). I picked up the original code for this project, which scrapes [Maryland's Schedule of Classes site](https://app.testudo.umd.edu/soc/), from Ed Summers. It was good code, but it also had some constraints: users could only scrape all of the classes for a given term, producing JSON files and a single CSV file with the combined data. Using Claude Code, I added a bunch of features, including optional scraping for syllabi. Plus tests and better documentation. It is a much better-organized and more reliable piece of software now.\n* [python-statement](https://github.com/dwillis/python-statement). Years ago, I extracted [a Ruby gem that scraped press releases](https://github.com/dwillis/statement) from the congressional news app I worked on at The New York Times. It was super useful but became a pain to maintain, and I haven't written much Ruby in the past five years. So I had Claude Code port it to Python, where I hope to keep it going with some student help. Translating code from one language to another is very much in an LLM's wheelhouse, although it was interesting and somewhat disappointing that given hundreds of methods, Claude literally gave up, coming up with lines such as \"# ... and more methods here\". These coding assistants really are like disinterested interns some times, but I would not have done this myself.\n* [College Sports Rosters](https://github.com/Sports-Roster-Data/). For a few years I've maintained scrapers to extract player information from official collegiate roster sites for women's basketball and volleyball. I based the latter code on the former, but that involved a lot of work and maintenance was an issue, making expanding to other sports an unappealing prospect. Claude Code changed that, not only producing very serviceable code for men's and women's soccer, lacrosse and women's field hockey, but also creating [common utilities](https://github.com/Sports-Roster-Data/utilities) shared across repositories. And it cleaned up and reorganized the original [women's basketball scraper code](https://github.com/dwillis/wbb/blob/main/ncaa/rosters/rosters.py), making it much easier to work with.\n* [Foreign Gifts to U.S. Officials](https://github.com/dwillis/all-foreign-gifts-around-us). I first built this repo to show off [the ability of LLMs to parse weirdly-structured PDFs](https://thescoop.org/archives/2024/06/22/all-foreign-gifts-around-us/), and my code was kind of a mess. Claude Code polished it up and reorganized the files, adding a CLI and a way to visualize the data. Improving or extending existing codebases is a pretty solid use case for these coding assistants.\n\n### Resurrected Apps\n\nI'll do a follow-up post on this category, because it is the most ambitious use case I could imagine: taking a code base that is large, old and unmaintained and bringing it back to life. I pointed Claude Code at several of these, including [official Senate disbursements](https://github.com/dwillis/senate_disbursements) (one of the white whales of the open government movement), a Django app [to download and parse IRS campaign finance filings](https://github.com/dwillis/django-irs-filings) and even [Fumblerooski](https://github.com/dwillis/fumblerooski), the first Django app I ever made. Oh, and a still-private repo containing the codebase of Capitol Words, [a former Sunlight Foundation joint that ProPublica briefly took over and tried to resurrect](https://www.propublica.org/nerds/sunlight-labs-takeover-update).\n\nWhy a separate post on these? Because unlike the other projects mentioned above, these are substantial and complicated codebases. I'll need some time to dig into them and assess the changes. I'm sure some of them will be incomplete or screwed up in some way - I didn't give many instructions aside from \"update the code so it runs now\" and tried to simplify things where I could. The results - as with most things generated by AI - sound impressive, but the devil's in the details.\n\n### The Takeaway\n\nProperly guided by users who know what they are doing, these coding assistants can be super useful. They certain can make quick work of what my friend and colleague [Matt Waite calls the \"throwaway news app\"](https://www.niemanlab.org/2025/12/the-rise-of-the-throwaway-news-app/), a simple HTML/JS/CSS site that can make data and information accessible. That alone is a huge benefit, if newsrooms are smart about how they do it. The way I employed Claude Code during that mid-November stretch wasn't ideal; a deadline often means things get left behind or neglected. But since then, I've made good use of Claude Code and a few other assistants, especially in writing tests, documentation and helping me draft plans for upgrades. Although these tools are designed for coding, you can throw other tasks at them, which sometimes is an improvement over a chat interface since they can use tools to help them understand and operate on files. I often use Claude Code to help me understand an unfamiliar code base, for example.\n\nThe other lesson I've learned is that these models write code _quickly_, and that's often not a great thing. But you can slow them down, both through direct instruction and via what I call \"constitutional\" documents like Claude.md that contain basic operational principles: do X this way, wait for confirmation before doing Y, etc. The \"magic\" of these assistants is that they seem like they can drive the bus. But you're the one who should be behind the wheel.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../../../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}