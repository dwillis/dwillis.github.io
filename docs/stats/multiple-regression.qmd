# Multiple Regression: Controlling for Multiple Factors

Your sports editor has a theory about football: "It's simple—offense wins games. Teams that gain more yards win more games." He wants you to write a feature proving this.

You pull the NFL statistics. He's not wrong—teams with more yards do tend to win more. But you've covered enough football to know it's more complicated. What about turnovers? What about actually scoring points versus just moving the ball? What about defense?

You could run separate correlations: yards vs. wins, points vs. wins, takeaways vs. wins. But that doesn't tell you which factor matters **most**, or whether one factor only seems important because it correlates with another factor that's the real driver.

This is where **multiple regression** comes in. It lets you examine several predictors simultaneously and see which ones truly matter—while controlling for all the others.

## The Problem with One Variable at a Time

Let's say you find that yards gained correlates strongly with wins (r = 0.70). Great! But yards also correlate with points scored (r = 0.85). And points also correlate with wins (r = 0.80).

So which is it? Do yards cause wins? Or do yards lead to points, and points cause wins? Or does something else entirely drive both?

**Simple regression** can't untangle this. It looks at one relationship at a time.

**Multiple regression** can. It asks: "What's the effect of yards on wins, *after accounting for points and everything else*?"

## What Multiple Regression Does

Multiple regression builds a model with multiple predictors:

```
Wins = β₀ + β₁(Points) + β₂(Yards) + β₃(Takeaways)
```

Each coefficient (β) shows the effect of that variable **while holding all other variables constant**.

- **β₁**: Effect of one additional point, *if yards and takeaways stay the same*
- **β₂**: Effect of one additional yard, *if points and takeaways stay the same*
- **β₃**: Effect of one additional takeaway, *if points and yards stay the same*

This "holding other things constant" is the magic. It's how you isolate the independent effect of each factor.

## Setting Up

```{r setup, message=FALSE}
library(tidyverse)
library(corrplot)
```

## Working with NFL Data

Let's test your editor's theory using 2019 NFL statistics:

```{r load-data, message=FALSE}
teams <- read_csv("https://raw.githubusercontent.com/dwillis/jour405_files/main/nfl_2019.csv")

glimpse(teams)
```

We have 32 teams with offensive stats: yards gained, points scored, and takeaways (turnovers forced).

## Exploring the Relationships

Before building a complex model, always explore simple relationships first:

```{r summary}
teams |>
  select(Wins, `Points Scored`, `Yards Gained`, Takeaways) |>
  summary()
```

Now let's see how these variables correlate with each other:

```{r correlation}
selected_vars <- teams |>
  select(Wins, `Yards Gained`, Takeaways, `Points Scored`)

cor_matrix <- cor(selected_vars)
knitr::kable(cor_matrix, digits = 3, caption = "Correlation matrix")
```

Let's visualize this:

```{r corr-viz, fig.width=8, fig.height=8}
corrplot(cor_matrix, method = "circle", type = "upper",
         tl.col = "black", tl.srt = 45,
         addCoef.col = "black", number.cex = 0.8)
```

**Key observations:**
- All three variables correlate positively with wins (good news—they all matter)
- Points scored has the strongest correlation with wins (r = 0.79)
- Yards and points correlate strongly with each other (r = 0.78)

That last one is important: yards and points move together. This means when we put both in a regression, we need to watch for **multicollinearity**—predictors that overlap so much they confuse the model.

## Building the Multiple Regression Model

Now let's build a model with all three predictors:

```{r model}
model <- lm(Wins ~ `Yards Gained` + Takeaways + `Points Scored`, data = teams)
summary(model)
```

This output looks intimidating, but let's break it down piece by piece.

## Understanding the Coefficients

The coefficients table is the heart of the output:

```{r extract-coefs}
coef_summary <- summary(model)$coefficients
knitr::kable(coef_summary, digits = 4, caption = "Regression coefficients")
```

Let's interpret each predictor:

**Points Scored (β ≈ 0.039, p = 0.007):**
- For each additional point scored **while holding yards and takeaways constant**, teams win about 0.039 more games
- Multiply by 10 points: each 10-point increase predicts 0.39 more wins
- **Statistically significant** (p < 0.01)
- **This is the only factor that clearly matters**

**Takeaways (β ≈ 0.15, p = 0.08):**
- Each additional takeaway predicts 0.15 more wins
- **Marginally significant** (p = 0.08, just above the 0.05 threshold)
- Might matter, but evidence is weaker

**Yards Gained (β ≈ -0.001, p = 0.34):**
- **Wait, negative?** Counterintuitive!
- **Not statistically significant** (p = 0.34)
- **Doesn't matter in this model** once you account for points and takeaways

That negative coefficient for yards is surprising and telling. Let's figure out what it means.

## Why Yards Don't Matter (Once You Account for Points)

Your editor was wrong—but in an interesting way. Yards *do* correlate with wins when you look at them alone. But once you account for points scored, yards add nothing.

**The explanation:** **Yards are a means to an end.** Teams gain yards to score points. Some teams are efficient (fewer yards, more points). Some are inefficient (lots of yards, fewer points).

What matters for winning is **scoring points**, not just **moving the ball**.

The negative coefficient probably reflects this: among teams that score the same number of points, those that needed more yards to do so might be slightly less effective.

This is actually a story: "Efficiency matters more than raw yardage in NFL success."

## Checking for Multicollinearity

When predictors correlate strongly (like yards and points), it's called **multicollinearity**. It makes coefficients unstable and hard to interpret.

We can check with **Variance Inflation Factor (VIF)**:

```{r vif}
# VIF > 10 indicates serious multicollinearity
vif_values <- car::vif(model)
print(vif_values)

if (any(vif_values > 10)) {
  cat("\nWARNING: Serious multicollinearity detected!\n")
} else if (any(vif_values > 5)) {
  cat("\nModerate multicollinearity present, but probably okay.\n")
} else {
  cat("\nNo serious multicollinearity issues.\n")
}
```

**VIF interpretation:**
- VIF < 5: No problem
- 5 < VIF < 10: Moderate multicollinearity (watch out)
- VIF > 10: Serious problem (consider dropping variables)

Points and yards likely have elevated VIF values because they correlate at r = 0.78. That's high, but not disqualifying.

## How Good Is the Model?

Look at R-squared:

```{r model-fit}
r_squared <- summary(model)$r.squared
adj_r_squared <- summary(model)$adj.r.squared

cat("R-squared:", round(r_squared, 3), "\n")
cat("Adjusted R-squared:", round(adj_r_squared, 3), "\n")
cat("\n")
cat("This model explains", round(r_squared * 100, 1), "% of the variation in wins.\n")
```

**R-squared = 0.62** means 62% of variation in wins is explained by these three variables. That's pretty good! But 38% is still unexplained—defense, coaching, luck, and other factors matter too.

**Adjusted R-squared** penalizes adding predictors that don't help. Use this when comparing models with different numbers of variables.

The overall F-statistic tests whether the model is better than nothing:

```{r f-test}
f_stat <- summary(model)$fstatistic
p_value <- pf(f_stat[1], f_stat[2], f_stat[3], lower.tail = FALSE)

cat("F-statistic:", round(f_stat[1], 2), "\n")
cat("P-value:", format(p_value, scientific = TRUE), "\n\n")

if (p_value < 0.001) {
  cat("Yes, this model is highly significant overall.\n")
}
```

With p < 0.001, the model definitely beats just guessing the average number of wins.

## Making Predictions

Let's use the model to predict wins for hypothetical teams:

```{r predictions}
# Example: A high-scoring, efficient team
hypothetical_team <- tibble(
  `Yards Gained` = 5500,
  Takeaways = 25,
  `Points Scored` = 400
)

predicted_wins <- predict(model, newdata = hypothetical_team)
cat("Predicted wins for this team:", round(predicted_wins, 1), "\n")
```

Now let's predict for all real teams and find the outliers:

```{r residuals}
teams <- teams |>
  mutate(
    predicted_wins = predict(model),
    residual = Wins - predicted_wins
  )
```

## Finding Over-Performers

Which teams won more games than their stats predicted?

```{r over-performers}
teams |>
  arrange(desc(residual)) |>
  select(Team, Wins, predicted_wins, residual, `Points Scored`, Takeaways, `Yards Gained`) |>
  head(5) |>
  knitr::kable(digits = 2, caption = "Teams that exceeded expectations")
```

These teams are story leads. Why did they over-perform? Better coaching? Winning close games? Home-field advantage? Great defense (not in our model)?

## Finding Under-Performers

Which teams won fewer games than expected?

```{r underperformers}
teams |>
  arrange(residual) |>
  select(Team, Wins, predicted_wins, residual, `Points Scored`, Takeaways, `Yards Gained`) |>
  head(5) |>
  knitr::kable(digits = 2, caption = "Teams that fell short of expectations")
```

These are also stories. What went wrong? Bad coaching? Losing close games? Injuries? Bad luck?

## Visualizing Actual vs. Predicted

Let's see how well the model fits:

```{r actual-vs-predicted, fig.width=10, fig.height=8}
ggplot(teams, aes(x = predicted_wins, y = Wins)) +
  geom_point(size = 3, color = "steelblue") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  geom_text(aes(label = Team), size = 2.5, hjust = -0.1, vjust = -0.1, check_overlap = TRUE) +
  theme_minimal() +
  labs(
    title = "Actual vs. Predicted Wins",
    subtitle = "Points on red line = perfect prediction; distance from line = residual",
    x = "Predicted Wins (from model)",
    y = "Actual Wins"
  )
```

Teams close to the diagonal line fit the model well. Teams far from it are outliers—investigate those.

## Checking Model Assumptions

Regression assumes:
1. **Linearity**: Relationships are straight-line
2. **Independence**: Observations don't influence each other
3. **Homoscedasticity**: Variance is constant across predictions
4. **Normality**: Residuals are normally distributed

Let's check with diagnostic plots.

### Residual Plot

```{r residual-plot, fig.width=10, fig.height=6}
ggplot(teams, aes(x = predicted_wins, y = residual)) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  geom_point(size = 3, color = "steelblue") +
  theme_minimal() +
  labs(
    title = "Residual Plot: Checking for Patterns",
    subtitle = "Should show random scatter around zero (red line)",
    x = "Predicted Wins",
    y = "Residual (Actual - Predicted)"
  )
```

**Look for:**
- **Random scatter**: Good! Assumptions met.
- **Curved pattern**: Relationship isn't linear. Try transformations.
- **Funnel shape**: Variance isn't constant. Might need weighted regression.

### Q-Q Plot for Normality

```{r qq-plot, fig.width=10, fig.height=6}
ggplot(teams, aes(sample = residual)) +
  stat_qq() +
  stat_qq_line(color = "red") +
  theme_minimal() +
  labs(
    title = "Normal Q-Q Plot: Checking Residual Distribution",
    subtitle = "Points should follow the red line if residuals are normal"
  )
```

If points deviate from the line, residuals aren't normally distributed. With small samples (n=32), slight deviations are okay.

## Comparing Simpler Models

Should we include all three predictors, or is a simpler model better?

```{r model-comparison}
# Model with just points
model_simple <- lm(Wins ~ `Points Scored`, data = teams)

# Model with points and takeaways
model_medium <- lm(Wins ~ `Points Scored` + Takeaways, data = teams)

# Compare
comparison <- tibble(
  Model = c("Points Only", "Points + Takeaways", "All Three Predictors"),
  Predictors = c(1, 2, 3),
  R_Squared = c(
    summary(model_simple)$r.squared,
    summary(model_medium)$r.squared,
    summary(model)$r.squared
  ),
  Adj_R_Squared = c(
    summary(model_simple)$adj.r.squared,
    summary(model_medium)$adj.r.squared,
    summary(model)$adj.r.squared
  )
)

knitr::kable(comparison, digits = 4, caption = "Model comparison")
```

**Observations:**
- Points alone explains 62% of variance
- Adding takeaways improves it slightly
- Adding yards doesn't help (adjusted R² barely changes)

**Best model:** Probably Points + Takeaways. Simpler, and yards isn't significant.

## How to Report This

You're ready to write the story. How do you frame it?

**Bad version:**
"Teams that gain more yards win more games, analysis shows."

**Better version:**
"**Scoring Efficiency Beats Raw Yardage in NFL Success**

An analysis of 2019 NFL statistics reveals that scoring points—not simply moving the ball—is what drives victories.

A statistical model examining yards gained, turnovers forced, and points scored found that only points scored significantly predicted wins (β = 0.039, p = 0.007). For every 10 additional points a team scored over the season, they won approximately 0.4 more games.

Surprisingly, yards gained showed no significant relationship with wins after accounting for points and turnovers (β = -0.001, p = 0.34). This suggests that offensive efficiency—converting drives into points—matters more than raw yardage.

'You can gain 400 yards and lose if you don't put points on the board,' said former NFL coach Mike Smith. 'Red zone execution and limiting turnovers are what separate winners from losers.'

The model explained 62% of variation in team wins. The New England Patriots significantly outperformed expectations, winning 12 games despite statistics that predicted only 9..."

**What this includes:**
- The finding in plain English
- Specific coefficients with interpretation (10 points = 0.4 wins)
- Statistical details in parentheses (not the lede)
- The surprising negative finding about yards
- Expert voice for context
- Model fit (62%)
- Notable outliers

## Common Pitfalls

**Don't include everything:** More predictors ≠ better model. Include only variables that make theoretical sense and are statistically significant.

**Don't ignore multicollinearity:** If VIF > 10, coefficients are unreliable. Drop one of the correlated variables.

**Don't overfit:** Rule of thumb: at least 10-20 observations per predictor. With 32 teams and 3 predictors, we're borderline.

**Don't extrapolate:** Don't predict for values outside your data range. If no team scored 600 points, don't predict wins for a 600-point team.

**Don't claim causation:** Multiple regression shows association while controlling for other variables. It doesn't prove causation. You'd need experimental design (randomized controlled trials) for that.

**Don't p-hack:** Don't try 50 different models and report only the one with significant results. Pre-specify your model based on theory.

## Other Applications in Journalism

**Education reporting:**

```{r education-example, eval=FALSE}
# What predicts school performance?
model <- lm(test_scores ~ poverty_rate + class_size +
            teacher_experience + funding_per_student,
            data = schools)

# Which factors matter most?
# Which schools beat expectations?
```

**Political coverage:**

```{r politics-example, eval=FALSE}
# What predicts county-level voting?
model <- lm(dem_vote_share ~ unemployment + median_income +
            college_pct + previous_dem_vote,
            data = counties)

# Find counties that flipped unexpectedly
```

**Criminal justice:**

```{r crime-example, eval=FALSE}
# What drives crime rates?
model <- lm(crime_rate ~ poverty + unemployment +
            police_per_capita + population_density,
            data = neighborhoods)

# Which neighborhoods are safer than expected?
```

## The Bottom Line

Multiple regression lets you:
- **Control for confounders**: See each variable's independent effect
- **Quantify relative importance**: Which factors matter most?
- **Make better predictions**: Use multiple factors together
- **Find outliers**: Who's over/under-performing given their circumstances?

**Key principles:**
- Each coefficient shows the effect while holding other variables constant
- Statistical significance (p < 0.05) indicates a reliable predictor
- R-squared shows percentage of variance explained
- Multicollinearity (VIF > 10) makes coefficients unreliable
- Always check assumptions with residual plots
- Simpler models are often better than complex ones
- Association ≠ causation

## Your Turn

Using the NFL data:

1. Build a model with just `Points Scored` and `Takeaways`. How does the R-squared compare to the full model? Are both predictors significant?

2. Find the team with the largest positive residual. Research that team's 2019 season. What factors not in the model might explain their over-performance?

3. Why might the yards coefficient be negative and non-significant? Write a one-paragraph explanation for a general audience.

4. Predict wins for a team with 380 points scored, 5,200 yards gained, and 18 takeaways. Is this prediction reliable?

## Final Thoughts

Multiple regression is one of the most powerful tools in data journalism. It moves you beyond simple "X correlates with Y" stories to "X matters even after accounting for Y and Z"—which is how the real world actually works.

Use it to:
- Hold institutions accountable fairly (controlling for demographic challenges)
- Find truly exceptional performers (beating expectations, not just having good numbers)
- Understand what really drives outcomes (not just what correlates with outcomes)
- Make data-driven predictions (using multiple relevant factors)

Master multiple regression, and you'll be equipped to handle complex analytical stories with sophistication and rigor. It's the culmination of everything we've learned in this book: loading data, calculating rates, finding outliers, measuring relationships, testing significance, and now—controlling for multiple factors simultaneously.

Welcome to advanced data journalism. Go tell important stories with numbers.
