# Getting Started with Data

Your editor has a breaking news tip: a source at city hall says the mayor's office is hiding data about police overtime costs. You make some calls, file a records request, and three days later, an email arrives with an attachment: `police_overtime_2015_2024.csv`.

You open it in Excel. Hundreds of rows. Dozens of columns. Numbers and dates and department codes. Some columns have missing values. Some have weird formatting. The column names have spaces and special characters. It's a mess.

This is real data journalism. Not clean academic datasets, but messy government spreadsheets that someone  created in between phone calls and lunch breaks. Your job: make sense of it, find the story, and do it accurately before the competition does.

This is where R comes in.

## Why R for Journalism?

You could do this in Excel. Many journalists do. But Excel has limits:

- It's hard to document your steps (did you manually delete rows? Which ones?)
- It's easy to make mistakes (one wrong click can corrupt data)
- It doesn't scale (try opening a CSV with 500,000 rows in Excel)
- It's not reproducible (try explaining your analysis to an editor)

R solves these problems. Every step is code. Code can be saved, shared, reviewed, and re-run. You can document your thinking. You can catch mistakes. And when the city sends you updated data next week, you just re-run your code instead of redoing hours of Excel work.

Plus, R is free. And it's what the smartest data journalists use.

## Loading Your First Tool: The Tidyverse

R comes with basic functionality, but for real work you need additional packages. The most important is the `tidyverse`, a collection of tools designed specifically for data analysis:

```{r setup, message=FALSE}
library(tidyverse)
```

That one line loads eight different packages:

- **readr**: Reads CSV files and other data formats
- **dplyr**: Manipulates data (filter, select, group, summarize)
- **ggplot2**: Creates visualizations
- **tidyr**: Reshapes data from wide to long format and vice versa
- **tibble**: A modern take on data frames
- **stringr**: Works with text
- **forcats**: Handles categorical data
- **purrr**: Functional programming tools

Think of the tidyverse as your data journalism Swiss Army knife. You'll use it every day.

## Getting Data Into R

Data comes in many formats. CSV (comma-separated values) is the most common in journalism because it's simple and universal. Government agencies love CSV files.

Here's how to load one:

```{r, eval=FALSE}
# Load from your computer
data <- read_csv("path/to/your/file.csv")

# Load from the internet (common in 2025)
data <- read_csv("https://example.com/data/file.csv")
```

R stores this in a **data frame** (or technically, a **tibble**, which is an upgraded data frame). Think of it like a spreadsheet: rows are observations, columns are variables.

## Looking at Your Data (Always Do This First!)

Never, ever start analyzing data before looking at it. You need to understand:
- What's in each column?
- How many rows?
- Any missing values?
- Do the numbers look reasonable?

Here are the commands you'll use every single time:

```{r, eval=FALSE}
# See the first six rows
head(data)

# See the last six rows
tail(data)

# Get a quick summary of structure
glimpse(data)

# See summary statistics for each column
summary(data)

# How many rows?
nrow(data)

# How many columns?
ncol(data)

# What are the column names?
names(data)
```

Get in the habit of running these commands immediately after loading data. They'll save you hours of confusion later.

## The Pipe: Your New Best Friend

The tidyverse uses an operator that will seem weird at first but will quickly become natural: the pipe. It looks like `|>` (or `%>%` in older code - they work the same).

The pipe means "take the output of this thing and pass it to the next thing." It lets you chain operations together in a way that reads like English:

```{r, eval=FALSE}
data |>
  filter(year == 2024) |>      # Start with data, keep only 2024
  select(department, overtime) |>  # Then select these columns
  arrange(desc(overtime))      # Then sort by overtime, high to low
```

Read that as: "Take the data, AND THEN filter for 2024, AND THEN select department and overtime columns, AND THEN sort by overtime descending."

This is much clearer than the old way of nesting functions inside each other. The pipe lets you build complex analysis step by step.

## The Five Essential Operations

Almost everything in data journalism involves five operations. Master these and you can do 90% of data analysis:

### 1. Filter: Keep Rows That Match Conditions

```{r, eval=FALSE}
# Keep only rows where overtime is over $50,000
data |> filter(overtime > 50000)

# Keep only police department
data |> filter(department == "Police")

# Keep rows meeting multiple conditions
data |> filter(year == 2024 & overtime > 50000)
```

### 2. Select: Pick Which Columns to Keep

```{r, eval=FALSE}
# Keep only these columns
data |> select(officer_name, department, overtime)

# Drop specific columns
data |> select(-badge_number, -home_address)

# Select columns by pattern
data |> select(starts_with("overtime"))
```

### 3. Mutate: Create New Columns

```{r, eval=FALSE}
# Create a new column
data |> mutate(overtime_per_hour = overtime / hours_worked)

# Create multiple columns
data |>
  mutate(
    total_pay = base_salary + overtime,
    overtime_percent = (overtime / base_salary) * 100
  )
```

### 4. Arrange: Sort Rows

```{r, eval=FALSE}
# Sort by overtime, low to high
data |> arrange(overtime)

# Sort by overtime, high to low
data |> arrange(desc(overtime))

# Sort by multiple columns
data |> arrange(department, desc(overtime))
```

### 5. Summarize: Calculate Statistics

```{r, eval=FALSE}
# Calculate summary statistics
data |>
  summarize(
    total_overtime = sum(overtime),
    avg_overtime = mean(overtime),
    max_overtime = max(overtime),
    count = n()  # n() counts rows
  )
```

But `summarize()` really shines when you combine it with `group_by()`:

```{r, eval=FALSE}
# Calculate overtime by department
data |>
  group_by(department) |>
  summarize(
    total_overtime = sum(overtime),
    avg_overtime = mean(overtime),
    officer_count = n()
  )
```

This is journalism gold. "Which department spent the most on overtime?" Answer: one line of code.

## Dealing with Real-World Messiness

Government data is messy. Column names have spaces. Values are inconsistent. You need tools to clean this up.

Enter the `janitor` package:

```{r, eval=FALSE}
library(janitor)

# Clean column names automatically
data <- data |> clean_names()
```

This converts "Officer Name" to "officer_name", "Overtime $$" to "overtime", and "Dept." to "dept". One function, instantly cleaner data.

## The Critical Concept: Rates, Not Counts

Let's say you're comparing crime across cities. City A has 1,000 crimes. City B has 500 crimes. City A is more dangerous, right?

Wrong. Maybe.

City A might have 1 million people. City B might have 10,000 people. To make fair comparisons, you need **rates** - counts divided by population:

```
rate = (count / population) × standard_unit
```

That standard unit is usually:
- 100,000 for crime rates ("per 100,000 people")
- 1,000 for birth/death rates
- 10,000 for some health data

Here's how you calculate it:

```{r, eval=FALSE}
data |>
  mutate(crime_rate = (total_crimes / population) * 100000)
```

Now City A's rate is 100 per 100,000 people (1,000 / 1,000,000 × 100,000). City B's rate is 5,000 per 100,000 people (500 / 10,000 × 100,000). City B is actually **50 times more dangerous**.

This mistake happens in news stories every day. Don't be that journalist.

## A Simple Complete Example

Let's put it all together. You have police overtime data and you want to know: Which departments have the highest average overtime costs?

```{r, eval=FALSE}
overtime_data <- read_csv("police_overtime.csv") |>
  clean_names() |>
  filter(!is.na(overtime)) |>  # Remove rows with missing overtime values
  group_by(department) |>
  summarize(
    total_officers = n(),
    avg_overtime = mean(overtime),
    total_overtime = sum(overtime)
  ) |>
  arrange(desc(avg_overtime))

# Look at the results
head(overtime_data, 10)
```

Six lines of code. You've loaded the data, cleaned the names, removed bad values, grouped by department, calculated statistics, and sorted the results.

That's the story. "Police officers in the Narcotics Division averaged $45,000 in overtime last year, more than any other department, according to records obtained by The Daily Times."

## Document Everything

Add comments to your code. Your future self (and your editor) will thank you:

```{r, eval=FALSE}
# Load 2024 police overtime data
overtime_data <- read_csv("police_overtime_2024.csv") |>
  clean_names() |>

  # Remove rows where overtime is missing or negative (data errors)
  filter(!is.na(overtime) & overtime >= 0) |>

  # Keep only sworn officers (exclude civilians)
  filter(employee_type == "Sworn") |>

  # Calculate average overtime by department
  group_by(department) |>
  summarize(
    officers = n(),
    avg_overtime = mean(overtime),
    total_overtime = sum(overtime)
  ) |>

  # Sort by highest average first
  arrange(desc(avg_overtime))
```

Comments start with `#`. Use them liberally. Explain why you're doing something, not just what you're doing.

## Common Beginner Mistakes

Everyone makes these mistakes. Learn from them now:

**Comparing raw counts instead of rates**: "City A has more crime than City B" → Wrong if populations differ

**Not checking for missing values**: R will often silently ignore NA values, giving you wrong results

**Forgetting to use assignment**: `data |> filter(year == 2024)` doesn't save the result. You need: `data <- data |> filter(year == 2024)`

**Overwriting original data**: Save filtered/modified data with a new name: `data_2024 <- data |> filter(year == 2024)`

**Assuming data is correct**: Always sanity-check your results. If the average overtime is $5 million, something's wrong.

## What You've Learned

You now know:
- How to load data into R
- The five essential data operations (filter, select, mutate, arrange, summarize)
- How to use the pipe to chain operations
- Why rates matter more than raw counts
- How to clean messy column names
- How to document your work with comments

This is your foundation. Every analysis in this book builds on these basics.

## Try It Yourself

The best way to learn is by doing. Here's a challenge:

1. Find a CSV file - any CSV file. Government data, sports statistics, whatever interests you.
2. Load it into R
3. Use `glimpse()` to explore it
4. Use `select()` to keep only three interesting columns
5. Use `filter()` to keep only rows that meet some condition
6. Use `arrange()` to sort by one column
7. Add comments explaining what you did

Do this a few times and the commands will become second nature.

## Looking Ahead

You have the basics. Now we'll apply them to real journalism problems:

- Calculating crime rates (Chapter 3)
- Understanding samples and populations (Chapter 4)
- Finding outliers with z-scores (Chapter 5)
- Measuring relationships with correlation (Chapter 6)
- Building regression models (Chapter 7)

Each chapter will build on what you've learned here. The code will get more sophisticated, but the fundamentals stay the same: load data, look at it, manipulate it, analyze it, find the story.

Welcome to data journalism. Let's get to work.
