# Survey Weighting: Making Samples Representative

The press release from the polling firm hits your inbox three weeks before Election Day: "Exclusive Poll: Mayor Johnson leads 54-43!" Your editor wants it for the front page.

But you dig into the methodology section buried at the bottom: "Survey of 823 likely voters." You call the polling firm for details. How did they define "likely voters"? How did they reach them?

The pollster explains: "We called landlines during business hours." You pause. Who has landlines anymore? Who's home on a Tuesday afternoon? Retirees, mostly. Young people have cell phones. Working people are at their jobs.

Your sample is skewed. It over-represents older people and under-represents younger ones. If those groups vote differentlyâ€”and they almost always doâ€”your poll doesn't represent the electorate. It represents the people you happened to reach.

This is where **survey weighting** comes in. It's a mathematical fix for an inevitable problem: your sample never perfectly matches the population you're trying to understand.

## The Fundamental Problem

Real-world surveys always have the same challenge: some types of people respond more than others.

**Who over-responds:**
- Older people (they answer phones)
- Women (higher response rates)
- College graduates (they like surveys)
- White people (demographic reality of sampling frames)
- Politically engaged people (they care about polls)

**Who under-responds:**
- Young people (ignore calls, use cell phones exclusively)
- Minorities (harder to reach, lower trust in institutions)
- Low-income residents (multiple jobs, no time)
- Renters who move frequently (outdated contact info)

If you just count responses, you're hearing disproportionately from some groups and not enough from others. And if those groups have different opinionsâ€”they usually doâ€”your results are biased.

## What Weighting Does

Survey weighting adjusts the "volume" on different voices to match the population.

**Simple example:**
- Your sample: 70% women, 30% men
- Actual population: 50% women, 50% men
- Solution: Give each woman's response less weight (count as ~0.71 people), give each man's response more weight (count as ~1.67 people)

Now your weighted sample reflects the actual gender split, even though your raw sample didn't.

## How Weights Work

Each survey respondent gets a **weight** that shows how many people they represent:

- **Weight = 1.0**: This person represents exactly themselves (properly represented in sample)
- **Weight = 0.5**: This person represents half a person (their group is over-represented)
- **Weight = 2.0**: This person represents two people (their group is under-represented)

When analyzing the data, you sum weights instead of counting people.

## Setting Up

```{r setup, message=FALSE}
library(tidyverse)
library(knitr)
```

## Working with Real Survey Data

Let's use data from a 2020 survey about voting behavior. The survey asked people if they planned to vote, and pollsters weighted responses to match population demographics:

```{r load-data, message=FALSE}
nonvoters_data <- read_csv("https://raw.githubusercontent.com/dwillis/jour405_files/refs/heads/main/nonvoters_data.csv")

glimpse(nonvoters_data)
```

**Key variables:**
- `weight`: The survey weight for each respondent
- `Q21`: Voting intention (1=Yes, 2=No, 3=Unsure)
- `voter_category`: Past voting history
- Demographics: race, gender, education, income

## Examining the Weights

Let's look at how the weights are distributed:

```{r examine-weights, fig.width=10, fig.height=6}
# Summary statistics
summary(nonvoters_data$weight)

# Histogram of weights
ggplot(nonvoters_data, aes(x = weight)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "white") +
  geom_vline(xintercept = 1.0, color = "red", linetype = "dashed", linewidth = 1) +
  theme_minimal() +
  labs(
    title = "Distribution of Survey Weights",
    subtitle = "Red line shows weight = 1.0 (properly represented); most cluster near this",
    x = "Weight",
    y = "Number of Respondents"
  )
```

**What this tells us:**
- Most weights cluster around 1.0 (sample reasonably represents these people)
- Some weights are as low as 0.2 (very over-represented)
- Some weights are as high as 3.0 (very under-represented)

## Interpreting Specific Weights

Here's what different weights mean:

```{r weight-examples}
weight_examples <- tibble(
  Weight = c(0.25, 0.50, 1.00, 1.50, 2.50),
  Interpretation = c(
    "Counts as 1/4 person (group very overrepresented in sample)",
    "Counts as 1/2 person (group moderately overrepresented)",
    "Counts as exactly 1 person (group properly represented)",
    "Counts as 1.5 people (group moderately underrepresented)",
    "Counts as 2.5 people (group very underrepresented)"
  )
)

kable(weight_examples, caption = "What different weights mean")
```

## The Critical Comparison: Weighted vs. Unweighted

Let's analyze the same question two ways and see how much weighting matters.

**Question:** "Do you plan to vote in the upcoming election?"

### Method 1: Unweighted (Just Count People)

This is what you'd get if you ignored the weightsâ€”just tally up responses:

```{r unweighted-analysis}
# Overall voting intentions, unweighted
unweighted_overall <- nonvoters_data |>
  filter(!is.na(Q21), Q21 > 0) |>
  count(Q21) |>
  mutate(
    percentage = n / sum(n) * 100,
    response = case_when(
      Q21 == 1 ~ "Yes - Plans to Vote",
      Q21 == 2 ~ "No - Doesn't Plan to Vote",
      Q21 == 3 ~ "Unsure/Undecided"
    )
  ) |>
  select(response, n, percentage)

kable(unweighted_overall, digits = 1,
      caption = "UNWEIGHTED: Simply counting respondents")
```

### Method 2: Weighted (Sum the Weights)

Now let's do it rightâ€”sum the weights instead of counting people:

```{r weighted-analysis}
# Overall voting intentions, weighted
weighted_overall <- nonvoters_data |>
  filter(!is.na(Q21), Q21 > 0) |>
  group_by(Q21) |>
  summarize(weighted_n = sum(weight)) |>
  mutate(
    percentage = weighted_n / sum(weighted_n) * 100,
    response = case_when(
      Q21 == 1 ~ "Yes - Plans to Vote",
      Q21 == 2 ~ "No - Doesn't Plan to Vote",
      Q21 == 3 ~ "Unsure/Undecided"
    )
  ) |>
  select(response, weighted_n, percentage)

kable(weighted_overall, digits = 1,
      caption = "WEIGHTED: Summing weights to reflect actual population")
```

### The Difference

Let's compare them side by side:

```{r compare-overall}
comparison <- unweighted_overall |>
  left_join(weighted_overall, by = "response", suffix = c("_unweighted", "_weighted")) |>
  mutate(difference = percentage_weighted - percentage_unweighted) |>
  select(response, percentage_unweighted, percentage_weighted, difference)

kable(comparison, digits = 1,
      col.names = c("Response", "Unweighted %", "Weighted %", "Difference"),
      caption = "Impact of weighting on poll results")
```

Notice the shifts! They might seem smallâ€”2-3 percentage pointsâ€”but in close elections, that's the difference between calling a race and saying it's too close to call.

## Breaking Down by Voter Type

Let's see how weighting affects different categories of voters. Some groups might be more skewed than others.

### Unweighted Results by Category

```{r unweighted-by-category}
unweighted_by_voter <- nonvoters_data |>
  filter(!is.na(Q21), Q21 > 0) |>
  group_by(voter_category, Q21) |>
  summarize(count = n(), .groups = "drop_last") |>
  mutate(
    total = sum(count),
    percentage = count / total * 100
  ) |>
  ungroup()

unweighted_summary <- unweighted_by_voter |>
  pivot_wider(
    id_cols = voter_category,
    names_from = Q21,
    values_from = percentage,
    names_prefix = "Q21_"
  ) |>
  rename(
    "Yes (%)" = Q21_1,
    "No (%)" = Q21_2,
    "Unsure (%)" = Q21_3
  )

kable(unweighted_summary, digits = 1,
      caption = "UNWEIGHTED: Voting intentions by past voting behavior")
```

### Weighted Results by Category

```{r weighted-by-category}
weighted_by_voter <- nonvoters_data |>
  filter(!is.na(Q21), Q21 > 0) |>
  group_by(voter_category, Q21) |>
  summarize(weighted_count = sum(weight), .groups = "drop_last") |>
  mutate(
    weighted_total = sum(weighted_count),
    percentage = weighted_count / weighted_total * 100
  ) |>
  ungroup()

weighted_summary <- weighted_by_voter |>
  pivot_wider(
    id_cols = voter_category,
    names_from = Q21,
    values_from = percentage,
    names_prefix = "Q21_"
  ) |>
  rename(
    "Yes (%)" = Q21_1,
    "No (%)" = Q21_2,
    "Unsure (%)" = Q21_3
  )

kable(weighted_summary, digits = 1,
      caption = "WEIGHTED: Voting intentions by past voting behavior")
```

### Where Weighting Made the Biggest Difference

```{r category-differences}
differences <- unweighted_summary |>
  inner_join(weighted_summary, by = "voter_category", suffix = c("_unwtd", "_wtd")) |>
  mutate(
    yes_diff = `Yes (%)_wtd` - `Yes (%)_unwtd`,
    no_diff = `No (%)_wtd` - `No (%)_unwtd`,
    unsure_diff = `Unsure (%)_wtd` - `Unsure (%)_unwtd`
  ) |>
  select(voter_category, yes_diff, no_diff, unsure_diff)

kable(differences, digits = 1,
      col.names = c("Voter Category", "Yes (Î”)", "No (Î”)", "Unsure (Î”)"),
      caption = "Percentage point differences: Weighted minus Unweighted")
```

**Interpreting these numbers:**
- **Positive**: Weighted percentage is higher (group was under-represented in raw sample)
- **Negative**: Weighted percentage is lower (group was over-represented in raw sample)
- **Large absolute values**: Sample was very unrepresentative for this group

## Visualizing the Impact

Let's see this graphically:

```{r viz-comparison, fig.width=12, fig.height=6}
# Combine weighted and unweighted for visualization
combined <- bind_rows(
  unweighted_summary |> mutate(method = "Unweighted"),
  weighted_summary |> mutate(method = "Weighted")
) |>
  pivot_longer(
    cols = c("Yes (%)", "No (%)", "Unsure (%)"),
    names_to = "response",
    values_to = "percentage"
  )

ggplot(combined, aes(x = voter_category, y = percentage, fill = response)) +
  geom_col(position = "dodge", alpha = 0.8) +
  facet_wrap(~method) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(
    title = "Impact of Survey Weighting on Voting Intentions",
    subtitle = "Notice the shiftsâ€”especially among infrequent voters",
    x = "Voter Category",
    y = "Percentage",
    fill = "Response"
  ) +
  scale_fill_brewer(palette = "Set2")
```

The shifts might look subtle, but they're real and consequential.

## When Weighting Matters Most

Weighting has the biggest effect when:

**1. The sample is badly skewed**: You have too many of one demographic, too few of another

**2. Groups differ in their views**: The over-represented group has different opinions than the under-represented group

**3. Margins are close**: A 2-point shift can flip a "lead" into a "tie"

**4. Demographics correlate with outcomes**: Age predicts voting behavior, education predicts policy views, etc.

Weighting matters less when:
- Sample already matches population well
- All demographic groups have similar views
- Margins are very wide (60-30, not 48-46)

## Evaluating Polls as a Journalist

When a poll lands on your desk, here's your checklist:

**Ask these questions:**

âœ“ **Was the data weighted?** If not, be skeptical. Unweighted polls are almost never representative.

âœ“ **What variables were used for weighting?** Age, race, education, gender, past voting? More is usually better.

âœ“ **How large are the weights?** If max weight > 3.0, the sample was very unrepresentative. That's a red flag.

âœ“ **Is methodology disclosed?** Reputable pollsters explain their weighting in detail. Secrecy is a bad sign.

âœ“ **What was the response rate?** If only 5% of people respond, even perfect weighting can't fix a fundamentally biased sample.

**Red flags:**

ðŸš© No mention of weighting at all

ðŸš© Extreme weights (>5.0) suggest the sample is terrible

ðŸš© Weighting on the outcome itself (circular reasoning)

ðŸš© Vague methodology ("results were adjusted to reflect population")

ðŸš© Online opt-in polls with no probability sample

## How to Report Weighted Results

You've decided the poll is credible. How do you write about it?

**Bad version:**
"A new poll shows Johnson leading 54-43."

**Better version:**
"A new poll of 1,247 likely voters shows Mayor Johnson leading challenger Smith 54% to 43%, according to results weighted to match county demographics.

The poll, conducted by Reliable Polling Inc., contacted voters via a mix of landlines and cell phones. Results were weighted to reflect the county's age, gender, education, and racial composition, as well as past voting patterns.

Before weighting, Johnson's lead was 5 points wider, but adjustments to account for an overrepresentation of voters over age 65 narrowed the margin. The poll has a margin of error of Â±3.2 percentage points."

**What this includes:**
- Sample size (1,247)
- How they were reached (landlines + cell phones)
- What variables were weighted (age, gender, education, race, past voting)
- That weighting changed results (5-point shift)
- Margin of error

## When Weighting Goes Wrong

Weighting can fix some problems, but it has limits:

**Problem 1: Over-reliance on small groups**

If you have only 10 respondents from a group that should be 15% of the sample, each one gets a weight around 1.5. But those 10 people might not represent their group well. You're amplifying a potentially unrepresentative subsample.

**Problem 2: Hiding a fundamentally bad sample**

Extreme weights (>3.0) mean your sample is so skewed that weighting is duct tape on a broken machine. You can't fix a landline-only survey by giving young people 5x weightsâ€”you simply didn't reach young people.

**Problem 3: Model dependency**

Different weighting schemes produce different results. Weight on age and gender? One answer. Add education and race? Different answer. This is why methodology mattersâ€”it determines the outcome.

## The Bottom Line

Survey weighting is a necessary tool, not a suspicious trick. Every reputable poll uses it because samples are never perfect.

**As a journalist, you need to:**
- Understand why weighting exists (samples are always biased)
- Know how to interpret weighted results (sum weights, don't count people)
- Evaluate whether weighting was done well (check methodology)
- Report it transparently (explain what was weighted and how)
- Recognize its limits (can't fix a terrible sample)

**Key principles:**
- Weights < 1: Group is over-represented in sample
- Weights > 1: Group is under-represented in sample
- Large weights (>3): Sample quality is questionable
- Always use weighted results when analyzing survey data
- Always report that results are weighted

## Your Turn

Test your understanding:

1. A survey over-samples college graduates (40% of sample, but only 30% of population). Should their weights be above or below 1.0? By approximately how much?

2. You see a poll where one respondent has a weight of 8.5. What does this tell you about the sample quality?

3. Find a recent poll in the news. Does it report weighting methodology? What variables does it weight on? Are there any red flags?

## Looking Ahead

We've covered how to make a single sample representative through weighting. But what if you want to understand how multiple factors simultaneously affect an outcome? That's where **multiple regression** comes inâ€”our final chapter. It's regression (which we learned in Chapter 7) but with many predictors at once, letting you control for multiple confounding variables simultaneously.

For now, remember: weighted surveys are good. Unweighted surveys are misleading. And surveys with terrible samples can't be saved by any amount of weighting.
