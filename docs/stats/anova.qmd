# ANOVA: Comparing Multiple Groups

The city councilman leans forward at the budget hearing: "We need more police in the Fourth District. Crime there is out of control—way worse than the other districts." He's got numbers: District Four had 847 reported crimes last year. District Two had 612. District Three had 701.

You're covering the hearing, and you need to know: Is District Four genuinely different, or is this just normal variation? And what about all five districts—are any of them truly outliers?

You could run t-tests comparing every pair: District 1 vs. District 2, District 1 vs. District 3, District 2 vs. District 3... but that's 10 different tests. And the more tests you run, the more likely you get false positives just by chance.

This is where **ANOVA** (Analysis of Variance) comes in. It tests whether multiple groups differ—all at once, in one test—while controlling for the increased risk of false positives.

## The Multiple Comparisons Problem

Let's say you're testing whether five districts have different crime rates. If you use t-tests, you'd need to compare:
- District 1 vs. 2
- District 1 vs. 3
- District 1 vs. 4
- District 1 vs. 5
- District 2 vs. 3
- District 2 vs. 4
- District 2 vs. 5
- District 3 vs. 4
- District 3 vs. 5
- District 4 vs. 5

That's 10 tests. Each test has a 5% chance of a false positive (p < 0.05). Run 20 such tests, and you'll probably get one "significant" result purely by chance, even if all districts are actually the same.

**ANOVA solves this.** It tests all groups at once, asking: "Is there significant variation among these groups?" If yes, then you can dig deeper to find which specific pairs differ.

## What ANOVA Actually Does

ANOVA compares two types of variation:

**Between-group variance**: How much the group averages differ from each other

**Within-group variance**: How much individual observations vary within each group

If the between-group variance is much larger than the within-group variance, the groups are probably different.

The test statistic is the **F-statistic**:

```
F = Between-group variance / Within-group variance
```

- **Large F**: Groups differ substantially (relative to noise within groups)
- **Small F**: Groups are similar (differences are just noise)
- **p-value**: Probability this F-value happened by chance

## Setting Up

```{r setup, message=FALSE}
library(tidyverse)
```

## A Concrete Example: College SAT Scores

You're reporting on higher education, and a source claims: "Community colleges, public universities, and private colleges serve completely different student populations." You want to test this with admissions data.

Let's look at SAT scores across three types of institutions. For this example, we'll use simulated data based on realistic distributions:

```{r create-data}
# Set seed for reproducibility
set.seed(123)

# Create simulated SAT scores (400-1600 scale)
# Based on realistic averages for each type
public_college <- rnorm(100, mean = 1350, sd = 120)
private_college <- rnorm(100, mean = 1450, sd = 100)
community_college <- rnorm(100, mean = 1050, sd = 150)

# Combine into a single dataframe
admissions_data <- tibble(
  college_type = factor(rep(c("Public", "Private", "Community"), each = 100)),
  sat_score = c(public_college, private_college, community_college)
)

# Ensure scores are in realistic range
admissions_data <- admissions_data |>
  mutate(sat_score = pmin(pmax(sat_score, 400), 1600))

head(admissions_data)
```

## Looking at the Numbers First

Before running any test, examine the descriptive statistics:

```{r summary-stats}
admission_summary <- admissions_data |>
  group_by(college_type) |>
  summarize(
    n = n(),
    mean_score = mean(sat_score),
    median_score = median(sat_score),
    sd_score = sd(sat_score),
    min_score = min(sat_score),
    max_score = max(sat_score)
  )

knitr::kable(admission_summary, digits = 1, caption = "SAT scores by college type")
```

**Observations:**
- Private colleges have the highest average (around 1,450)
- Community colleges have the lowest (around 1,050)
- Public colleges fall in between (around 1,350)

But are these differences **statistically significant**, or could they be random variation?

## Visualizing the Distributions

Let's see what the distributions look like:

```{r density-plot, fig.width=10, fig.height=6}
ggplot(admissions_data, aes(x = sat_score, fill = college_type)) +
  geom_density(alpha = 0.5) +
  scale_fill_brewer(palette = "Set1") +
  theme_minimal() +
  labs(
    title = "SAT Score Distributions by College Type",
    subtitle = "Clear separation suggests real differences, but let's test statistically",
    x = "SAT Score (400-1600 scale)",
    y = "Density",
    fill = "College Type"
  )
```

The curves barely overlap. Private colleges cluster high, community colleges cluster low. This looks like real differences—but let's quantify it.

```{r boxplot, fig.width=10, fig.height=6}
ggplot(admissions_data, aes(x = college_type, y = sat_score, fill = college_type)) +
  geom_boxplot(alpha = 0.7) +
  geom_jitter(width = 0.2, alpha = 0.1, size = 1) +
  scale_fill_brewer(palette = "Set1") +
  theme_minimal() +
  labs(
    title = "SAT Scores by College Type",
    subtitle = "Boxplots show medians and spread",
    x = "College Type",
    y = "SAT Score",
    fill = "College Type"
  )
```

The boxplots show clear separation. The highest community college scores barely reach the lowest private college scores. But eyeballing isn't enough—we need statistics.

## Running the ANOVA Test

Here's the test that answers: "Do these groups differ significantly?"

```{r anova-test}
# Perform one-way ANOVA
anova_result <- aov(sat_score ~ college_type, data = admissions_data)

# View the results
summary(anova_result)
```

Let's interpret this output:

**Df (Degrees of Freedom)**: Number of groups minus 1 (we have 3 groups, so df = 2)

**Sum Sq (Sum of Squares)**: Total variation explained by college type

**Mean Sq (Mean Square)**: Sum Sq divided by df

**F value**: The test statistic. This is huge—between-group variance is vastly larger than within-group variance.

**Pr(>F)**: The p-value. If this is less than 0.05, the groups differ significantly.

Let's extract and interpret:

```{r interpret-anova}
f_value <- summary(anova_result)[[1]]$`F value`[1]
p_value <- summary(anova_result)[[1]]$`Pr(>F)`[1]

cat("F-statistic:", round(f_value, 2), "\n")
cat("P-value:", format(p_value, scientific = TRUE), "\n\n")

if (p_value < 0.001) {
  cat("HIGHLY SIGNIFICANT (p < 0.001)\n")
  cat("At least one college type has significantly different SAT scores.\n")
} else if (p_value < 0.05) {
  cat("SIGNIFICANT (p < 0.05)\n")
  cat("At least one college type differs.\n")
} else {
  cat("NOT SIGNIFICANT (p >= 0.05)\n")
  cat("No significant differences detected.\n")
}
```

With such a tiny p-value, we can confidently say: **College types have significantly different SAT scores.**

But which specific pairs differ? ANOVA doesn't tell us that—it only says "at least one group is different." We need a follow-up test.

## Post-Hoc Tests: Which Pairs Differ?

Once ANOVA shows significance, we use **Tukey's HSD (Honestly Significant Difference)** test to compare every pair while controlling for multiple comparisons:

```{r tukey}
# Tukey's HSD test
tukey_result <- TukeyHSD(anova_result)
print(tukey_result)
```

This shows every pairwise comparison:

**diff**: The difference in means between the two groups

**lwr, upr**: 95% confidence interval for that difference

**p adj**: Adjusted p-value (corrected for multiple comparisons)

Let's make this more readable:

```{r tukey-interpret}
tukey_df <- as.data.frame(tukey_result$college_type)

cat("Pairwise Comparisons:\n\n")
for(i in 1:nrow(tukey_df)) {
  comparison <- rownames(tukey_df)[i]
  diff <- tukey_df$diff[i]
  p_val <- tukey_df$`p adj`[i]

  cat(comparison, "\n")
  cat("  Difference:", round(diff, 1), "points\n")
  cat("  P-value:", format(p_val, scientific = TRUE), "\n")

  if (p_val < 0.001) {
    cat("  Result: HIGHLY SIGNIFICANT ***\n\n")
  } else if (p_val < 0.05) {
    cat("  Result: Significant *\n\n")
  } else {
    cat("  Result: Not significant\n\n")
  }
}
```

**All three comparisons are significant!** Every college type differs from every other type.

## Visualizing the Post-Hoc Results

Let's visualize these pairwise differences:

```{r tukey-viz, fig.width=10, fig.height=6}
# Extract Tukey results
tukey_df_plot <- as.data.frame(tukey_result$college_type) |>
  rownames_to_column("comparison")

ggplot(tukey_df_plot, aes(x = comparison, y = diff)) +
  geom_point(size = 4, color = "steelblue") +
  geom_errorbar(aes(ymin = lwr, ymax = upr), width = 0.2, linewidth = 1) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  coord_flip() +
  theme_minimal() +
  labs(
    title = "Pairwise Differences in SAT Scores",
    subtitle = "95% confidence intervals; if interval crosses zero (red line), not significant",
    x = "Comparison",
    y = "Difference in Mean SAT Score"
  )
```

None of the confidence intervals cross zero. Every difference is statistically significant.

## Checking Assumptions

ANOVA assumes:

1. **Independence**: Each observation is independent (students don't influence each other's scores)
2. **Normality**: Data in each group is roughly normally distributed
3. **Homogeneity of variance**: Groups have similar spreads (standard deviations)

Let's check normality with Q-Q plots:

```{r normality-check, fig.width=10, fig.height=4}
# Q-Q plot for each group
ggplot(admissions_data, aes(sample = sat_score)) +
  stat_qq() +
  stat_qq_line(color = "red") +
  facet_wrap(~college_type) +
  theme_minimal() +
  labs(
    title = "Q-Q Plots: Checking Normality Assumption",
    subtitle = "Points should follow red line if data is normally distributed"
  )
```

If points roughly follow the red line, normality assumption is met. Deviations at the ends are normal and usually okay with large samples.

Now check variance homogeneity:

```{r variance-check}
# Levene's test for equal variances
levene_test <- car::leveneTest(sat_score ~ college_type, data = admissions_data)
print(levene_test)

if (levene_test$`Pr(>F)`[1] > 0.05) {
  cat("\nVariances are similar (p > 0.05). Homogeneity assumption met.\n")
} else {
  cat("\nVariances differ (p < 0.05). Consider Welch's ANOVA instead.\n")
}
```

If p > 0.05, variances are similar enough. If p < 0.05, the groups have substantially different spreads, and you might need Welch's ANOVA (a robust alternative).

## Reporting the Findings

You're back at your desk. How do you write this story?

**Bad version:**
"Private colleges have higher SAT scores than community colleges."

**Better version:**
"Private colleges enroll students with significantly higher SAT scores than both public universities and community colleges, according to an analysis of admissions data from 300 institutions.

Private college students averaged 1,450 on the SAT, compared to 1,350 for public universities and 1,050 for community colleges. All pairwise differences were statistically significant (ANOVA: F = 284.3, p < 0.001).

The 400-point gap between private and community colleges represents roughly one standard deviation in the national SAT distribution, suggesting substantially different student populations.

Dr. Jane Smith, a higher education policy expert at State University, noted that these differences reflect both admissions selectivity and socioeconomic factors. 'Private colleges can afford to be more selective, and they also tend to draw from wealthier families whose children have access to test prep and strong high schools,' she said."

Notice what this includes:
- **The actual numbers**: 1,450 vs. 1,350 vs. 1,050
- **Statistical significance**: F-statistic and p-value (in parentheses, not the lede)
- **Effect size**: "400-point gap" and "one standard deviation"
- **Context**: What this means practically
- **Expert voice**: Interpretation from a subject matter expert

## When to Use ANOVA

**Use ANOVA when:**
- You have 3 or more groups to compare
- Data is roughly normally distributed (or you have large samples)
- Groups have similar variances
- You want to control for multiple comparison problems

**Don't use ANOVA when:**
- You only have 2 groups (use a t-test—it's simpler)
- Data is severely non-normal with small samples (use non-parametric alternatives like Kruskal-Wallis)
- Variances are wildly different (use Welch's ANOVA)
- Your outcome isn't continuous (use chi-square for categorical data)

## Back to the Police Districts

Remember that city council hearing? Let's say you got crime data for all five police districts. Here's how you'd analyze it:

```{r districts-example, eval=FALSE}
# Load district crime data
crime_data <- read_csv("district_crimes.csv")

# Test if districts differ significantly
crime_anova <- aov(crime_rate ~ district, data = crime_data)
summary(crime_anova)

# If significant, find which districts differ
TukeyHSD(crime_anova)
```

If the ANOVA is significant, you'd write: "Crime rates vary significantly across the city's five police districts (F = X.XX, p < 0.01)."

Then Tukey's test tells you which specific pairs differ: "District Four's crime rate (84.7 per 1,000 residents) is significantly higher than Districts One, Two, and Three, but not significantly different from District Five."

Now the councilman's claim is either supported or debunked—with statistical rigor.

## Common Pitfalls

**Don't fish for significance**: If you test 20 different groupings hoping one is significant, you're p-hacking. Decide what to test before you see the data.

**Don't stop at the ANOVA**: ANOVA only says "groups differ." You must do post-hoc tests to find which ones.

**Don't ignore assumptions**: If variances differ wildly or data is severely non-normal, use robust alternatives.

**Don't confuse statistical and practical significance**: p < 0.001 doesn't mean the difference matters in the real world. Always report effect sizes.

**Don't report F-statistics in the lede**: Save technical details for later. Lead with the newsworthy finding.

## Beyond One-Way ANOVA

We've covered **one-way ANOVA**: one grouping variable (college type, police district, etc.).

More complex designs exist:

**Two-way ANOVA**: Two grouping variables. Example: Do SAT scores differ by college type AND geographic region?

**Repeated measures ANOVA**: Same subjects measured multiple times. Example: Do students' test scores change across freshman, sophomore, junior, and senior years?

**MANOVA**: Multiple outcome variables. Example: Do colleges differ in both SAT and GPA simultaneously?

These are beyond our scope, but know they exist for more complex questions.

## The Bottom Line

ANOVA is your tool for comparing multiple groups without inflating false positive rates. It tells you whether groups differ overall, then post-hoc tests identify which specific pairs are different.

Use it when:
- A politician claims one district, neighborhood, or demographic is "much worse" or "much better"
- You're comparing outcomes across multiple categories (schools, hospitals, treatments, etc.)
- You want to avoid the multiple comparisons problem

Always report:
- The actual means for each group
- Whether the overall test was significant
- Which specific pairs differ (from post-hoc tests)
- Effect sizes (how big the differences are)
- Context (what the differences mean practically)

## Your Turn

Using the college data:

1. Which comparison showed the smallest difference? Was it still statistically significant?

2. If we had 6 college types instead of 3, how many pairwise comparisons would Tukey's test perform? (Hint: it's n × (n-1) / 2)

3. Write a headline and opening paragraph reporting these findings. What's the most newsworthy angle?

ANOVA gives you the statistical firepower to evaluate claims about differences across multiple groups. Master it, and you'll catch politicians and PR departments when they cherry-pick comparisons or exaggerate differences.

Next up: we'll tackle survey weighting, because raw survey results almost never represent the population accurately without adjustment.
