[
  {
    "objectID": "writingwithdata.html",
    "href": "writingwithdata.html",
    "title": "Writing with numbers",
    "section": "",
    "text": "The number one sin of all early career data journalist is to get really, really, really attached to the analysis you’ve done and include every number you find.\nDon’t do that.\nNumbers tell you what. Numbers rarely tell you why. What question has driven most people since they were three years old? Why. The very first thing to do is realize that is the purpose of reporting. You’ve done the analysis to determine the what. Now go do the reporting to do the why. Or as an old editor of mine used to say “Now go do that reporting shit you do.”\nThe trick to writing a numbers story is to frame your story around people. Sometimes, your lead can be a number, if that number is compelling. Often, your lead is a person, a person who is one of the numbers you are writing about.\nTell their story. Briefly. Then, let us hear from them. Let them speak about what it is you are writing about.\nThen come the numbers.\n\n\nWriting complex stories is often a battle against that complexity. You don’t want to overwhelm. You want to simplify where you can. The first place you can do that is only use exact numbers where an exact number is called for.\nWhere you can, do the following:\n\nUsing ratios instead of percents\nOften, it’s better to put it in counts of 10. 6 of 10, 4 of 10. It’s easy to translate that from a percentage to a ratio.\nBut be careful when your number is 45 percent. Is that 4 in 10 or 5 in 10?\nIf a ratio doesn’t make sense, round. There’s 287,401 people in Lincoln, according to the Census Bureau. It’s easier, and no less accurate, to say there’s more than 287,000 people in Lincoln.\n\nA critical question your writing should answer: As compared to what?\nHow does this compare to the average? The state? The nation? The top? The bottom?\nOne of the most damning numbers in the series of stories Craig Pittman and I wrote that became the book Paving Paradise was comparing approvals and denials.\nWe were looking at the US Army Corps of Engineers and their permitting program. We were able to get a dataset of just a few years of permits that was relatively clean. From that, we were able to count the number of times the corps had said yes to a developer to wipe out wetlands the law protected and how many times they said no.\nThey said yes 12,000 times. They said no once.\nThat one time? Someone wanted to build an eco-lodge in the Everglades. Literally. Almost every acre of the property was wetlands. So in order to build it, the developer would have to fill in the very thing they were going to try to bring people into. The corps said no.\n\n\n\nSometimes ratios and rounding are not appropriate.\nThis is being written in the days of the coronavirus. Case counts are where an exact number is called for. You don’t say that there are more than 70 cases in Lancaster County on the day this was written. You specify. It’s 75.\nYou don’t say almost 30 deaths. It’s 28.\nWhere this also comes into play is any time there are deaths: Do not round bodies.\n\n\n\nRead this story from USA Today and the Arizona Republic. Notice first that the top sets up a conflict: People say one thing, and that thing is not true.\n\nNo one could have anticipated such a catastrophe, people said. The fire’s speed was unprecedented, the ferocity unimaginable, the devastation unpredictable.\n\n\nThose declarations were simply untrue. Though the toll may be impossible to predict, worst-case fires are a historic and inevitable fact.\n\nThe first voice you hear? An expert who studies wildfires.\n\nPhillip Levin, a researcher at the University of Washington and lead scientist for the Nature Conservancy in Washington, puts it this way: “Fire is natural. But the disaster happens because people didn’t know to leave, or couldn’t leave. It didn’t have to happen.”\n\nThen notice how they take what is a complex analysis using geographic information systems, raster analysis, the merging of multiple different datasets together and show that it’s quite simple – the averaging together of pixels on a 1-5 scale.\nThen, the compare what they found to a truly massive fire: The Paradise fire that burned 19,000 structures.\n\nAcross the West, 526 small communities — more than 10 percent of all places — rank higher.\n\nAnd that is how it’s done. Simplify, round, ratios: simple metrics, powerful results.",
    "crumbs": [
      "Writing with numbers"
    ]
  },
  {
    "objectID": "writingwithdata.html#how-to-write-about-numbers-without-overwhelming-with-numbers.",
    "href": "writingwithdata.html#how-to-write-about-numbers-without-overwhelming-with-numbers.",
    "title": "Writing with numbers",
    "section": "",
    "text": "Writing complex stories is often a battle against that complexity. You don’t want to overwhelm. You want to simplify where you can. The first place you can do that is only use exact numbers where an exact number is called for.\nWhere you can, do the following:\n\nUsing ratios instead of percents\nOften, it’s better to put it in counts of 10. 6 of 10, 4 of 10. It’s easy to translate that from a percentage to a ratio.\nBut be careful when your number is 45 percent. Is that 4 in 10 or 5 in 10?\nIf a ratio doesn’t make sense, round. There’s 287,401 people in Lincoln, according to the Census Bureau. It’s easier, and no less accurate, to say there’s more than 287,000 people in Lincoln.\n\nA critical question your writing should answer: As compared to what?\nHow does this compare to the average? The state? The nation? The top? The bottom?\nOne of the most damning numbers in the series of stories Craig Pittman and I wrote that became the book Paving Paradise was comparing approvals and denials.\nWe were looking at the US Army Corps of Engineers and their permitting program. We were able to get a dataset of just a few years of permits that was relatively clean. From that, we were able to count the number of times the corps had said yes to a developer to wipe out wetlands the law protected and how many times they said no.\nThey said yes 12,000 times. They said no once.\nThat one time? Someone wanted to build an eco-lodge in the Everglades. Literally. Almost every acre of the property was wetlands. So in order to build it, the developer would have to fill in the very thing they were going to try to bring people into. The corps said no.",
    "crumbs": [
      "Writing with numbers"
    ]
  },
  {
    "objectID": "writingwithdata.html#when-exact-numbers-matter",
    "href": "writingwithdata.html#when-exact-numbers-matter",
    "title": "Writing with numbers",
    "section": "",
    "text": "Sometimes ratios and rounding are not appropriate.\nThis is being written in the days of the coronavirus. Case counts are where an exact number is called for. You don’t say that there are more than 70 cases in Lancaster County on the day this was written. You specify. It’s 75.\nYou don’t say almost 30 deaths. It’s 28.\nWhere this also comes into play is any time there are deaths: Do not round bodies.",
    "crumbs": [
      "Writing with numbers"
    ]
  },
  {
    "objectID": "writingwithdata.html#an-example",
    "href": "writingwithdata.html#an-example",
    "title": "Writing with numbers",
    "section": "",
    "text": "Read this story from USA Today and the Arizona Republic. Notice first that the top sets up a conflict: People say one thing, and that thing is not true.\n\nNo one could have anticipated such a catastrophe, people said. The fire’s speed was unprecedented, the ferocity unimaginable, the devastation unpredictable.\n\n\nThose declarations were simply untrue. Though the toll may be impossible to predict, worst-case fires are a historic and inevitable fact.\n\nThe first voice you hear? An expert who studies wildfires.\n\nPhillip Levin, a researcher at the University of Washington and lead scientist for the Nature Conservancy in Washington, puts it this way: “Fire is natural. But the disaster happens because people didn’t know to leave, or couldn’t leave. It didn’t have to happen.”\n\nThen notice how they take what is a complex analysis using geographic information systems, raster analysis, the merging of multiple different datasets together and show that it’s quite simple – the averaging together of pixels on a 1-5 scale.\nThen, the compare what they found to a truly massive fire: The Paradise fire that burned 19,000 structures.\n\nAcross the West, 526 small communities — more than 10 percent of all places — rank higher.\n\nAnd that is how it’s done. Simplify, round, ratios: simple metrics, powerful results.",
    "crumbs": [
      "Writing with numbers"
    ]
  },
  {
    "objectID": "visualizing-for-reporting.html",
    "href": "visualizing-for-reporting.html",
    "title": "Visualizing your data for reporting",
    "section": "",
    "text": "Visualizing data is becoming a much greater part of journalism. Large news organizations are creating graphics desks that create complex visuals with data to inform the public about important events.\nTo do it well is a course on its own. And not every story needs a feat of programming and art. Sometimes, you can help yourself and your story by just creating a quick chart, which helps you see patterns in the data that wouldn’t otherwise surface.\nGood news: one of the best libraries for visualizing data is in the tidyverse and it’s pretty simple to make simple charts quickly with just a little bit of code. It’s called ggplot2.\nLet’s revisit some data we’ve used in the past and turn it into charts. First, let’s load libraries. When we load the tidyverse, we get ggplot2, and we’ll need lubridate, too.\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\nThe dataset we’ll use is 911 overdose calls from Baltimore County in 2022. Let’s load it.\n\nbaltco_911_calls &lt;- read_csv(\"data/baltco_911_calls.csv\")\n\nRows: 3822 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): address\ndbl  (1): callnumber\ndate (1): date\ntime (1): time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nThe first kind of chart we’ll create is a simple bar chart.\nIt’s a chart designed to show differences between things – the magnitude of one thing, compared to the next thing, and the next, and the next.\nSo if we have thing, like a county, or a state, or a group name, and then a count of that group, we can make a bar chart.\nSo what does the chart of the top months with the most 911 overdose calls look like?\nFirst, we’ll add a month column to our dataframe using lubridate.\n\nbaltco_911_calls_by_month &lt;- baltco_911_calls |&gt;\n  mutate(month = month(date, label=TRUE)) |&gt; \n  group_by(month) |&gt; \n  summarize(total_calls = n()) |&gt; \n  arrange(desc(total_calls))\n\nbaltco_911_calls_by_month\n\n# A tibble: 12 × 2\n   month total_calls\n   &lt;ord&gt;       &lt;int&gt;\n 1 Aug           366\n 2 May           365\n 3 Mar           364\n 4 Apr           357\n 5 Jul           339\n 6 Oct           334\n 7 Jun           314\n 8 Jan           293\n 9 Nov           293\n10 Sep           289\n11 Dec           271\n12 Feb           237\n\n\nNow let’s create a bar chart using ggplot.\nWith ggplot, the first thing we’ll always do is draw a blank canvas that will house our chart. We start with our dataframe name, and then (|&gt;) we invoke the ggplot() function to make that blank canvas. All this does is make a gray box, the blank canvas that will hold our chart.\n\nbaltco_911_calls_by_month |&gt;\n  ggplot()\n\n\n\n\n\n\n\n\nNext we need to tell ggplot what kind of chart to make.\nIn ggplot, we work with two key concepts called geometries (abbreviated frequently as geom) and aesthetics (abbreviated as aes).\nGeometries are the shape that the data will take; think of line charts, bar charts, scatterplots, histograms, pie charts and other common graphics forms.\nAsesthetics help ggplot know what component of our data to visualize – why we’ll visualize values from one column instead of another.\nIn a bar chart, we first pass in the data to the geometry, then set the aesthetic.\nIn the codeblock below, we’ve added a new function, geom_bar().\nUsing geom_bar() – as opposed to geom_line() – says we’re making a bar chart.\nInside of that function, the asthetic, aes, says which columns to use in drawing the chart.\nWe’re setting the values on the x axis (horizontal) to be the name of the county. We set weight to total loans, and it uses that value to “weight” or set the height of each bar.\nOne quirk here with ggplot.\nAfter we’ve invoked the ggplot() function, you’ll notice we’re using a + symbol. It means the same thing as |&gt; – “and then do this”. It’s just a quirk of ggplot() that after you invoke the ggplot() function, you use + instead of |&gt;. It makes no sense to me either, just something to live with.\n\nbaltco_911_calls_by_month |&gt;\n  ggplot() +\n  geom_bar(aes(x=month, weight=total_calls))\n\n\n\n\n\n\n\n\nThis is a very basic chart. But it’s hard to derive much meaning from this chart, because the counties aren’t ordered from highest to lowest by total_loans. We can fix that by using the reorder() function to do just that:\n\nbaltco_911_calls_by_month |&gt;\n  ggplot() +\n  geom_bar(aes(x=reorder(month,total_calls), weight=total_calls))\n\n\n\n\n\n\n\n\nThis is a little more useful. But the bottom is kind of a mess, with overlapping names. We can fix that by flipping it from a vertical bar chart (also called a column chart) to a horizontal one. coord_flip() does that for you.\n\nbaltco_911_calls_by_month |&gt;\n  ggplot() +\n  geom_bar(aes(x=reorder(month,total_calls), weight=total_calls)) +\n  coord_flip()\n\n\n\n\n\n\n\n\nIs this art? No. Does it quickly tell you something meaningful? It does.\nWe’re mainly going to use these charts to help us in reporting, so style isn’t that important.\nBut it’s worth mentioning that we can pretty up these charts for publication, if we wanted to, with some more code. To style the chart, we can change or even modify the “theme”, a kind of skin that makes the chart look better.\nIt’s kind of like applying CSS to html. Here I’m changing the theme slightly to remove the gray background with one of ggplot’s built in themes, theme_minimal()\n\nbaltco_911_calls_by_month |&gt;\n  ggplot() +\n  geom_bar(aes(x=reorder(month,total_calls), weight=total_calls)) +\n  coord_flip() + \n  theme_minimal()\n\n\n\n\n\n\n\n\nThe ggplot universe is pretty big, and lots of people have made and released cool themes for you to use. Want to make your graphics look kind of like The Economist’s graphics? There’s a theme for that.\nFirst, you have to install and load a package that contains lots of extra themes, called ggthemes.\n\n#install.packages('ggthemes')\nlibrary(ggthemes)\n\nAnd now we’ll apply the economist theme from that package with theme_economist()\n\nbaltco_911_calls_by_month |&gt;\n  ggplot() +\n  geom_bar(aes(x=reorder(month,total_calls), weight=total_calls)) +\n  coord_flip() + \n  theme_economist()\n\n\n\n\n\n\n\n\nThose axis titles are kind of a mess. Let’s change “count” on the x axis to “net change” and change “reorder(County,TOTAL_DIFF)” to “county”. And while we’re at it, let’s add a basic title and a source as a caption. We’ll use a new function, labs(), which is short for labels.\n\nbaltco_911_calls_by_month |&gt;\n  ggplot() +\n  geom_bar(aes(x=reorder(month,total_calls), weight=total_calls)) +\n  coord_flip() + \n  theme_economist() +\n  labs(\n    title=\"More 911 Overdose Calls in Warmer Months\",\n    x = \"month\",\n    y = \"total calls\",\n    caption = \"source: Baltimore County\"\n  )\n\n\n\n\n\n\n\n\nViola. Not super pretty, but good enough to show an editor to help them understand the conclusions you reached with your data analysis.\n\n\n\nLet’s look at how to make another common chart type that will help you understand patterns in your data.\nLine charts can show change over time. It works much the same as a bar chart, code wise, but instead of a weight, it uses a y.\n\nbaltco_911_calls_by_date &lt;- baltco_911_calls |&gt;\n  group_by(date) |&gt;\n  summarise(\n    total_calls=n()\n  )\n\nbaltco_911_calls_by_date\n\n# A tibble: 366 × 2\n   date       total_calls\n   &lt;date&gt;           &lt;int&gt;\n 1 2022-02-06           8\n 2 2022-02-07          15\n 3 2022-02-08           7\n 4 2022-02-09           5\n 5 2022-02-10           5\n 6 2022-02-11           8\n 7 2022-02-12           9\n 8 2022-02-13           7\n 9 2022-02-14          11\n10 2022-02-15           7\n# ℹ 356 more rows\n\n\nAnd now let’s make a line chart to look for patterns in this data.\nWe’ll put the date on the x axis and total calls on the y axis.\n\nbaltco_911_calls_by_date |&gt;\n  ggplot() + \n  geom_line(aes(x=date, y=total_calls))\n\n\n\n\n\n\n\n\nIt’s not super pretty, but there’s a bit of a pattern here: the number of calls fluctuates between 5 and 20 a day for most of this period, and then jumps way up at certain points during the year. In particular there are spikes in July, early October and January.\nRight now, it’s kind of hard to see specifics, though. When did some of those smaller spikes and troughs happen?\nWe can’t really tell. So let’s modify the x axis to have one tick mark and label per month. We can do that with a function called scale_x_date().\nWe’ll set the date_breaks to appear for every week; if we wanted every month, we’d say date_breaks = “1 month”. We can set the date to appear as month abbreviated name (%b) and day (%d).\n\nbaltco_911_calls_by_date |&gt;\n  ggplot() + \n  geom_line(aes(x=date, y=total_calls)) + \n  scale_x_date(date_breaks = \"1 week\", date_labels = \"%b %d\")\n\n\n\n\n\n\n\n\nThose are a little hard to read, so we can turn them 45 degrees to remove the overlap using the theme() function for styling. With “axis.text.x = element_text(angle = 45, hjust=1)” we’re saying, turn the date labels 45 degrees.\n\nbaltco_911_calls_by_date |&gt;\n  ggplot() + \n  geom_line(aes(x=date, y=total_calls)) + \n  scale_x_date(date_breaks = \"1 week\", date_labels = \"%b %d\") +\n  theme(\n    axis.text.x = element_text(angle = 45,  hjust=1)\n  )\n\n\n\n\n\n\n\n\nAgain, this isn’t as pretty as we could make it. But by charting this, we can quickly see a pattern that can help guide our reporting.\nWe’re just scratching the surface of what ggplot can do, and chart types. There’s so much more you can do, so many other chart types you can make. But the basics we’ve shown here will get you started.",
    "crumbs": [
      "Visualizing your data for reporting"
    ]
  },
  {
    "objectID": "visualizing-for-reporting.html#bar-charts",
    "href": "visualizing-for-reporting.html#bar-charts",
    "title": "Visualizing your data for reporting",
    "section": "",
    "text": "The first kind of chart we’ll create is a simple bar chart.\nIt’s a chart designed to show differences between things – the magnitude of one thing, compared to the next thing, and the next, and the next.\nSo if we have thing, like a county, or a state, or a group name, and then a count of that group, we can make a bar chart.\nSo what does the chart of the top months with the most 911 overdose calls look like?\nFirst, we’ll add a month column to our dataframe using lubridate.\n\nbaltco_911_calls_by_month &lt;- baltco_911_calls |&gt;\n  mutate(month = month(date, label=TRUE)) |&gt; \n  group_by(month) |&gt; \n  summarize(total_calls = n()) |&gt; \n  arrange(desc(total_calls))\n\nbaltco_911_calls_by_month\n\n# A tibble: 12 × 2\n   month total_calls\n   &lt;ord&gt;       &lt;int&gt;\n 1 Aug           366\n 2 May           365\n 3 Mar           364\n 4 Apr           357\n 5 Jul           339\n 6 Oct           334\n 7 Jun           314\n 8 Jan           293\n 9 Nov           293\n10 Sep           289\n11 Dec           271\n12 Feb           237\n\n\nNow let’s create a bar chart using ggplot.\nWith ggplot, the first thing we’ll always do is draw a blank canvas that will house our chart. We start with our dataframe name, and then (|&gt;) we invoke the ggplot() function to make that blank canvas. All this does is make a gray box, the blank canvas that will hold our chart.\n\nbaltco_911_calls_by_month |&gt;\n  ggplot()\n\n\n\n\n\n\n\n\nNext we need to tell ggplot what kind of chart to make.\nIn ggplot, we work with two key concepts called geometries (abbreviated frequently as geom) and aesthetics (abbreviated as aes).\nGeometries are the shape that the data will take; think of line charts, bar charts, scatterplots, histograms, pie charts and other common graphics forms.\nAsesthetics help ggplot know what component of our data to visualize – why we’ll visualize values from one column instead of another.\nIn a bar chart, we first pass in the data to the geometry, then set the aesthetic.\nIn the codeblock below, we’ve added a new function, geom_bar().\nUsing geom_bar() – as opposed to geom_line() – says we’re making a bar chart.\nInside of that function, the asthetic, aes, says which columns to use in drawing the chart.\nWe’re setting the values on the x axis (horizontal) to be the name of the county. We set weight to total loans, and it uses that value to “weight” or set the height of each bar.\nOne quirk here with ggplot.\nAfter we’ve invoked the ggplot() function, you’ll notice we’re using a + symbol. It means the same thing as |&gt; – “and then do this”. It’s just a quirk of ggplot() that after you invoke the ggplot() function, you use + instead of |&gt;. It makes no sense to me either, just something to live with.\n\nbaltco_911_calls_by_month |&gt;\n  ggplot() +\n  geom_bar(aes(x=month, weight=total_calls))\n\n\n\n\n\n\n\n\nThis is a very basic chart. But it’s hard to derive much meaning from this chart, because the counties aren’t ordered from highest to lowest by total_loans. We can fix that by using the reorder() function to do just that:\n\nbaltco_911_calls_by_month |&gt;\n  ggplot() +\n  geom_bar(aes(x=reorder(month,total_calls), weight=total_calls))\n\n\n\n\n\n\n\n\nThis is a little more useful. But the bottom is kind of a mess, with overlapping names. We can fix that by flipping it from a vertical bar chart (also called a column chart) to a horizontal one. coord_flip() does that for you.\n\nbaltco_911_calls_by_month |&gt;\n  ggplot() +\n  geom_bar(aes(x=reorder(month,total_calls), weight=total_calls)) +\n  coord_flip()\n\n\n\n\n\n\n\n\nIs this art? No. Does it quickly tell you something meaningful? It does.\nWe’re mainly going to use these charts to help us in reporting, so style isn’t that important.\nBut it’s worth mentioning that we can pretty up these charts for publication, if we wanted to, with some more code. To style the chart, we can change or even modify the “theme”, a kind of skin that makes the chart look better.\nIt’s kind of like applying CSS to html. Here I’m changing the theme slightly to remove the gray background with one of ggplot’s built in themes, theme_minimal()\n\nbaltco_911_calls_by_month |&gt;\n  ggplot() +\n  geom_bar(aes(x=reorder(month,total_calls), weight=total_calls)) +\n  coord_flip() + \n  theme_minimal()\n\n\n\n\n\n\n\n\nThe ggplot universe is pretty big, and lots of people have made and released cool themes for you to use. Want to make your graphics look kind of like The Economist’s graphics? There’s a theme for that.\nFirst, you have to install and load a package that contains lots of extra themes, called ggthemes.\n\n#install.packages('ggthemes')\nlibrary(ggthemes)\n\nAnd now we’ll apply the economist theme from that package with theme_economist()\n\nbaltco_911_calls_by_month |&gt;\n  ggplot() +\n  geom_bar(aes(x=reorder(month,total_calls), weight=total_calls)) +\n  coord_flip() + \n  theme_economist()\n\n\n\n\n\n\n\n\nThose axis titles are kind of a mess. Let’s change “count” on the x axis to “net change” and change “reorder(County,TOTAL_DIFF)” to “county”. And while we’re at it, let’s add a basic title and a source as a caption. We’ll use a new function, labs(), which is short for labels.\n\nbaltco_911_calls_by_month |&gt;\n  ggplot() +\n  geom_bar(aes(x=reorder(month,total_calls), weight=total_calls)) +\n  coord_flip() + \n  theme_economist() +\n  labs(\n    title=\"More 911 Overdose Calls in Warmer Months\",\n    x = \"month\",\n    y = \"total calls\",\n    caption = \"source: Baltimore County\"\n  )\n\n\n\n\n\n\n\n\nViola. Not super pretty, but good enough to show an editor to help them understand the conclusions you reached with your data analysis.",
    "crumbs": [
      "Visualizing your data for reporting"
    ]
  },
  {
    "objectID": "visualizing-for-reporting.html#line-charts",
    "href": "visualizing-for-reporting.html#line-charts",
    "title": "Visualizing your data for reporting",
    "section": "",
    "text": "Let’s look at how to make another common chart type that will help you understand patterns in your data.\nLine charts can show change over time. It works much the same as a bar chart, code wise, but instead of a weight, it uses a y.\n\nbaltco_911_calls_by_date &lt;- baltco_911_calls |&gt;\n  group_by(date) |&gt;\n  summarise(\n    total_calls=n()\n  )\n\nbaltco_911_calls_by_date\n\n# A tibble: 366 × 2\n   date       total_calls\n   &lt;date&gt;           &lt;int&gt;\n 1 2022-02-06           8\n 2 2022-02-07          15\n 3 2022-02-08           7\n 4 2022-02-09           5\n 5 2022-02-10           5\n 6 2022-02-11           8\n 7 2022-02-12           9\n 8 2022-02-13           7\n 9 2022-02-14          11\n10 2022-02-15           7\n# ℹ 356 more rows\n\n\nAnd now let’s make a line chart to look for patterns in this data.\nWe’ll put the date on the x axis and total calls on the y axis.\n\nbaltco_911_calls_by_date |&gt;\n  ggplot() + \n  geom_line(aes(x=date, y=total_calls))\n\n\n\n\n\n\n\n\nIt’s not super pretty, but there’s a bit of a pattern here: the number of calls fluctuates between 5 and 20 a day for most of this period, and then jumps way up at certain points during the year. In particular there are spikes in July, early October and January.\nRight now, it’s kind of hard to see specifics, though. When did some of those smaller spikes and troughs happen?\nWe can’t really tell. So let’s modify the x axis to have one tick mark and label per month. We can do that with a function called scale_x_date().\nWe’ll set the date_breaks to appear for every week; if we wanted every month, we’d say date_breaks = “1 month”. We can set the date to appear as month abbreviated name (%b) and day (%d).\n\nbaltco_911_calls_by_date |&gt;\n  ggplot() + \n  geom_line(aes(x=date, y=total_calls)) + \n  scale_x_date(date_breaks = \"1 week\", date_labels = \"%b %d\")\n\n\n\n\n\n\n\n\nThose are a little hard to read, so we can turn them 45 degrees to remove the overlap using the theme() function for styling. With “axis.text.x = element_text(angle = 45, hjust=1)” we’re saying, turn the date labels 45 degrees.\n\nbaltco_911_calls_by_date |&gt;\n  ggplot() + \n  geom_line(aes(x=date, y=total_calls)) + \n  scale_x_date(date_breaks = \"1 week\", date_labels = \"%b %d\") +\n  theme(\n    axis.text.x = element_text(angle = 45,  hjust=1)\n  )\n\n\n\n\n\n\n\n\nAgain, this isn’t as pretty as we could make it. But by charting this, we can quickly see a pattern that can help guide our reporting.\nWe’re just scratching the surface of what ggplot can do, and chart types. There’s so much more you can do, so many other chart types you can make. But the basics we’ve shown here will get you started.",
    "crumbs": [
      "Visualizing your data for reporting"
    ]
  },
  {
    "objectID": "start-story.html",
    "href": "start-story.html",
    "title": "Learn a new way to read",
    "section": "",
    "text": "Getting started in data journalism often feels as if you’ve left the newsroom and entered the land of statistics, computer programming and data science. This chapter will help you start seeing data reporting in a new way, by learning how to study great works of the craft as a writer rather than a reader.\n\n\n\n\njelani cobb\n\n\n\nJelani Cobb tweeted, “an engineer doesn’t look at a bridge the same way pedestrians or drivers do.” They see it as a “language of angles and load bearing structures.” We just see a bridge. While he was referring to long-form writing, reporting with data can also be learned by example – if you spend enough time with the examples.\nAlmost all good writers and reporters try to learn from exemplary work. I know more than one reporter who studies prize-winning journalism to hone their craft. This site will have plenty of examples, but you should stay on the lookout for others.\n\n\nTry to approach data or empirical reporting as a reporter first, and a consumer second. The goal is to triangulate how the story was discovered, reported and constructed. You’ll want to think about why this story, told this way, at this time, was considered newsworthy enough to publish when another approach on the same topic might not have been.\n\n\nIn data journalism, we often start with a tip, or a hypothesis. Sometimes it’s a simple question. Walt Bogdanich of The New York Times is renowned for seeing stories around every corner. Bogdanich has said that the prize-winning story “A Disability Epidemic Among a Railroad’s Retirees” came from a simple question he had when railway workers went on strike over pension benefits – how much were they worth? The story led to an FBI investigation and arrests, along with pension reform at the largest commuter rail in the country.\nThe hypothesis for some stories might be more directed. In 2021, the Howard Center for Investigative Journalism at ASU published “Little victims everywhere”, a set of stories on the lack of justice for survivors of child sexual assault on Native American reservations. That story came after previous reporters for the center analyzed data from the Justice Department showing that the FBI dropped most of the cases it investigated, and the Justice Department then only prosecuted about half of the matters referred to it by investigators. The hypothesis was that they were rarely pursued because federal prosecutors – usually focused on immigration, white collar crime and drugs – weren’t as prepared to pursue violent crime in Indian Country.\nWhen studying a data-driven investigation, try to imagine what the reporters were trying to prove or disprove, and what they used to do it. In journalism, we rely on a mixture of quantitative and qualitative methods. It’s not enough to prove the “numbers” or have the statistical evidence. That is just the beginning of the story. We are supposed to ground-truth them with the stories of actual people and places.\n\n\n\nIt’s easy to focus on the numbers or statistics that make up the key findings, or the reason for the story. Some reporters make the mistake of thinking all of the numbers came from the same place – a rarity in most long-form investigations. Instead, the sources have been woven together and are a mix of original research and research done by others. Try to pay attention to any sourcing done in the piece. Sometimes, it will tell you that the analysis was original. Other times it’s more subtle.\nBut don’t just look at the statistics being reported in the story. In many (most?) investigations, some of the key people, places or time elements come directly from a database.\nWhen I was analyzing Paycheck Protection Program loan data for ProPublica, one fact hit me as I was looking at a handful of sketchy-looking records: a lot of them were from a single county in coastal New Jersey. It turned out to be a pretty good story.\nOften, the place that a reporter visits is determined by examples found in data. In this story on rural development funds, all of the examples came from an analysis of the database. Once the data gave us a good lead, the reporters examined press releases and other easy-to-get sources before calling and visiting the recipients or towns.\n\n\n\n\nYou’ll get better at reading investigations and data-driven work over time, but for now, remember to go beyond the obvious:\n\nWhere might the reporters have found their key examples, and what made them good characters or illustrations of the larger issue? Could they have come from the data?\nWhat do you think came first – a narrative single example that was broadened by data (naively, qualitative method), or a big idea that was illustrated with characters (quantitative method)?\nWhat records were used? Were they public records, leaks, or proprietary data?\nWhat methods did they use? Did they do their own testing, use statistical analysis, or geographic methods? You won’t always know, but look for a methodology section or a description alongside each story.\nHow might you localize or adapt these methods to find your own stories?\nPick out the key findings (usually in the nut graf or in a series of bullets after the opening chapter): are they controversial? How might they have been derived? What might have been the investigative hypothesis? Have they given critics their due and tried to falsify their own work?\nHow effective is the writing and presentation of the story? What makes it compelling journalism rather than a dry study? How might you have done it differently? Is a video story better told in text, or would a text story have made a good documentary? Are the visual elements well integrated? Does the writing draw you in and keep you reading? Think about structure, story length, entry points and graphics all working together.\nAre you convinced? Are there holes or questions that didn’t get addressed?\n\n\n\n\nAs journalists we’ll often be using data, social science methods and even interviewing differently than true experts. We’re seeking stories, not studies. Recognizing news in data is one of the hardest skills for less experienced reporters new to data journalism. This list of potential newsworthy data points is adapted from Paul Bradshaw’s “Data Journalism Heist”.\n\n\n\n\nCompare the claims of powerful people and institutions against facts – the classic investigative approach.\nReport on unexpected highs and lows (of change, or of some other characteristic)\nLook for outliers – individual values that buck a trend seen in the rest\nVerify or bust some myths\nFind signs of distress, happiness or dishonesty or any other emotion.\nUncover new or under-reported long-term trends.\nFind data suggesting your area is the same or different than most others of its kind.\n\nBradshaw also did a recent study of data journalism pieces: “Here are the angles journalists use most often to tell the stories in data”, in Online Journalism Blog. I’m not sure I agree, only because he’s looking mainly at visualizations rather than stories, but they’re worth considering.\n\n\n\n\nIf you’re a member of Investigative Reporters and Editors, go to the site and find a recent prize-winning entry (usually text rather than broadcast). Get a copy of the IRE contest entry from the Resources page. Try to match up what the reporters said they did and how they did it with key portions of the story.\nThe next time you find a good data source, try to find a story that references it. If your data is local, you might look for a story that used similar data elsewhere, such as 911 response times or overdose deaths. But many stories use federal datasets that can easily be localized. Look at a description of the dataset and then the story to see how the data might have been used.",
    "crumbs": [
      "Learn a new way to read"
    ]
  },
  {
    "objectID": "start-story.html#read-like-a-reporter",
    "href": "start-story.html#read-like-a-reporter",
    "title": "Learn a new way to read",
    "section": "",
    "text": "Try to approach data or empirical reporting as a reporter first, and a consumer second. The goal is to triangulate how the story was discovered, reported and constructed. You’ll want to think about why this story, told this way, at this time, was considered newsworthy enough to publish when another approach on the same topic might not have been.\n\n\nIn data journalism, we often start with a tip, or a hypothesis. Sometimes it’s a simple question. Walt Bogdanich of The New York Times is renowned for seeing stories around every corner. Bogdanich has said that the prize-winning story “A Disability Epidemic Among a Railroad’s Retirees” came from a simple question he had when railway workers went on strike over pension benefits – how much were they worth? The story led to an FBI investigation and arrests, along with pension reform at the largest commuter rail in the country.\nThe hypothesis for some stories might be more directed. In 2021, the Howard Center for Investigative Journalism at ASU published “Little victims everywhere”, a set of stories on the lack of justice for survivors of child sexual assault on Native American reservations. That story came after previous reporters for the center analyzed data from the Justice Department showing that the FBI dropped most of the cases it investigated, and the Justice Department then only prosecuted about half of the matters referred to it by investigators. The hypothesis was that they were rarely pursued because federal prosecutors – usually focused on immigration, white collar crime and drugs – weren’t as prepared to pursue violent crime in Indian Country.\nWhen studying a data-driven investigation, try to imagine what the reporters were trying to prove or disprove, and what they used to do it. In journalism, we rely on a mixture of quantitative and qualitative methods. It’s not enough to prove the “numbers” or have the statistical evidence. That is just the beginning of the story. We are supposed to ground-truth them with the stories of actual people and places.\n\n\n\nIt’s easy to focus on the numbers or statistics that make up the key findings, or the reason for the story. Some reporters make the mistake of thinking all of the numbers came from the same place – a rarity in most long-form investigations. Instead, the sources have been woven together and are a mix of original research and research done by others. Try to pay attention to any sourcing done in the piece. Sometimes, it will tell you that the analysis was original. Other times it’s more subtle.\nBut don’t just look at the statistics being reported in the story. In many (most?) investigations, some of the key people, places or time elements come directly from a database.\nWhen I was analyzing Paycheck Protection Program loan data for ProPublica, one fact hit me as I was looking at a handful of sketchy-looking records: a lot of them were from a single county in coastal New Jersey. It turned out to be a pretty good story.\nOften, the place that a reporter visits is determined by examples found in data. In this story on rural development funds, all of the examples came from an analysis of the database. Once the data gave us a good lead, the reporters examined press releases and other easy-to-get sources before calling and visiting the recipients or towns.",
    "crumbs": [
      "Learn a new way to read"
    ]
  },
  {
    "objectID": "start-story.html#reading-tips",
    "href": "start-story.html#reading-tips",
    "title": "Learn a new way to read",
    "section": "",
    "text": "You’ll get better at reading investigations and data-driven work over time, but for now, remember to go beyond the obvious:\n\nWhere might the reporters have found their key examples, and what made them good characters or illustrations of the larger issue? Could they have come from the data?\nWhat do you think came first – a narrative single example that was broadened by data (naively, qualitative method), or a big idea that was illustrated with characters (quantitative method)?\nWhat records were used? Were they public records, leaks, or proprietary data?\nWhat methods did they use? Did they do their own testing, use statistical analysis, or geographic methods? You won’t always know, but look for a methodology section or a description alongside each story.\nHow might you localize or adapt these methods to find your own stories?\nPick out the key findings (usually in the nut graf or in a series of bullets after the opening chapter): are they controversial? How might they have been derived? What might have been the investigative hypothesis? Have they given critics their due and tried to falsify their own work?\nHow effective is the writing and presentation of the story? What makes it compelling journalism rather than a dry study? How might you have done it differently? Is a video story better told in text, or would a text story have made a good documentary? Are the visual elements well integrated? Does the writing draw you in and keep you reading? Think about structure, story length, entry points and graphics all working together.\nAre you convinced? Are there holes or questions that didn’t get addressed?",
    "crumbs": [
      "Learn a new way to read"
    ]
  },
  {
    "objectID": "start-story.html#analyze-data-for-story-not-study",
    "href": "start-story.html#analyze-data-for-story-not-study",
    "title": "Learn a new way to read",
    "section": "",
    "text": "As journalists we’ll often be using data, social science methods and even interviewing differently than true experts. We’re seeking stories, not studies. Recognizing news in data is one of the hardest skills for less experienced reporters new to data journalism. This list of potential newsworthy data points is adapted from Paul Bradshaw’s “Data Journalism Heist”.\n\n\n\n\nCompare the claims of powerful people and institutions against facts – the classic investigative approach.\nReport on unexpected highs and lows (of change, or of some other characteristic)\nLook for outliers – individual values that buck a trend seen in the rest\nVerify or bust some myths\nFind signs of distress, happiness or dishonesty or any other emotion.\nUncover new or under-reported long-term trends.\nFind data suggesting your area is the same or different than most others of its kind.\n\nBradshaw also did a recent study of data journalism pieces: “Here are the angles journalists use most often to tell the stories in data”, in Online Journalism Blog. I’m not sure I agree, only because he’s looking mainly at visualizations rather than stories, but they’re worth considering.",
    "crumbs": [
      "Learn a new way to read"
    ]
  },
  {
    "objectID": "start-story.html#exercises",
    "href": "start-story.html#exercises",
    "title": "Learn a new way to read",
    "section": "",
    "text": "If you’re a member of Investigative Reporters and Editors, go to the site and find a recent prize-winning entry (usually text rather than broadcast). Get a copy of the IRE contest entry from the Resources page. Try to match up what the reporters said they did and how they did it with key portions of the story.\nThe next time you find a good data source, try to find a story that references it. If your data is local, you might look for a story that used similar data elsewhere, such as 911 response times or overdose deaths. But many stories use federal datasets that can easily be localized. Look at a description of the dataset and then the story to see how the data might have been used.",
    "crumbs": [
      "Learn a new way to read"
    ]
  },
  {
    "objectID": "start-data-def.html",
    "href": "start-data-def.html",
    "title": "Defining “Data”",
    "section": "",
    "text": "data /ˈdeɪ.tə/ :\ninformation in an electronic form that can be stored and used by a computer, or information, especially facts or numbers, collected to be examined and &gt;considered and used to help decision-making\n– Cambridge Dictionary – sort of 1\n\n\n\nMost journalism uses data collected for one purpose for something entirely different. Understanding its original uses – what matters to the people who collected it, and what doesn’t – will profoundly affect its accuracy or usefulness.\n\n\nIn “The Art of Access”, David Cullier and Charles N. Davis describe a process of tracking down the life and times of a dataset. Their purpose is to make sure they know how to request it from a government agency. The same idea applies to using data that we acquire elsewhere.\nUnderstanding how and why data exists is crucial to understanding what you, as a reporter, might do with it.\nAnything you can systematically search or analyze could be considered one piece of of data. As reporters, we usually deal with data that was created in the process of doing something else – conducting an inspection, delivering a tweet, or scoring a musical. In the sciences, this flotsam and jetsom that is left behind is called “digital trace data” if it was born digitally.\nIn journalism and in the social sciences, many of our data sources were born during some government process – a safety inspection, a traffic ticket, or the filing of a death certificate. These administrative records form the basis of much investigative reporting and they are often the subject of public records and FOIA requests. They were born as part of the government doing its job, without any thought given to how it might be used in another way. In the sciences, those are often called “administrative records”.\nThis trace data might be considered the first part of the definition above – information that can be stored and used.\nHere’s how Chris Bail from Duke University describes it.\n\n\n\nAnother kind of data is that which is compiled or collected specifically for the purpose of studying something. It might collected in the form of a survey or a poll, or it might be a system of sampling to measure pollution or weather. But it’s there because the information has intrinsic value AS information.\nThe video suggests a hard line between trace data and custom data. In practice, it’s not that clear. Many newsrooms may curate data published in other sources or in administrative records, such as the Washington Post’s police shooting dataset. In other cases, the agencies we are covering get already-compiled data from state and local governments.\nThis type of data might be considered the second type in the definition – tabular information that is used for decision-making.\n\n\n\n\nOne of the hardest concepts for a lot of new data journalists is the idea of granularity of your data source. There are a lot of ways to think about this: individual items in a list vs. figures in a table; original records vs. compilations; granular data vs. statistics.\nGenerally, an investigative reporter is interested in getting data that is as close as possible to the most granular information that exists, at least on computer files. Here’s an example, which might give you a little intuition about why it’s so important to think this way:\nWhen someone dies in the US, a standard death certificate is filled out by a series of officials - the attending physician, the institution where they died and even the funeral direcor.\nClick on this link to see a blank version of the standard US death certificate form – notice the detail and the detailed instructions on how it is supposed to be filled out. 2\nA good reporter could imagine many stories coming out of these little boxes. Limiting yourself to just to COVID-19-related stories: You could profile the local doctor who signed the most COVID-19-related death certificates in their city, or examine the number of deaths that had COVID as a contributing, but not underlying or immediate, cause of death. You could compare smoking rates in the city with the number of decedents whose tobacco use likely contributed to their death. Maybe you’d want to know how long patients suffered with the disease before they died. And you could map the deaths to find the block in your town most devastated by the virus.\nEarly in the pandemic, Coulter Jones and Jon Kamp examined the records from one of the few states that makes them public, and concluded that “Coronavirus Deaths were Likely Missed in Michigan, Death Certificates Suggest”\nBut you probably can’t do that. The reason is that, in most states, death certificates are not public records and are treated as secrets. 3. Instead, state and local governments provide limited statistics related to the deaths, usually by county, with no detail. That’s the difference between granular data and aggregate data. Here are some of the typical (not universal) characteristics of each:\n\n\n\n\n\n\n\nGranular\nAggregate\n\n\n\n\nIntended for some purpose other than your work\nIntended to be presented as is to the public\n\n\nMany rows (records), few columns (variables)\nMany columns (variables), few rows (records)\n\n\nRequires a good understanding of the source\nExplanatory notes usually come with the data\n\n\nEasy to cross-reference and compile\nOften impossible to repurpose\n\n\nHas few numeric columns\nMay be almost entirely numerical\n\n\nIs intended for use in a database\nIs intended for use in a spreadsheet\n\n\n\nWe often have to consider the trade-offs. Granular data with the detail we need - especially when it involves personally identifiable information like names and addresses - can take months or years of negotiation over public records requests, even when the law allows it. It’s often much easier to convince an agency to provide summarized or incomplete data. Don’t balk at using it if it works for you. But understand that in the vast majority of cases, it’s been summarized in a way that’s lost information that could be important to your story.\n\n\n\nThat brings us to one of the most important things you must find out about any data you begin to analyze: What “noun” does each row in a tabular dataset represent? In statistics, they might be called observations or cases. In data science, they’re usually called records. Either way, every row must represent the same thing – a person, a place, a year, a water sample or a school. And you can’t really do anything with it until you figure out what that is.\nIn 2015, Sarah Cohen did a story at The New York Times called “More Deportation Follow Minor Crimes, Records Show”. The government had claimed it was only removing hardened criminals from the country, but our analysis of the data suggested that many of them were for minor infractions.\nIn writing the piece, they had to work around a problem in our data: the agency refused to provide them anything that would help us distinguish individuals from one another. All the reporters knew was that each row represented one deportation – not one person! Without a column, or field or a variable or an attribute for an individual – say, name and date of birth, or some scrambled version of an their DHS number – they had no way to even estimate how often people were deported multiple times. If you read the story, you’ll see the very careful wording, except when they had reported out and spoken to people on the ground.\n\n\n\n\n“Basic steps in working with data”, the Data Journalism Handbook, Steve Doig, ASU Professor. He describes in this piece the problem of not knowing exactly how the data was compiled.\n“Counting the Infected” , Rob Gebellof on The Daily, July 8, 2020.\n“Spreadsheet thinking vs. Database thinking”, by Robert Kosara, gets at the idea that looking at individual items is often a “database”, and statistical compilations are often “spreadsheets”.\n“Tidy Data”, in the Journal of Statistical Software (linked here in a pre-print) by Hadley Wickham , is the quintessential article on describing what we think of as “clean” data. For our purposes, much of what he describes as “tidy” comes when we have individual, granular records – not statistical compilations. It’s an academic article, but it has the underlying concepts that we’ll be working with all year.\n\n\n\n\n\nThe next time you get a government statistical report, scour all of the footnotes to find some explanation of where the data came from. You’ll be surprised how often they are compilations of administrative records - the government version of trace data.",
    "crumbs": [
      "Defining \"Data\""
    ]
  },
  {
    "objectID": "start-data-def.html#the-birth-of-a-dataset",
    "href": "start-data-def.html#the-birth-of-a-dataset",
    "title": "Defining “Data”",
    "section": "",
    "text": "Most journalism uses data collected for one purpose for something entirely different. Understanding its original uses – what matters to the people who collected it, and what doesn’t – will profoundly affect its accuracy or usefulness.\n\n\nIn “The Art of Access”, David Cullier and Charles N. Davis describe a process of tracking down the life and times of a dataset. Their purpose is to make sure they know how to request it from a government agency. The same idea applies to using data that we acquire elsewhere.\nUnderstanding how and why data exists is crucial to understanding what you, as a reporter, might do with it.\nAnything you can systematically search or analyze could be considered one piece of of data. As reporters, we usually deal with data that was created in the process of doing something else – conducting an inspection, delivering a tweet, or scoring a musical. In the sciences, this flotsam and jetsom that is left behind is called “digital trace data” if it was born digitally.\nIn journalism and in the social sciences, many of our data sources were born during some government process – a safety inspection, a traffic ticket, or the filing of a death certificate. These administrative records form the basis of much investigative reporting and they are often the subject of public records and FOIA requests. They were born as part of the government doing its job, without any thought given to how it might be used in another way. In the sciences, those are often called “administrative records”.\nThis trace data might be considered the first part of the definition above – information that can be stored and used.\nHere’s how Chris Bail from Duke University describes it.\n\n\n\nAnother kind of data is that which is compiled or collected specifically for the purpose of studying something. It might collected in the form of a survey or a poll, or it might be a system of sampling to measure pollution or weather. But it’s there because the information has intrinsic value AS information.\nThe video suggests a hard line between trace data and custom data. In practice, it’s not that clear. Many newsrooms may curate data published in other sources or in administrative records, such as the Washington Post’s police shooting dataset. In other cases, the agencies we are covering get already-compiled data from state and local governments.\nThis type of data might be considered the second type in the definition – tabular information that is used for decision-making.",
    "crumbs": [
      "Defining \"Data\""
    ]
  },
  {
    "objectID": "start-data-def.html#granular-and-aggregated-data",
    "href": "start-data-def.html#granular-and-aggregated-data",
    "title": "Defining “Data”",
    "section": "",
    "text": "One of the hardest concepts for a lot of new data journalists is the idea of granularity of your data source. There are a lot of ways to think about this: individual items in a list vs. figures in a table; original records vs. compilations; granular data vs. statistics.\nGenerally, an investigative reporter is interested in getting data that is as close as possible to the most granular information that exists, at least on computer files. Here’s an example, which might give you a little intuition about why it’s so important to think this way:\nWhen someone dies in the US, a standard death certificate is filled out by a series of officials - the attending physician, the institution where they died and even the funeral direcor.\nClick on this link to see a blank version of the standard US death certificate form – notice the detail and the detailed instructions on how it is supposed to be filled out. 2\nA good reporter could imagine many stories coming out of these little boxes. Limiting yourself to just to COVID-19-related stories: You could profile the local doctor who signed the most COVID-19-related death certificates in their city, or examine the number of deaths that had COVID as a contributing, but not underlying or immediate, cause of death. You could compare smoking rates in the city with the number of decedents whose tobacco use likely contributed to their death. Maybe you’d want to know how long patients suffered with the disease before they died. And you could map the deaths to find the block in your town most devastated by the virus.\nEarly in the pandemic, Coulter Jones and Jon Kamp examined the records from one of the few states that makes them public, and concluded that “Coronavirus Deaths were Likely Missed in Michigan, Death Certificates Suggest”\nBut you probably can’t do that. The reason is that, in most states, death certificates are not public records and are treated as secrets. 3. Instead, state and local governments provide limited statistics related to the deaths, usually by county, with no detail. That’s the difference between granular data and aggregate data. Here are some of the typical (not universal) characteristics of each:\n\n\n\n\n\n\n\nGranular\nAggregate\n\n\n\n\nIntended for some purpose other than your work\nIntended to be presented as is to the public\n\n\nMany rows (records), few columns (variables)\nMany columns (variables), few rows (records)\n\n\nRequires a good understanding of the source\nExplanatory notes usually come with the data\n\n\nEasy to cross-reference and compile\nOften impossible to repurpose\n\n\nHas few numeric columns\nMay be almost entirely numerical\n\n\nIs intended for use in a database\nIs intended for use in a spreadsheet\n\n\n\nWe often have to consider the trade-offs. Granular data with the detail we need - especially when it involves personally identifiable information like names and addresses - can take months or years of negotiation over public records requests, even when the law allows it. It’s often much easier to convince an agency to provide summarized or incomplete data. Don’t balk at using it if it works for you. But understand that in the vast majority of cases, it’s been summarized in a way that’s lost information that could be important to your story.",
    "crumbs": [
      "Defining \"Data\""
    ]
  },
  {
    "objectID": "start-data-def.html#nouns",
    "href": "start-data-def.html#nouns",
    "title": "Defining “Data”",
    "section": "",
    "text": "That brings us to one of the most important things you must find out about any data you begin to analyze: What “noun” does each row in a tabular dataset represent? In statistics, they might be called observations or cases. In data science, they’re usually called records. Either way, every row must represent the same thing – a person, a place, a year, a water sample or a school. And you can’t really do anything with it until you figure out what that is.\nIn 2015, Sarah Cohen did a story at The New York Times called “More Deportation Follow Minor Crimes, Records Show”. The government had claimed it was only removing hardened criminals from the country, but our analysis of the data suggested that many of them were for minor infractions.\nIn writing the piece, they had to work around a problem in our data: the agency refused to provide them anything that would help us distinguish individuals from one another. All the reporters knew was that each row represented one deportation – not one person! Without a column, or field or a variable or an attribute for an individual – say, name and date of birth, or some scrambled version of an their DHS number – they had no way to even estimate how often people were deported multiple times. If you read the story, you’ll see the very careful wording, except when they had reported out and spoken to people on the ground.",
    "crumbs": [
      "Defining \"Data\""
    ]
  },
  {
    "objectID": "start-data-def.html#further-reading",
    "href": "start-data-def.html#further-reading",
    "title": "Defining “Data”",
    "section": "",
    "text": "“Basic steps in working with data”, the Data Journalism Handbook, Steve Doig, ASU Professor. He describes in this piece the problem of not knowing exactly how the data was compiled.\n“Counting the Infected” , Rob Gebellof on The Daily, July 8, 2020.\n“Spreadsheet thinking vs. Database thinking”, by Robert Kosara, gets at the idea that looking at individual items is often a “database”, and statistical compilations are often “spreadsheets”.\n“Tidy Data”, in the Journal of Statistical Software (linked here in a pre-print) by Hadley Wickham , is the quintessential article on describing what we think of as “clean” data. For our purposes, much of what he describes as “tidy” comes when we have individual, granular records – not statistical compilations. It’s an academic article, but it has the underlying concepts that we’ll be working with all year.",
    "crumbs": [
      "Defining \"Data\""
    ]
  },
  {
    "objectID": "start-data-def.html#exercises",
    "href": "start-data-def.html#exercises",
    "title": "Defining “Data”",
    "section": "",
    "text": "The next time you get a government statistical report, scour all of the footnotes to find some explanation of where the data came from. You’ll be surprised how often they are compilations of administrative records - the government version of trace data.",
    "crumbs": [
      "Defining \"Data\""
    ]
  },
  {
    "objectID": "start-data-def.html#footnotes",
    "href": "start-data-def.html#footnotes",
    "title": "Defining “Data”",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI flipped the order of these two definitions!↩︎\nYou should do this whenever you get a dataset created from administrative records. That is, track down its origin and examine the pieces you were given and the pieces that were left out; look at what is written in free-form vs what is presented as a check box. You may need a copy of the template that an agency uses to collect the information, but many governments make these available on their websites or are willing to provide them without a fuss.↩︎\nSee “Secrecy in Death Records: A call to action”, by Megain Craig and Madeleine Davison, Journal of Civic Information, December 2020↩︎",
    "crumbs": [
      "Defining \"Data\""
    ]
  },
  {
    "objectID": "replication.html",
    "href": "replication.html",
    "title": "Data journalism in the age of replication",
    "section": "",
    "text": "A single word in a single job ad for Buzzfeed News posted in 2017 offered an indication of a profound shift in how data journalism is both practiced and taught.\n“We’re looking for someone with a passion for news and a commitment to using data to find amazing, important stories — both quick hits and deeper analyses that drive conversations,” the posting seeking a data journalist says. It goes on to list five things BuzzFeed is looking for: Excellent collaborator, clear writer, deep statistical understanding, knowledge of obtaining and restructuring data.\nAnd then there’s this:\n“You should have a strong command of at least one toolset that (a) allows for filtering, joining, pivoting, and aggregating tabular data, and (b) enables reproducible workflows.”\nThe word you’re seeing more and more of? Reproducible. And it started in earnest in 2017 when data journalism crossed a major threshold in American journalism: It got it’s own section in the Associated Press Stylebook.\n“Data journalism has become a staple of reporting across beats and platforms,” the Data Journalism section of the Stylebook opens. “The ability to analyze quantitative information and present conclusions in an engaging and accurate way is no longer the domain of specialists alone.”\nThe AP’s Data Journalism section discusses how to request data and in what format, guidelines for scraping data from websites with automation, the ethics of using leaked or hacked data and other topics long part of data journalism conference talks.\nBut the third page of the section contains perhaps the most profound commandment: “As a general rule, all assertions in a story based on data analysis should be reproducible. The methodology description in the story or accompanying materials should provide a road map to replicate the analysis.”\nReproducible research – replication – is a cornerstone of scientific inquiry. Researchers across a range of academic disciplines use methods to find new knowledge and publish it in peer reviewed journals. And, when it works, other researchers take that knowledge and try it with their own samples in their own locations. Replication studies exist to take something from an “interesting finding” to a “theory” and beyond.\nIt doesn’t always work.\nReplication studies aren’t funded at nearly the level as new research. And, to the alarm of many, scores of studies can’t be replicated by others. Researchers across disciplines are finding that when their original studies are replicated, flaws are found, or the effects found aren’t as strong as the original. Because of this, academics across a number of disciplines have written about a replication crisis in their respective fields, particularly psychology, social science and medical research.\nIn Chapter 1 of the New Precision Journalism, Phil Meyer wrote that “we journalists would be wrong less often if we adapted to our own use some of the research tools of the social scientists.”\nMeyer would go on to write about how computers pouring over datasets too large to crunch by hand had changed social science from a discipline with “a few data and a lot of interpretation” into a much more meaningful and powerful area of study. If journalists could become comfortable with data and some basic statistics, they too could harness this power.\n“It used to be said that journalism is history in a hurry,” Meyer wrote. “The argument of this book is that to cope with the acceleration of social change in today’s world, journalism must become social science in a hurry.”\nHe wrote that in 1971. It might as well have been yesterday.\nJournalism doesn’t have a history of replication, but the concerns about credibility are substantially greater. Trust in media is at an all time low and shows no signs of improving. While the politics of the day have quite a bit to do with this mistrust of media, being more transparent about what journalists do can’t hurt.\nThe AP’s commandment that “Thou must replicate your findings” could, if taken seriously by the news business, have substantial impacts on how data journalism gets done in newsrooms and how data journalism gets taught, both at professional conferences and universities.\nHow? Two ways.\n\nThe predominant way that data journalism gets done in a newsroom is through simple tools like Microsoft Excel or Google Sheets. Those simple tools, on their own, lack significant logging functions that automatically keep track of steps a data journalist took to reach a given conclusion. That means journalists using those tools have to maintain separate, detailed logs of what they did so any analysis can be replicated.\nThe predominant way that data journalism gets taught – both in professional settings and at most universities – doesn’t deal with replication at all. The tools and the training stress “getting things done” – an entirely logical focus for a deadline driven business. The choices of tools – like spreadsheet programs – are made to get from data to story as quick as possible, without frightening away math and tech phobic students.\n\nIf the AP’s replication rules are to be followed, journalism needs to become much more serious about the tools and techniques used to do data journalism. The days of “point and click” tools to do “quick and dirty” analysis that get published are dying. The days of formal methods using documented steps are here.\n\n\nTroy Thibodeaux, the editor of the AP’s data journalism team, said the stylebook entry started when the data team found themselves answering the same questions over and over. With a grant from the Knight Foundation, the team began to document their own standards and turn that into a stylebook section.\nFrom the beginning, they had a fairly clear idea of what they wanted to do – think through a project and ask what the frequently asked questions are that came up. It was not going to be a soup-to-nuts guide to how to do a data project.\nWhen the section came out, eyebrows went up on the replication parts, surprising Thibodeaux.\n“From our perspective, this is a core value for us,” he said. “Just for our own benefit, we need to be able to have someone give us a second set of eyes. We benefit from that every day. We catch things for each other.”\nThibodeaux said the AP data team has two audiences when it comes to replication – they have the readers of the work, and members of the collective who may want to do their own work with the data.\n“This is something that’s essential to the way we work,” he said. “And it’s important in terms of transparency and credibility going forward. We thought it would be kind of unexceptionable.”\n\n\n\nMeyer, now 86, said he’s delighted to see replication up for discussion now, but warned that we shouldn’t take it too far.\n“Making the analysis replicable was something I worried about from the very beginning,” he wrote in an email. So much so that in 1967, after publishing stories from his landmark survey after the Detroit riots, he shipped the data and backup materials about it to a social science data repository at the University of North Carolina.\nAnd, in doing so, he opened the door to others replicating his results. One scholar attempted to find fault with Meyer’s analysis by slicing the data ever thinner until the differences weren’t significant – gaming the analysis to criticize the stories.\nMeyer believes replication is vitally important, but doesn’t believe it should take on the trappings of science replication, where newsrooms take their own samples or re-survey a community. That would be prohibitively expensive.\nBut journalists should be sharing their data and analysis steps. And it doesn’t need to be complicated, he said.\n“Replication is a theoretical standard, not a requirement that every investigator duplicate his or her own work for every project,” he said. “Giving enough information in the report to enable another investigator to follow in your footsteps is enough. Just telling enough to make replication possible will build confidence.”\nBut as simple as that sounds, it’s not so simple. Ask social scientists.\nAndrew Gelman, a professor of statistics and political science and director of the Applied Statistics Center at Columbia University, wrote in the journal CHANCE that difficulties with replication in empirical research are pervasive.\n“When an outsider requests data from a published paper, the authors will typically not post or send their data files and code, but instead will point to their sources, so replicators have to figure out exactly what to do from there,” Gelman wrote. “End-to-end replicability is not the norm, even among scholars who actively advocate for the principles of open science.”\nSo goes science, so goes journalism.\nUntil a recent set of exceptions, journalists rarely shared data. The “nerd box” – a sidebar story that explains how a news organization did what they did – is a term that first appeared on NICAR-L, a email listserv of data journalists, in the 1990s.\nIt was a form born in print.\nAs newsrooms adapted to the internet, some news organizations began linking to their data sources if they were online. Often, the data used in stories were obtained through records requests. Sometimes, reporters created the data themselves.\nJournalism, more explicitly than science, is a competitive business. There have been arguments that nerd boxes and downloadable links give too much away to competitors.\nEnter the AP Stylebook.\nThe AP Stylebook argues explicitly for both internal and external replication. Externally, they argue that the “methodology description in the story or accompanying materials should provide a road map to replicate the analysis”, meaning someone else could do the replication post publication.\nInternally, the AP Stylebook says: “If at all possible, an editor or another reporter should attempt to reproduce the results of the analysis and confirm all findings before publication.”\nThere are two problems here.\nFirst is that journalism, unlike science, has no history of replication. There is no “scientific method” for stories. There is no standard “research methods” class taught at every journalism school, at least not where it comes to writing stories. And, beyond that, journalism school isn’t a requirement to get into the news business. In other words, journalism lacks the standards other disciplines have.\nThe second problem is, in many ways, worse: Except for the largest newsrooms, most news organizations lack editors who could replicate the analysis. Many don’t have a second person who would know what to do.\nNot having a second set of eyes in a newsroom is a problem, Thibodeaux acknowledges. Having a data journalism team “is an incredible luxury” at the AP, he said, and their rule is nothing goes on the wire without a second set of eyes.\nThibodeaux, for his part, wants to see fewer “lone nerds in the corner” – it’s too much pressure. That person gets too much credibility from people who don’t understand what they do, and they get too much blame when a mistake is made.\nSo what would replication look like in a newsroom? What does this mean for how newsrooms do data journalism on deadline?\n\n\n\nFor decades, Excel has been the gateway drug for data journalists, the Swiss Army knife of data tools, the “One Tool You Can’t Live Without.” Investigative Reporters and Editors, an organization that trains investigative journalists, have built large amounts of their curricula around Excel. Of the journalism schools that teach data journalism, most of them begin and end with spreadsheets.\nThe Stylebook says at a minimum, today’s data journalists should keep a log that details:\n\nThe source of the data, making sure to work on a copy of the data and not the original file.\nData dictionaries or any other supporting documentation of the data.\n“Description of all steps required to transform the data and perform the analysis.”\n\nThe trouble with Excel (or Google Sheets) is, unless you are keeping meticulous notes on what steps you are taking, there’s no way to keep track. Many data journalists will copy and paste the values of a formula over the formula itself to prevent Excel from fouling up cell references when moving data around – a practical step that also cuts off another path to being able to replicate the results.\nAn increasing number of data journalists are switching to tools like analysis notebooks, which use languages like Python and R, to document their work. The notebooks, generally speaking, allow a data journalist to mix code and explanation in the same document.\nCombined with online sharing tools like GitHub, analysis notebooks seem to solve the problem of replication. But the number using them is small compared to those using spreadsheets. Recent examples of news organizations using analysis notebooks include the Los Angeles Times, the New York Times, FiveThirtyEight, and Buzzfeed.\nPeter Aldous, a data journalist at Buzzfeed recently published a story about how the online news site used machine learning to find airplanes being used to spy on people in American cities. Published with the story is the code Aldous used to build his case.\n“I think of it this way: As a journalist, I don’t like to simply trust what people tell me. Sometimes sources lie. Sometimes they’re just mistaken. So I like to verify what I’m told,” he wrote in an email. “By the same token, why should someone reading one of my articles believe my conclusions, if I don’t provide the evidence that explains how I reached them?”\nThe methodology document, associated code and source data took Aldous a few hours to create. The story, from the initial data work through the reporting required to make sense of it all, took a year. Aldous said there wasn’t a discussion about if the methodology would be published because it was assumed – “it’s written into our DNA at BuzzFeed News.”\n“My background is in science journalism, and before that (way back in the 1980s) in science,” Aldous said. “In science, there’s been a shift from descriptive methods sections to publishing data and analysis code for reproducible research. And I think we’re seeing a similar shift in data journalism. Simply saying what you’ve done is not as powerful as providing the means for others to repeat and build on your work.”\nThibodeaux said that what Buzzfeed and others do with analysis notebooks and code repositories that include their data is “lovely.”\n“That to me is the shining city on the hill,” Thibodeaux said. “We’re not going to get there, and I don’t think we have to for every story and every use case, and I don’t think it’s necessarily practical for every person working with data to get to that point.”\nThere’s a wide spectrum of approaches that still gets journalists to the essence of what the stylebook is trying to do, Thibodeaux said. There are many tools, many strategies, and the AP isn’t going to advocate for any single one of them, he said. They’re just arguing for transparency and replicability, even if that means doing more work.\n“There’s a certain burden that comes with transparency,” he said. “And I think we have to accept that burden.”\nThe question, Thibodeaux said, is what is sufficient? What’s enough transparency? What does someone need for replicability?\n“Maybe we do have to set a higher standard – the more critical the analysis is to the story, and the more complex that analysis is, that’s going to push the bar on what is a sufficient methodology statement,” he said. “And it could end up being a whole code repo in order to just say, this isn’t black magic, here’s how we got it if you’re so interested.”\n\n\n\nThough written almost half a century ago, Meyer foresaw how data journalism was going to arrive in the newsroom.\n“For the new methods to gain currency in journalism, two things must happen,” he wrote. “Editors must feel the need strongly enough to develop the in-house capacity for systematic research … The second need, of course, is for the editors to be able to find the talent to fill this need.”\nMeyer optimistically wrote that journalism schools were prepared to provide that talent – they were not then, and only small handful are now – but students were unlikely to be drawn to these new skills if they didn’t see a chance to use those skills in their careers.\nIt’s taken 45 years, but we are now at this point.\n“The potential for receptivity, especially among the younger generation of newspaper managers, is high,” Meyer wrote.\n\n\n\nFor our purposes in this book, replication requires two things from you, the student: What and why. What is this piece of code doing, and why are you doing that here and now? What lead you to this place? That you can copy and paste code from this book or the internet is not impressive. What is necessary for learning is that you know what a piece of code is doing a thing and why you want to do that thing here.\nHow will we replicate? We’ll make use of special text files – R Markdown, also known as R Notebooks – that combine contextual text; the code we use to load, clean, analyze and visualize data; and the output of that code that allowed us to draw certain conclusions to use in stories.\nIn an R Notebook, there are two blocks: A block that uses markdown, which has no special notation, and a code block. The code blocks can run mulitple languages inside R Studio. There’s R, of course, but we could also run Python, a general purpose scripting language; and SQL, or Structured Query Language, the language of databases.\nFor the rest of the class, we’re going to be working in notebooks.\nIn notebooks, you will both run your code and explain each step, much as I am doing here in this online book. This entire book was produced with R markdown files.\nTo start a notebook in R Studio, you click on the green plus in the top left corner and go down to R Notebook.\n\nIn our first lab, we’ll go through the process of editing a markdown notebook.",
    "crumbs": [
      "Data journalism in the age of replication"
    ]
  },
  {
    "objectID": "replication.html#the-stylebook",
    "href": "replication.html#the-stylebook",
    "title": "Data journalism in the age of replication",
    "section": "",
    "text": "Troy Thibodeaux, the editor of the AP’s data journalism team, said the stylebook entry started when the data team found themselves answering the same questions over and over. With a grant from the Knight Foundation, the team began to document their own standards and turn that into a stylebook section.\nFrom the beginning, they had a fairly clear idea of what they wanted to do – think through a project and ask what the frequently asked questions are that came up. It was not going to be a soup-to-nuts guide to how to do a data project.\nWhen the section came out, eyebrows went up on the replication parts, surprising Thibodeaux.\n“From our perspective, this is a core value for us,” he said. “Just for our own benefit, we need to be able to have someone give us a second set of eyes. We benefit from that every day. We catch things for each other.”\nThibodeaux said the AP data team has two audiences when it comes to replication – they have the readers of the work, and members of the collective who may want to do their own work with the data.\n“This is something that’s essential to the way we work,” he said. “And it’s important in terms of transparency and credibility going forward. We thought it would be kind of unexceptionable.”",
    "crumbs": [
      "Data journalism in the age of replication"
    ]
  },
  {
    "objectID": "replication.html#replication",
    "href": "replication.html#replication",
    "title": "Data journalism in the age of replication",
    "section": "",
    "text": "Meyer, now 86, said he’s delighted to see replication up for discussion now, but warned that we shouldn’t take it too far.\n“Making the analysis replicable was something I worried about from the very beginning,” he wrote in an email. So much so that in 1967, after publishing stories from his landmark survey after the Detroit riots, he shipped the data and backup materials about it to a social science data repository at the University of North Carolina.\nAnd, in doing so, he opened the door to others replicating his results. One scholar attempted to find fault with Meyer’s analysis by slicing the data ever thinner until the differences weren’t significant – gaming the analysis to criticize the stories.\nMeyer believes replication is vitally important, but doesn’t believe it should take on the trappings of science replication, where newsrooms take their own samples or re-survey a community. That would be prohibitively expensive.\nBut journalists should be sharing their data and analysis steps. And it doesn’t need to be complicated, he said.\n“Replication is a theoretical standard, not a requirement that every investigator duplicate his or her own work for every project,” he said. “Giving enough information in the report to enable another investigator to follow in your footsteps is enough. Just telling enough to make replication possible will build confidence.”\nBut as simple as that sounds, it’s not so simple. Ask social scientists.\nAndrew Gelman, a professor of statistics and political science and director of the Applied Statistics Center at Columbia University, wrote in the journal CHANCE that difficulties with replication in empirical research are pervasive.\n“When an outsider requests data from a published paper, the authors will typically not post or send their data files and code, but instead will point to their sources, so replicators have to figure out exactly what to do from there,” Gelman wrote. “End-to-end replicability is not the norm, even among scholars who actively advocate for the principles of open science.”\nSo goes science, so goes journalism.\nUntil a recent set of exceptions, journalists rarely shared data. The “nerd box” – a sidebar story that explains how a news organization did what they did – is a term that first appeared on NICAR-L, a email listserv of data journalists, in the 1990s.\nIt was a form born in print.\nAs newsrooms adapted to the internet, some news organizations began linking to their data sources if they were online. Often, the data used in stories were obtained through records requests. Sometimes, reporters created the data themselves.\nJournalism, more explicitly than science, is a competitive business. There have been arguments that nerd boxes and downloadable links give too much away to competitors.\nEnter the AP Stylebook.\nThe AP Stylebook argues explicitly for both internal and external replication. Externally, they argue that the “methodology description in the story or accompanying materials should provide a road map to replicate the analysis”, meaning someone else could do the replication post publication.\nInternally, the AP Stylebook says: “If at all possible, an editor or another reporter should attempt to reproduce the results of the analysis and confirm all findings before publication.”\nThere are two problems here.\nFirst is that journalism, unlike science, has no history of replication. There is no “scientific method” for stories. There is no standard “research methods” class taught at every journalism school, at least not where it comes to writing stories. And, beyond that, journalism school isn’t a requirement to get into the news business. In other words, journalism lacks the standards other disciplines have.\nThe second problem is, in many ways, worse: Except for the largest newsrooms, most news organizations lack editors who could replicate the analysis. Many don’t have a second person who would know what to do.\nNot having a second set of eyes in a newsroom is a problem, Thibodeaux acknowledges. Having a data journalism team “is an incredible luxury” at the AP, he said, and their rule is nothing goes on the wire without a second set of eyes.\nThibodeaux, for his part, wants to see fewer “lone nerds in the corner” – it’s too much pressure. That person gets too much credibility from people who don’t understand what they do, and they get too much blame when a mistake is made.\nSo what would replication look like in a newsroom? What does this mean for how newsrooms do data journalism on deadline?",
    "crumbs": [
      "Data journalism in the age of replication"
    ]
  },
  {
    "objectID": "replication.html#goodbye-excel",
    "href": "replication.html#goodbye-excel",
    "title": "Data journalism in the age of replication",
    "section": "",
    "text": "For decades, Excel has been the gateway drug for data journalists, the Swiss Army knife of data tools, the “One Tool You Can’t Live Without.” Investigative Reporters and Editors, an organization that trains investigative journalists, have built large amounts of their curricula around Excel. Of the journalism schools that teach data journalism, most of them begin and end with spreadsheets.\nThe Stylebook says at a minimum, today’s data journalists should keep a log that details:\n\nThe source of the data, making sure to work on a copy of the data and not the original file.\nData dictionaries or any other supporting documentation of the data.\n“Description of all steps required to transform the data and perform the analysis.”\n\nThe trouble with Excel (or Google Sheets) is, unless you are keeping meticulous notes on what steps you are taking, there’s no way to keep track. Many data journalists will copy and paste the values of a formula over the formula itself to prevent Excel from fouling up cell references when moving data around – a practical step that also cuts off another path to being able to replicate the results.\nAn increasing number of data journalists are switching to tools like analysis notebooks, which use languages like Python and R, to document their work. The notebooks, generally speaking, allow a data journalist to mix code and explanation in the same document.\nCombined with online sharing tools like GitHub, analysis notebooks seem to solve the problem of replication. But the number using them is small compared to those using spreadsheets. Recent examples of news organizations using analysis notebooks include the Los Angeles Times, the New York Times, FiveThirtyEight, and Buzzfeed.\nPeter Aldous, a data journalist at Buzzfeed recently published a story about how the online news site used machine learning to find airplanes being used to spy on people in American cities. Published with the story is the code Aldous used to build his case.\n“I think of it this way: As a journalist, I don’t like to simply trust what people tell me. Sometimes sources lie. Sometimes they’re just mistaken. So I like to verify what I’m told,” he wrote in an email. “By the same token, why should someone reading one of my articles believe my conclusions, if I don’t provide the evidence that explains how I reached them?”\nThe methodology document, associated code and source data took Aldous a few hours to create. The story, from the initial data work through the reporting required to make sense of it all, took a year. Aldous said there wasn’t a discussion about if the methodology would be published because it was assumed – “it’s written into our DNA at BuzzFeed News.”\n“My background is in science journalism, and before that (way back in the 1980s) in science,” Aldous said. “In science, there’s been a shift from descriptive methods sections to publishing data and analysis code for reproducible research. And I think we’re seeing a similar shift in data journalism. Simply saying what you’ve done is not as powerful as providing the means for others to repeat and build on your work.”\nThibodeaux said that what Buzzfeed and others do with analysis notebooks and code repositories that include their data is “lovely.”\n“That to me is the shining city on the hill,” Thibodeaux said. “We’re not going to get there, and I don’t think we have to for every story and every use case, and I don’t think it’s necessarily practical for every person working with data to get to that point.”\nThere’s a wide spectrum of approaches that still gets journalists to the essence of what the stylebook is trying to do, Thibodeaux said. There are many tools, many strategies, and the AP isn’t going to advocate for any single one of them, he said. They’re just arguing for transparency and replicability, even if that means doing more work.\n“There’s a certain burden that comes with transparency,” he said. “And I think we have to accept that burden.”\nThe question, Thibodeaux said, is what is sufficient? What’s enough transparency? What does someone need for replicability?\n“Maybe we do have to set a higher standard – the more critical the analysis is to the story, and the more complex that analysis is, that’s going to push the bar on what is a sufficient methodology statement,” he said. “And it could end up being a whole code repo in order to just say, this isn’t black magic, here’s how we got it if you’re so interested.”",
    "crumbs": [
      "Data journalism in the age of replication"
    ]
  },
  {
    "objectID": "replication.html#receptivity-is-high",
    "href": "replication.html#receptivity-is-high",
    "title": "Data journalism in the age of replication",
    "section": "",
    "text": "Though written almost half a century ago, Meyer foresaw how data journalism was going to arrive in the newsroom.\n“For the new methods to gain currency in journalism, two things must happen,” he wrote. “Editors must feel the need strongly enough to develop the in-house capacity for systematic research … The second need, of course, is for the editors to be able to find the talent to fill this need.”\nMeyer optimistically wrote that journalism schools were prepared to provide that talent – they were not then, and only small handful are now – but students were unlikely to be drawn to these new skills if they didn’t see a chance to use those skills in their careers.\nIt’s taken 45 years, but we are now at this point.\n“The potential for receptivity, especially among the younger generation of newspaper managers, is high,” Meyer wrote.",
    "crumbs": [
      "Data journalism in the age of replication"
    ]
  },
  {
    "objectID": "replication.html#replication-in-notebooks",
    "href": "replication.html#replication-in-notebooks",
    "title": "Data journalism in the age of replication",
    "section": "",
    "text": "For our purposes in this book, replication requires two things from you, the student: What and why. What is this piece of code doing, and why are you doing that here and now? What lead you to this place? That you can copy and paste code from this book or the internet is not impressive. What is necessary for learning is that you know what a piece of code is doing a thing and why you want to do that thing here.\nHow will we replicate? We’ll make use of special text files – R Markdown, also known as R Notebooks – that combine contextual text; the code we use to load, clean, analyze and visualize data; and the output of that code that allowed us to draw certain conclusions to use in stories.\nIn an R Notebook, there are two blocks: A block that uses markdown, which has no special notation, and a code block. The code blocks can run mulitple languages inside R Studio. There’s R, of course, but we could also run Python, a general purpose scripting language; and SQL, or Structured Query Language, the language of databases.\nFor the rest of the class, we’re going to be working in notebooks.\nIn notebooks, you will both run your code and explain each step, much as I am doing here in this online book. This entire book was produced with R markdown files.\nTo start a notebook in R Studio, you click on the green plus in the top left corner and go down to R Notebook.\n\nIn our first lab, we’ll go through the process of editing a markdown notebook.",
    "crumbs": [
      "Data journalism in the age of replication"
    ]
  },
  {
    "objectID": "publicrecords.html",
    "href": "publicrecords.html",
    "title": "Public records",
    "section": "",
    "text": "Public records are the lifeblood of investigative reporting. They carry their own philosophical framework, in a manner of speaking.\n\nSunlight is the best disinfectant. Corruption hides in the shadows.\nYou paid for it with your taxes. It should be yours (with exceptions).\nJournalism with a capital J is about holding the powerful accountable for their actions.\n\nKeeping those things in mind as you navigate public records is helpful.\n\n\nYour access to public records and public meetings is a matter of the law. As a journalist, it is your job to know this law better than most lawyers. Which law applies depends on which branch of government you are asking. In addition to documents and other kinds of information, FOIA also provides access to structured datasets of the kind we’ll use in this class.\nThe Federal Government is covered by the Freedom of Information Act, or FOIA. FOIA is not a universal term. Do not use it if you are not talking to a federal agency. FOIA is a beacon of openness to the world. FOIA is deeply flawed and frustrating.\nWhy?\n\nThere is no real timetable with FOIA. Requests can take months, even years.\nAs a journalist, you can ask that your request be expedited.\nGuess what? That requires review. More delays.\nExemptions are broad. National security, personal privacy, often overused.\nDenied? You can appeal. More delays.\n\nThe law was enacted in 1966, but it’s still poorly understood by most federal employees, if not outright flouted by political appointees. Lawsuits are common.\nPost 9/11, the Bush administration rolled back many agency rules. Obama ordered a “presumption of openness” but followed it with some of the most restrictive policies ever seen. The Trump Administration, similar to the Obama administration, claims to be the most transparent administration, but has steadily removed records from open access and broadly denied access to records.\nResult? FOIA is in trouble.\nSPJ is a good resource.\n\n\n\nStates are – generally – more open than the federal government. The distance between the government and the governed is smaller. Some states, like Florida and Texas, are very open. Others, like Virginia and Pennsylvania, are not. Maryland is somewhere in the middle.\nThese laws generally give you license to view – and obtain a copy of – a record held by a state or local government agency.\nWhat is a public record? Generally speaking, public records are information stored on paper or in an electronic format held by a state or local government agency, but each state has its own list of types of records – called “exemptions” – that are not subject to disclosure.\nIf a record has both exempt and non-exempt information mixed in, most states require an agency to disclose it after removing the exempt information, a process called “redaction.” Agencies aren’t required to create a record in order to fill your request.\nIn some states but not all – the public information law (or related case law) explicitly dictates that extracting a slice of a database doesn’t constitute creation of a record. Most states can charge you a reasonable fee for time spent retrieving or copying records, though many have provisions to waive those fees for journalists. Every state law operates on a different timeline. Some only require agencies respond in a “reasonable” time, but others spell out exactly how fast an agency must respond to you, and how fast they must turn over the record.\nThe Reporters Committee For Freedom of the Press has a good resource for learning the law in your state.\nPlease and thank you will get you more records than any lawyer or well-written request. Be nice. Be polite. And be persistent. Following up regularly to check on status of a request lets an agency know they can’t ignore you (and some will try). Hunting for records is like any other kind of reporting – you have to do research. You have to ask questions. Ask them: What records do you keep? For how long?\nWhen requesting data, you are going to scare the press office and you are going to confuse the agency lawyer. Request to have their data person on the phone.\nA good source of info? Records retention schedules, often required by law or administrative rule at an agency. Here’s an example from Maryland’s Circuit Courts.",
    "crumbs": [
      "Public records"
    ]
  },
  {
    "objectID": "publicrecords.html#federal-law",
    "href": "publicrecords.html#federal-law",
    "title": "Public records",
    "section": "",
    "text": "Your access to public records and public meetings is a matter of the law. As a journalist, it is your job to know this law better than most lawyers. Which law applies depends on which branch of government you are asking. In addition to documents and other kinds of information, FOIA also provides access to structured datasets of the kind we’ll use in this class.\nThe Federal Government is covered by the Freedom of Information Act, or FOIA. FOIA is not a universal term. Do not use it if you are not talking to a federal agency. FOIA is a beacon of openness to the world. FOIA is deeply flawed and frustrating.\nWhy?\n\nThere is no real timetable with FOIA. Requests can take months, even years.\nAs a journalist, you can ask that your request be expedited.\nGuess what? That requires review. More delays.\nExemptions are broad. National security, personal privacy, often overused.\nDenied? You can appeal. More delays.\n\nThe law was enacted in 1966, but it’s still poorly understood by most federal employees, if not outright flouted by political appointees. Lawsuits are common.\nPost 9/11, the Bush administration rolled back many agency rules. Obama ordered a “presumption of openness” but followed it with some of the most restrictive policies ever seen. The Trump Administration, similar to the Obama administration, claims to be the most transparent administration, but has steadily removed records from open access and broadly denied access to records.\nResult? FOIA is in trouble.\nSPJ is a good resource.",
    "crumbs": [
      "Public records"
    ]
  },
  {
    "objectID": "publicrecords.html#state-law",
    "href": "publicrecords.html#state-law",
    "title": "Public records",
    "section": "",
    "text": "States are – generally – more open than the federal government. The distance between the government and the governed is smaller. Some states, like Florida and Texas, are very open. Others, like Virginia and Pennsylvania, are not. Maryland is somewhere in the middle.\nThese laws generally give you license to view – and obtain a copy of – a record held by a state or local government agency.\nWhat is a public record? Generally speaking, public records are information stored on paper or in an electronic format held by a state or local government agency, but each state has its own list of types of records – called “exemptions” – that are not subject to disclosure.\nIf a record has both exempt and non-exempt information mixed in, most states require an agency to disclose it after removing the exempt information, a process called “redaction.” Agencies aren’t required to create a record in order to fill your request.\nIn some states but not all – the public information law (or related case law) explicitly dictates that extracting a slice of a database doesn’t constitute creation of a record. Most states can charge you a reasonable fee for time spent retrieving or copying records, though many have provisions to waive those fees for journalists. Every state law operates on a different timeline. Some only require agencies respond in a “reasonable” time, but others spell out exactly how fast an agency must respond to you, and how fast they must turn over the record.\nThe Reporters Committee For Freedom of the Press has a good resource for learning the law in your state.\nPlease and thank you will get you more records than any lawyer or well-written request. Be nice. Be polite. And be persistent. Following up regularly to check on status of a request lets an agency know they can’t ignore you (and some will try). Hunting for records is like any other kind of reporting – you have to do research. You have to ask questions. Ask them: What records do you keep? For how long?\nWhen requesting data, you are going to scare the press office and you are going to confuse the agency lawyer. Request to have their data person on the phone.\nA good source of info? Records retention schedules, often required by law or administrative rule at an agency. Here’s an example from Maryland’s Circuit Courts.",
    "crumbs": [
      "Public records"
    ]
  },
  {
    "objectID": "open-refine.html",
    "href": "open-refine.html",
    "title": "Data Cleaning Part III: Open Refine",
    "section": "",
    "text": "Gather ’round kids and let me tell you a tale. Back in the previous century, Los Angeles Times journalists Sara Fritz and Dwight Morris wanted to answer this seemingly simple question: what do political campaigns spend their money on?\nWhile campaigns are required to list a purpose of each expenditure, the problem is that they can choose what words to use. There’s no standard dictionary or drop-down menu to choose from. Want to call that donut purchase “Food”? Sure. What about “Supplies for volunteers”? Works for me. How about “Meals”? Mom might disagree, but the FEC won’t.\nIn order to answer their initial question, the reporters had to standardize their data. In other words, all food-related purchases had to be labeled “Food”. All travel expenses had to be “Travel”. It took them months - many months - to do this for every federal candidate.\nI tell you this because if they had Open Refine, it would have taken them a week or two, not months.\nI did data standardization before Open Refine, and every time I think about it, I get mad.\nFortunately (unfortunately?) several columns in the data we’ll work with are flawed in the same way that the LA Times’ data was, so we can do this work in a better, faster way.\nWe’re going to explore two ways into Open Refine: Through R, and through Open Refine itself.\n\n\nWhat is Open Refine?\nOpen Refine is a software program that has tools – algorithms – that find small differences in text and helps you fix them quickly. How Open Refine finds those small differences is through something called clustering. The algorithms behind clustering are not exclusive to Open Refine, so they can be used elsewhere.\nEnter refinr, a package that contains the same clustering algorithms as Open Refine but all within R. Go ahead and install it if you haven’t already by opening the console and running install.packages(\"refinr\"). Then we can load libraries as we do.\n\nlibrary(tidyverse)\nlibrary(refinr)\nlibrary(janitor)\n\nLet’s load that Maryland state government grants and loan data that we’ve been working with, and to make our standardization work easier we’ll change all the grantees to upper-case.\nNow let’s try and group and count the number of grants by recipient. To make it a bit more manageable, let’s use another string function from stringr and filter for recipients that start with the uppercase “W” or lowercase “w” using the function str_detect() with a regular expression.\nThe filter function in the codeblock below says: look in the city column, and pluck out any value that starts with (the “^” symbol means “starts with”) a lowercase “w” OR (the vertical “|”, called a pipe, means OR) an uppercase “W”.\n\nmd_grant_loans |&gt;\n  group_by(Grantee) |&gt;\n  summarise(\n    count=n()\n  ) |&gt;\n  filter(str_detect(Grantee, '^w|^W')) |&gt;\n  arrange(Grantee)\n\n# A tibble: 342 × 2\n   Grantee                          count\n   &lt;chr&gt;                            &lt;int&gt;\n 1 W. F. DELAUTER & SON, INC.           1\n 2 W.R. GRACE & CO.                     1\n 3 WABASH MARKETS OF MARYLAND, INC.     1\n 4 WAGABOUT                             1\n 5 WAH,LLC                              1\n 6 WALBROOK MILL APARTMENTS             1\n 7 WALDEN SIERRA                        1\n 8 WALDEN SIERRA INC                    1\n 9 WALDEN SIERRA, INC.                 31\n10 WALDORF ARETE, LLC                   1\n# ℹ 332 more rows\n\n\nThere are several problems in this data that will prevent proper grouping and summarizing - you can see multiple versions of “Walden Sierra”, for example. We’ve learned several functions to fix this manually, but that could take awhile.\nBy using the Open Refine package for R, refinr, our hope is that it can identify and standardize the data with a little more ease.\nThe first merging technique that’s part of the refinr package we’ll try is the key_collision_merge.\nThe key collision merge function takes each string and extracts the key parts of it. It then puts every key in a bin based on the keys matching.\nOne rule you should follow when using this is: do not overwrite your original fields. Always work on a copy. If you overwrite your original field, how will you know if it did the right thing? How can you compare it to your original data? To follow this, I’m going to mutate a new field called clean_city and put the results of key collision merge there.\n\ncleaned_md_grant_loans &lt;- md_grant_loans |&gt;\n  mutate(grantee_clean=key_collision_merge(Grantee)) |&gt;\n  select(Grantee, grantee_clean, everything())\n\ncleaned_md_grant_loans\n\n# A tibble: 19,482 × 10\n   Grantee     grantee_clean Grantor `Zip Code` `Fiscal Year` Amount Description\n   &lt;chr&gt;       &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      \n 1 PRINCE GEO… PRINCE GEORG… Commer… 20772               2017 1.28e5 Maryland T…\n 2 ASSOCIATED… ASSOCIATED B… Depart… 21201               2010 9.34e4 Minority O…\n 3 WESTED/PUB… WESTED/PUBLI… Maryla… 94107-1242          2014 1.61e6 GRANTS FOR…\n 4 MARYLAND P… MARYLAND PAT… Depart… 21075               2010 2.14e5 Babies Bor…\n 5 ANACOSTIA … ANACOSTIA WA… Depart… 20710               2017 1.74e5 Payments m…\n 6 WASHINGTON… WASHINGTON C… Depart… 21740               2009 5.59e4 grant fund…\n 7 DOMESTIC V… DOMESTIC VIO… Boards… 21045               2014 1.72e5 Domestic V…\n 8 DACORE INV… DACORE INVES… MD Sma… 20601               2018 1.04e5 Maryland S…\n 9 MOUNT ST M… MOUNT ST MAR… Maryla… 21727               2015 1.75e6 Sellinger …\n10 OLNEY THEA… OLNEY THEATR… Depart… 20830               2010 2.16e5 Grant fund…\n# ℹ 19,472 more rows\n# ℹ 3 more variables: Category &lt;chr&gt;, `Fiscal Period` &lt;dbl&gt;, Date &lt;chr&gt;\n\n\nTo examine changes refinr made, let’s examine the changes it made to cities that start with the letter “W”.\n\ncleaned_md_grant_loans |&gt;\n  group_by(Grantee, grantee_clean) |&gt;\n  summarise(\n    count=n()\n  ) |&gt;\n  filter(str_detect(Grantee, '^w|^W')) |&gt;\n  arrange(Grantee)\n\n`summarise()` has grouped output by 'Grantee'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 342 × 3\n# Groups:   Grantee [342]\n   Grantee                          grantee_clean                    count\n   &lt;chr&gt;                            &lt;chr&gt;                            &lt;int&gt;\n 1 W. F. DELAUTER & SON, INC.       W. F. DELAUTER & SON, INC.           1\n 2 W.R. GRACE & CO.                 W.R. GRACE & CO.                     1\n 3 WABASH MARKETS OF MARYLAND, INC. WABASH MARKETS OF MARYLAND, INC.     1\n 4 WAGABOUT                         WAGABOUT                             1\n 5 WAH,LLC                          WAH,LLC                              1\n 6 WALBROOK MILL APARTMENTS         WALBROOK MILL APARTMENTS             1\n 7 WALDEN SIERRA                    WALDEN SIERRA, INC.                  1\n 8 WALDEN SIERRA INC                WALDEN SIERRA, INC.                  1\n 9 WALDEN SIERRA, INC.              WALDEN SIERRA, INC.                 31\n10 WALDORF ARETE, LLC               WALDORF ARETE, LLC                   1\n# ℹ 332 more rows\n\n\nYou can see several changes on the first page of results, including that refinr consolidated all the Walden Sierra entries into a single one in grantee_clean, which is pretty smart. Other potential changes, grouping together “WALTER’S ART MUSEUM” and “THE WALTERS ART MUSEUM”, didn’t happen. Key collision will do well with different cases, but all of our records are upper case.\nThere’s another merging algorithim that’s part of refinr that works a bit differently, called n_gram_merge(). Let’s try applying that one.\n\ncleaned_md_grant_loans &lt;- md_grant_loans |&gt;\n  mutate(grantee_clean=n_gram_merge(Grantee)) |&gt;\n  select(Grantee, grantee_clean, everything())\n\ncleaned_md_grant_loans\n\n# A tibble: 19,482 × 10\n   Grantee     grantee_clean Grantor `Zip Code` `Fiscal Year` Amount Description\n   &lt;chr&gt;       &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      \n 1 PRINCE GEO… PRINCE GEORG… Commer… 20772               2017 1.28e5 Maryland T…\n 2 ASSOCIATED… ASSOCIATED B… Depart… 21201               2010 9.34e4 Minority O…\n 3 WESTED/PUB… WESTED/PUBLI… Maryla… 94107-1242          2014 1.61e6 GRANTS FOR…\n 4 MARYLAND P… MARYLAND PAT… Depart… 21075               2010 2.14e5 Babies Bor…\n 5 ANACOSTIA … ANACOSTIA WA… Depart… 20710               2017 1.74e5 Payments m…\n 6 WASHINGTON… WASHINGTON C… Depart… 21740               2009 5.59e4 grant fund…\n 7 DOMESTIC V… DOMESTIC VIO… Boards… 21045               2014 1.72e5 Domestic V…\n 8 DACORE INV… DACORE INVES… MD Sma… 20601               2018 1.04e5 Maryland S…\n 9 MOUNT ST M… MOUNT ST MAR… Maryla… 21727               2015 1.75e6 Sellinger …\n10 OLNEY THEA… OLNEY THEATR… Depart… 20830               2010 2.16e5 Grant fund…\n# ℹ 19,472 more rows\n# ℹ 3 more variables: Category &lt;chr&gt;, `Fiscal Period` &lt;dbl&gt;, Date &lt;chr&gt;\n\n\nTo examine changes refinr made with this algorithm, let’s again look at recipients that start with the letter “W”. We see there wasn’t a substantial change from the previous method, and it even missed a few the first method got.\n\ncleaned_md_grant_loans |&gt;\n  group_by(Grantee, grantee_clean) |&gt;\n  summarise(\n    count=n()\n  ) |&gt;\n  filter(str_detect(Grantee, '^w|^W')) |&gt;\n  arrange(Grantee)\n\n`summarise()` has grouped output by 'Grantee'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 342 × 3\n# Groups:   Grantee [342]\n   Grantee                          grantee_clean                    count\n   &lt;chr&gt;                            &lt;chr&gt;                            &lt;int&gt;\n 1 W. F. DELAUTER & SON, INC.       W. F. DELAUTER & SON, INC.           1\n 2 W.R. GRACE & CO.                 W.R. GRACE & CO.                     1\n 3 WABASH MARKETS OF MARYLAND, INC. WABASH MARKETS OF MARYLAND, INC.     1\n 4 WAGABOUT                         WAGABOUT                             1\n 5 WAH,LLC                          WAH,LLC                              1\n 6 WALBROOK MILL APARTMENTS         WALBROOK MILL APARTMENTS             1\n 7 WALDEN SIERRA                    WALDEN SIERRA, INC.                  1\n 8 WALDEN SIERRA INC                WALDEN SIERRA, INC.                  1\n 9 WALDEN SIERRA, INC.              WALDEN SIERRA, INC.                 31\n10 WALDORF ARETE, LLC               WALDORF ARETE, LLC                   1\n# ℹ 332 more rows\n\n\nThis method also made some good changes, but not in every case. No single method will be perfect and often a combination is necessary.\nThat’s how you use the Open Refine r package, refinr.\nNow let’s upload the data to the interactive version of OpenRefine, which really shines at this task.\n\n\n\nOpen Refine is free software. You should download and install it; the most recent version is 3.6.0. Refinr is great for quick things on smaller datasets that you can check to make sure it’s not up to any mischief.\nFor bigger datasets, Open Refine is the way to go. And it has a lot more tools than refinr does (by design).\nAfter you install it, run it. (If you are on a Mac it might tell you that it can’t run the program. Go to System Preferences -&gt; Security & Privacy -&gt; General and click “Open Anyway”.) Open Refine works in the browser, and the app spins up a small web server visible only on your computer to interact with it. A browser will pop up automatically.\nYou first have to import your data into a project. Click the choose files button and upload a csv of the Maryland state grants and loans.\n\n\n\n\n\n\n\n\n\nAfter your data is loaded into the app, you’ll get a screen to look over what the data looks like. On the top right corner, you’ll see a button to create the project. Click that.\n\n\n\n\n\n\n\n\n\nOpen Refine has many, many tools. We’re going to use one piece of it, as a tool for data cleaning. To learn how to use it, we’re going to clean the “Grantee” field.\nFirst, let’s make a copy of the original Grantee column so that we can preserve the original data while cleaning the new one.\nClick the dropdown arrow next to the Grantee column, choose “edit column” &gt; “Add column based on this column”:\n\n\n\n\n\n\n\n\n\nOn the window that pops up, type “grantee_clean” in the “new column name” field. Then hit the OK button. We’ll work on that new column.\n\n\n\n\n\n\n\n\n\nNow, let’s get to work cleaning the grantee_clean column.\nNext to the grantee_clean field name, click the down arrow, then facet, then text facet.\n\n\n\n\n\n\n\n\n\nAfter that, a new box will appear on the left. It tells us how many unique recipient_names there are: 8,956 (you may need to . And, there’s a button on the right of the box that says Cluster.\n\n\n\n\n\n\n\n\n\nClick the cluster button. A new window will pop up, a tool to help us identify things that need to be cleaned, and quickly clean them.\n\n\n\n\n\n\n\n\n\nThe default “method” used is a clustering algorithim called “key collision”, using the fingerprint function. This is the same method we used with the refinr package above.\nAt the top, you’ll see which method was used, and how many clusters that algorithm identified. There are several different methods, each of which work slightly differently and produce different results.\n\n\n\n\n\n\n\n\n\nThen, below that, you can see what those clusters are. Right away, we can see how useful this program is. It identified 23 rows that have some variation on “University of Maryland - Baltimore” in the grantee_clean field. It proposed changing them all to “UNIVERSITY OF MARYLAND BALTIMORE”.\nUsing human judgement, you can say if you agree with the cluster. If you do, click the “merge” checkbox. When it merges, the new result will be what it says in New Cell Value. Most often, that’s the row with the most common result. You also can manually edit the “New Cell Value” if you want it to be something else:\nNow begins the fun part: You have to look at all the clusters found and decide if they are indeed valid. The key collision method is very good, and very conservative. You’ll find that most of them are usually valid.\nBe careful! If you merge two things that aren’t supposed to be together, it will change your data in a way that could lead to inaccurate results.\nWhen you’re done, click Merge Selected and Re-Cluster.\nIf any new clusters come up, evaluate them. Repeat until either no clusters come up or the clusters that do come up are ones you reject.\nNow. Try a new method, maybe the “nearest neighbor levenshtein” method. Notice that it finds even more clusters - using a slightly different approach.\nRinse and repeat.\nYou’ll keep doing this, and if the dataset is reasonably clean, you’ll find the end.\nWhen you’re finished cleaning, click “Merge Selected & Close”.\nThen, export the data as a csv so you can load it back into R.\n\n\n\n\n\n\n\n\n\nA question for all data analysts – if the dataset is bad enough, can it ever be cleaned?\nThere’s no good answer. You have to find it yourself.",
    "crumbs": [
      "Data Cleaning Part III: Open Refine"
    ]
  },
  {
    "objectID": "open-refine.html#refinr-open-refine-in-r",
    "href": "open-refine.html#refinr-open-refine-in-r",
    "title": "Data Cleaning Part III: Open Refine",
    "section": "",
    "text": "What is Open Refine?\nOpen Refine is a software program that has tools – algorithms – that find small differences in text and helps you fix them quickly. How Open Refine finds those small differences is through something called clustering. The algorithms behind clustering are not exclusive to Open Refine, so they can be used elsewhere.\nEnter refinr, a package that contains the same clustering algorithms as Open Refine but all within R. Go ahead and install it if you haven’t already by opening the console and running install.packages(\"refinr\"). Then we can load libraries as we do.\n\nlibrary(tidyverse)\nlibrary(refinr)\nlibrary(janitor)\n\nLet’s load that Maryland state government grants and loan data that we’ve been working with, and to make our standardization work easier we’ll change all the grantees to upper-case.\nNow let’s try and group and count the number of grants by recipient. To make it a bit more manageable, let’s use another string function from stringr and filter for recipients that start with the uppercase “W” or lowercase “w” using the function str_detect() with a regular expression.\nThe filter function in the codeblock below says: look in the city column, and pluck out any value that starts with (the “^” symbol means “starts with”) a lowercase “w” OR (the vertical “|”, called a pipe, means OR) an uppercase “W”.\n\nmd_grant_loans |&gt;\n  group_by(Grantee) |&gt;\n  summarise(\n    count=n()\n  ) |&gt;\n  filter(str_detect(Grantee, '^w|^W')) |&gt;\n  arrange(Grantee)\n\n# A tibble: 342 × 2\n   Grantee                          count\n   &lt;chr&gt;                            &lt;int&gt;\n 1 W. F. DELAUTER & SON, INC.           1\n 2 W.R. GRACE & CO.                     1\n 3 WABASH MARKETS OF MARYLAND, INC.     1\n 4 WAGABOUT                             1\n 5 WAH,LLC                              1\n 6 WALBROOK MILL APARTMENTS             1\n 7 WALDEN SIERRA                        1\n 8 WALDEN SIERRA INC                    1\n 9 WALDEN SIERRA, INC.                 31\n10 WALDORF ARETE, LLC                   1\n# ℹ 332 more rows\n\n\nThere are several problems in this data that will prevent proper grouping and summarizing - you can see multiple versions of “Walden Sierra”, for example. We’ve learned several functions to fix this manually, but that could take awhile.\nBy using the Open Refine package for R, refinr, our hope is that it can identify and standardize the data with a little more ease.\nThe first merging technique that’s part of the refinr package we’ll try is the key_collision_merge.\nThe key collision merge function takes each string and extracts the key parts of it. It then puts every key in a bin based on the keys matching.\nOne rule you should follow when using this is: do not overwrite your original fields. Always work on a copy. If you overwrite your original field, how will you know if it did the right thing? How can you compare it to your original data? To follow this, I’m going to mutate a new field called clean_city and put the results of key collision merge there.\n\ncleaned_md_grant_loans &lt;- md_grant_loans |&gt;\n  mutate(grantee_clean=key_collision_merge(Grantee)) |&gt;\n  select(Grantee, grantee_clean, everything())\n\ncleaned_md_grant_loans\n\n# A tibble: 19,482 × 10\n   Grantee     grantee_clean Grantor `Zip Code` `Fiscal Year` Amount Description\n   &lt;chr&gt;       &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      \n 1 PRINCE GEO… PRINCE GEORG… Commer… 20772               2017 1.28e5 Maryland T…\n 2 ASSOCIATED… ASSOCIATED B… Depart… 21201               2010 9.34e4 Minority O…\n 3 WESTED/PUB… WESTED/PUBLI… Maryla… 94107-1242          2014 1.61e6 GRANTS FOR…\n 4 MARYLAND P… MARYLAND PAT… Depart… 21075               2010 2.14e5 Babies Bor…\n 5 ANACOSTIA … ANACOSTIA WA… Depart… 20710               2017 1.74e5 Payments m…\n 6 WASHINGTON… WASHINGTON C… Depart… 21740               2009 5.59e4 grant fund…\n 7 DOMESTIC V… DOMESTIC VIO… Boards… 21045               2014 1.72e5 Domestic V…\n 8 DACORE INV… DACORE INVES… MD Sma… 20601               2018 1.04e5 Maryland S…\n 9 MOUNT ST M… MOUNT ST MAR… Maryla… 21727               2015 1.75e6 Sellinger …\n10 OLNEY THEA… OLNEY THEATR… Depart… 20830               2010 2.16e5 Grant fund…\n# ℹ 19,472 more rows\n# ℹ 3 more variables: Category &lt;chr&gt;, `Fiscal Period` &lt;dbl&gt;, Date &lt;chr&gt;\n\n\nTo examine changes refinr made, let’s examine the changes it made to cities that start with the letter “W”.\n\ncleaned_md_grant_loans |&gt;\n  group_by(Grantee, grantee_clean) |&gt;\n  summarise(\n    count=n()\n  ) |&gt;\n  filter(str_detect(Grantee, '^w|^W')) |&gt;\n  arrange(Grantee)\n\n`summarise()` has grouped output by 'Grantee'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 342 × 3\n# Groups:   Grantee [342]\n   Grantee                          grantee_clean                    count\n   &lt;chr&gt;                            &lt;chr&gt;                            &lt;int&gt;\n 1 W. F. DELAUTER & SON, INC.       W. F. DELAUTER & SON, INC.           1\n 2 W.R. GRACE & CO.                 W.R. GRACE & CO.                     1\n 3 WABASH MARKETS OF MARYLAND, INC. WABASH MARKETS OF MARYLAND, INC.     1\n 4 WAGABOUT                         WAGABOUT                             1\n 5 WAH,LLC                          WAH,LLC                              1\n 6 WALBROOK MILL APARTMENTS         WALBROOK MILL APARTMENTS             1\n 7 WALDEN SIERRA                    WALDEN SIERRA, INC.                  1\n 8 WALDEN SIERRA INC                WALDEN SIERRA, INC.                  1\n 9 WALDEN SIERRA, INC.              WALDEN SIERRA, INC.                 31\n10 WALDORF ARETE, LLC               WALDORF ARETE, LLC                   1\n# ℹ 332 more rows\n\n\nYou can see several changes on the first page of results, including that refinr consolidated all the Walden Sierra entries into a single one in grantee_clean, which is pretty smart. Other potential changes, grouping together “WALTER’S ART MUSEUM” and “THE WALTERS ART MUSEUM”, didn’t happen. Key collision will do well with different cases, but all of our records are upper case.\nThere’s another merging algorithim that’s part of refinr that works a bit differently, called n_gram_merge(). Let’s try applying that one.\n\ncleaned_md_grant_loans &lt;- md_grant_loans |&gt;\n  mutate(grantee_clean=n_gram_merge(Grantee)) |&gt;\n  select(Grantee, grantee_clean, everything())\n\ncleaned_md_grant_loans\n\n# A tibble: 19,482 × 10\n   Grantee     grantee_clean Grantor `Zip Code` `Fiscal Year` Amount Description\n   &lt;chr&gt;       &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      \n 1 PRINCE GEO… PRINCE GEORG… Commer… 20772               2017 1.28e5 Maryland T…\n 2 ASSOCIATED… ASSOCIATED B… Depart… 21201               2010 9.34e4 Minority O…\n 3 WESTED/PUB… WESTED/PUBLI… Maryla… 94107-1242          2014 1.61e6 GRANTS FOR…\n 4 MARYLAND P… MARYLAND PAT… Depart… 21075               2010 2.14e5 Babies Bor…\n 5 ANACOSTIA … ANACOSTIA WA… Depart… 20710               2017 1.74e5 Payments m…\n 6 WASHINGTON… WASHINGTON C… Depart… 21740               2009 5.59e4 grant fund…\n 7 DOMESTIC V… DOMESTIC VIO… Boards… 21045               2014 1.72e5 Domestic V…\n 8 DACORE INV… DACORE INVES… MD Sma… 20601               2018 1.04e5 Maryland S…\n 9 MOUNT ST M… MOUNT ST MAR… Maryla… 21727               2015 1.75e6 Sellinger …\n10 OLNEY THEA… OLNEY THEATR… Depart… 20830               2010 2.16e5 Grant fund…\n# ℹ 19,472 more rows\n# ℹ 3 more variables: Category &lt;chr&gt;, `Fiscal Period` &lt;dbl&gt;, Date &lt;chr&gt;\n\n\nTo examine changes refinr made with this algorithm, let’s again look at recipients that start with the letter “W”. We see there wasn’t a substantial change from the previous method, and it even missed a few the first method got.\n\ncleaned_md_grant_loans |&gt;\n  group_by(Grantee, grantee_clean) |&gt;\n  summarise(\n    count=n()\n  ) |&gt;\n  filter(str_detect(Grantee, '^w|^W')) |&gt;\n  arrange(Grantee)\n\n`summarise()` has grouped output by 'Grantee'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 342 × 3\n# Groups:   Grantee [342]\n   Grantee                          grantee_clean                    count\n   &lt;chr&gt;                            &lt;chr&gt;                            &lt;int&gt;\n 1 W. F. DELAUTER & SON, INC.       W. F. DELAUTER & SON, INC.           1\n 2 W.R. GRACE & CO.                 W.R. GRACE & CO.                     1\n 3 WABASH MARKETS OF MARYLAND, INC. WABASH MARKETS OF MARYLAND, INC.     1\n 4 WAGABOUT                         WAGABOUT                             1\n 5 WAH,LLC                          WAH,LLC                              1\n 6 WALBROOK MILL APARTMENTS         WALBROOK MILL APARTMENTS             1\n 7 WALDEN SIERRA                    WALDEN SIERRA, INC.                  1\n 8 WALDEN SIERRA INC                WALDEN SIERRA, INC.                  1\n 9 WALDEN SIERRA, INC.              WALDEN SIERRA, INC.                 31\n10 WALDORF ARETE, LLC               WALDORF ARETE, LLC                   1\n# ℹ 332 more rows\n\n\nThis method also made some good changes, but not in every case. No single method will be perfect and often a combination is necessary.\nThat’s how you use the Open Refine r package, refinr.\nNow let’s upload the data to the interactive version of OpenRefine, which really shines at this task.",
    "crumbs": [
      "Data Cleaning Part III: Open Refine"
    ]
  },
  {
    "objectID": "open-refine.html#manually-cleaning-data-with-open-refine",
    "href": "open-refine.html#manually-cleaning-data-with-open-refine",
    "title": "Data Cleaning Part III: Open Refine",
    "section": "",
    "text": "Open Refine is free software. You should download and install it; the most recent version is 3.6.0. Refinr is great for quick things on smaller datasets that you can check to make sure it’s not up to any mischief.\nFor bigger datasets, Open Refine is the way to go. And it has a lot more tools than refinr does (by design).\nAfter you install it, run it. (If you are on a Mac it might tell you that it can’t run the program. Go to System Preferences -&gt; Security & Privacy -&gt; General and click “Open Anyway”.) Open Refine works in the browser, and the app spins up a small web server visible only on your computer to interact with it. A browser will pop up automatically.\nYou first have to import your data into a project. Click the choose files button and upload a csv of the Maryland state grants and loans.\n\n\n\n\n\n\n\n\n\nAfter your data is loaded into the app, you’ll get a screen to look over what the data looks like. On the top right corner, you’ll see a button to create the project. Click that.\n\n\n\n\n\n\n\n\n\nOpen Refine has many, many tools. We’re going to use one piece of it, as a tool for data cleaning. To learn how to use it, we’re going to clean the “Grantee” field.\nFirst, let’s make a copy of the original Grantee column so that we can preserve the original data while cleaning the new one.\nClick the dropdown arrow next to the Grantee column, choose “edit column” &gt; “Add column based on this column”:\n\n\n\n\n\n\n\n\n\nOn the window that pops up, type “grantee_clean” in the “new column name” field. Then hit the OK button. We’ll work on that new column.\n\n\n\n\n\n\n\n\n\nNow, let’s get to work cleaning the grantee_clean column.\nNext to the grantee_clean field name, click the down arrow, then facet, then text facet.\n\n\n\n\n\n\n\n\n\nAfter that, a new box will appear on the left. It tells us how many unique recipient_names there are: 8,956 (you may need to . And, there’s a button on the right of the box that says Cluster.\n\n\n\n\n\n\n\n\n\nClick the cluster button. A new window will pop up, a tool to help us identify things that need to be cleaned, and quickly clean them.\n\n\n\n\n\n\n\n\n\nThe default “method” used is a clustering algorithim called “key collision”, using the fingerprint function. This is the same method we used with the refinr package above.\nAt the top, you’ll see which method was used, and how many clusters that algorithm identified. There are several different methods, each of which work slightly differently and produce different results.\n\n\n\n\n\n\n\n\n\nThen, below that, you can see what those clusters are. Right away, we can see how useful this program is. It identified 23 rows that have some variation on “University of Maryland - Baltimore” in the grantee_clean field. It proposed changing them all to “UNIVERSITY OF MARYLAND BALTIMORE”.\nUsing human judgement, you can say if you agree with the cluster. If you do, click the “merge” checkbox. When it merges, the new result will be what it says in New Cell Value. Most often, that’s the row with the most common result. You also can manually edit the “New Cell Value” if you want it to be something else:\nNow begins the fun part: You have to look at all the clusters found and decide if they are indeed valid. The key collision method is very good, and very conservative. You’ll find that most of them are usually valid.\nBe careful! If you merge two things that aren’t supposed to be together, it will change your data in a way that could lead to inaccurate results.\nWhen you’re done, click Merge Selected and Re-Cluster.\nIf any new clusters come up, evaluate them. Repeat until either no clusters come up or the clusters that do come up are ones you reject.\nNow. Try a new method, maybe the “nearest neighbor levenshtein” method. Notice that it finds even more clusters - using a slightly different approach.\nRinse and repeat.\nYou’ll keep doing this, and if the dataset is reasonably clean, you’ll find the end.\nWhen you’re finished cleaning, click “Merge Selected & Close”.\nThen, export the data as a csv so you can load it back into R.\n\n\n\n\n\n\n\n\n\nA question for all data analysts – if the dataset is bad enough, can it ever be cleaned?\nThere’s no good answer. You have to find it yourself.",
    "crumbs": [
      "Data Cleaning Part III: Open Refine"
    ]
  },
  {
    "objectID": "merging.html",
    "href": "merging.html",
    "title": "Combining and joining",
    "section": "",
    "text": "Often, as data journalists, we’re looking at data across time or at data stored in multiple tables. And to do that, we need to often need to merge that data together.\nDepending on what we have, we may just need to stack data on top of each other to make new data. If we have 2019 data and 2018 data and we want that to be one file, we stack them. If we have a dataset of cows in counties and a dataset of populations in county, we’re going to join those two together on the county – the common element.\nLet’s explore.\n\n\nLet’s say that we have Maryland county voter registration data from five different elections in five different files. They have the same record layout and the same number of counties (plus Baltimore City). We can combine them into a single dataframe.\nLet’s do what we need to import them properly. I’ve merged it all into one step for each of the datasets.\n\nlibrary(tidyverse)\n\n\ncounty_voters_2016 &lt;- read_csv(\"data/county_voters_2016.csv\")\n\nRows: 24 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): COUNTY\ndbl (8): YEAR, DEM, REP, LIB, GRN, UNA, OTH, TOTAL\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncounty_voters_2018 &lt;- read_csv(\"data/county_voters_2018.csv\")\n\nRows: 24 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): COUNTY\ndbl (8): YEAR, DEM, REP, LIB, GRN, UNA, OTH, TOTAL\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncounty_voters_2020 &lt;- read_csv(\"data/county_voters_2020.csv\")\n\nRows: 24 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): COUNTY\ndbl (8): YEAR, DEM, REP, GRN, LIB, OTH, UNA, TOTAL\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncounty_voters_2022 &lt;- read_csv(\"data/county_voters_2022.csv\")\n\nRows: 24 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): COUNTY\ndbl (8): YEAR, DEM, REP, GRN, LIB, UNA, OTH, TOTAL\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncounty_voters_2024 &lt;- read_csv(\"data/county_voters_2024.csv\")\n\nRows: 25 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): COUNTY\ndbl (7): YEAR, DEM, REP, LIB, UNA, OTH, TOTAL\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAll of these datasets have the same number of columns, all with the same names, so if we want to merge them together to compare them over time, we need to stack them together. The verb here, in R, is bind_rows. You tell the function what you want to combine and it does it, assuming that you’ve got column names in common containing identically formatted data.\nSince we have five dataframes, we’re going to need to pass them as a list, meaning they’ll be enclosed inside the list function.\n\ncounty_voters_combined &lt;- bind_rows(list(county_voters_2016, county_voters_2018, county_voters_2020, county_voters_2022, county_voters_2024))\n\nAnd boom, like that, we have 125 rows of data together instead of five dataframes. Now we can ask more interesting questions like how a county’s registration patterns have changed over time.\nThere are plenty of uses for bind_rows: any regularly updated data that comes in the same format like crime reports or award recipients or player game statistics. Or election results.\n\n\n\nMore complicated is when you have two separate tables that are connected by a common element or elements. But there’s a verb for that, too: join.\nLet’s start by reading in some Maryland 2020 county population data:\n\nmaryland_population &lt;- read_csv('data/maryland_population_2020.csv')\n\nRows: 24 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): COUNTY\ndbl (1): POP2020\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nOne of the columns we have is called county, which is what we have in our county_voters_2020 dataframe.\nTo put the Maryland population data and voter registration data together, we need to use something called a join. There are different kinds of joins. It’s better if you think of two tables sitting next to each other. A left_join takes all the records from the left table and only the records that match in the right one. A right_join does the same thing. An inner_join takes only the records where they are equal. There’s one other join – a full_join which returns all rows of both, regardless of if there’s a match – but I’ve never once had a use for a full join.\nIn the best-case scenario, the two tables we want to join share a common column. In this case, both of our tables have a column called county that has the same characteristics: values in both look identical, including how they distinguish Baltimore City from Baltimore County. This is important, because joins work on exact matches.\nWe can do this join multiple ways and get a similar result. We can put the population file on the left and the registration data on the right and use a left join to get them all together. And we use by= to join by the correct column. I’m going to count the rows at the end. The reason I’m doing this is important: Rule 1 in joining data is having an idea of what you are expecting to get. So with a left join with population on the left, I have 24 rows, so I expect to get 24 rows when I’m done.\n\nmaryland_population |&gt; left_join(county_voters_2020, by=\"COUNTY\") |&gt; nrow()\n\n[1] 24\n\n\nRemove the nrow and run it again for yourself. By default, dplyr will do a “natural” join, where it’ll match all the matching columns in both tables. So if we take out the by, it’ll use all the common columns between the tables. That may not be right in every instance but let’s try it. If it works, we should get 24 rows.\n\nmaryland_population |&gt; left_join(county_voters_2020)\n\nJoining with `by = join_by(COUNTY)`\n\n\n# A tibble: 24 × 10\n   COUNTY           POP2020  YEAR    DEM    REP   GRN   LIB   OTH    UNA  TOTAL\n   &lt;chr&gt;              &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Allegany           68106  2020  12820  22530    74   204   434   7674  43736\n 2 Anne Arundel      588261  2020 174494 135457   564  1922  3017  90162 405616\n 3 Baltimore City    585708  2020 311610  30163   802   951  3709  52450 399685\n 4 Baltimore County  854535  2020 313870 142534   898  2227  6303 100576 566408\n 5 Calvert            92783  2020  24587  28181    89   332   617  14178  67984\n 6 Caroline           33293  2020   6629  10039    33    86   182   4208  21177\n 7 Carroll           172891  2020  33662  63967   155   670  1137  25770 125361\n 8 Cecil             103725  2020  21601  30880   103   341   784  15110  68819\n 9 Charles           166617  2020  72416  24711   112   349   865  19849 118302\n10 Dorchester         32531  2020   9848   8730    19    78   164   3348  22187\n# ℹ 14 more rows\n\n\nSince we only have one column in common between the two tables, the join only used that column. And we got the same answer. If we had more columns in common, you could see in your results columns with .X after them - that’s a sign of duplicative columns between two tables, and you may decide you don’t need both moving forward.\nLet’s save our joined data to a new dataframe, but this time let’s remove the select function so we don’t limit the columns to just three.\n\nmaryland_population_with_voters &lt;- maryland_population |&gt; left_join(county_voters_2020)\n\nJoining with `by = join_by(COUNTY)`\n\n\nNow, with our joined data, we can answer questions in a more useful way. But joins can do even more than just bring data together; they can include additional data to enable you to ask more sophisticated questions. Right now we have registered voters and total population. But we can do more.\nLet’s try adding more Maryland demographic data to the mix. Using a file describing the 18-and-over population (from which eligible voters come) from the state’s data catalog, we can read it into R:\n\nmaryland_demographics &lt;- read_csv('data/maryland_demographics.csv')\n\nRows: 24 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): NAME\ndbl (10): GEOCODE, pop_18_over, pop_one_race, pop_white, pop_black, pop_nati...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAgain, we can use a left_join to make our demographic data available. This time we’ll need to specify the two fields to join because they do not have identical names. We’ll use COUNTY from our population data and NAME from the demographic data, and the order matters - the first column is from the dataframe you name first.\n\nmaryland_population_with_voters_and_demographics &lt;- maryland_population_with_voters |&gt; left_join(maryland_demographics, by=c(\"COUNTY\"=\"NAME\"))\n\nNow we’ve got population data and demographic data by county. That means we can draw from both datasets in asking our questions. For example, we could see the counties with the highest 18+ Black population as a percentage of all population 18 and over and also the percentage of Democrats in that county.\nWe can get this by using mutate and arrange:\n\nmaryland_population_with_voters_and_demographics |&gt;\n  mutate(pct_black_18_plus = (pop_black/pop_18_over)*100, pct_dems = (DEM/TOTAL)*100) |&gt;\n  arrange(desc(pct_black_18_plus)) |&gt;\n  select(COUNTY, pct_black_18_plus, pct_dems)\n\n# A tibble: 24 × 3\n   COUNTY           pct_black_18_plus pct_dems\n   &lt;chr&gt;                        &lt;dbl&gt;    &lt;dbl&gt;\n 1 Prince George's               60.9     78.3\n 2 Baltimore City                56.3     78.0\n 3 Charles                       48.2     61.2\n 4 Somerset                      39.0     41.8\n 5 Baltimore County              28.8     55.4\n 6 Dorchester                    26.2     44.4\n 7 Wicomico                      25.6     42.3\n 8 Howard                        18.7     52.4\n 9 Montgomery                    18.1     61.0\n10 Anne Arundel                  17.4     43.0\n# ℹ 14 more rows\n\n\nIf you know Maryland political demographics, this result isn’t too surprising, but Somerset County - the state’s 2nd smallest in terms of population - stands out for its Black population, which is a greater percentage than Baltimore County and Montgomery County.\nLet’s change that to look at Asian population:\n\nmaryland_population_with_voters_and_demographics |&gt;\n  mutate(pct_asian_18_plus = (pop_asian/pop_18_over)*100, pct_dems = (DEM/TOTAL)*100) |&gt;\n  arrange(desc(pct_asian_18_plus)) |&gt;\n  select(COUNTY, pct_asian_18_plus, pct_dems)\n\n# A tibble: 24 × 3\n   COUNTY           pct_asian_18_plus pct_dems\n   &lt;chr&gt;                        &lt;dbl&gt;    &lt;dbl&gt;\n 1 Howard                       19.4      52.4\n 2 Montgomery                   16.0      61.0\n 3 Baltimore County              6.34     55.4\n 4 Frederick                     4.88     38.9\n 5 Prince George's               4.68     78.3\n 6 Anne Arundel                  4.52     43.0\n 7 Baltimore City                4.17     78.0\n 8 Charles                       3.55     61.2\n 9 Harford                       3.15     35.4\n10 St. Mary's                    3.13     35.7\n# ℹ 14 more rows\n\n\nHere, Howard and Montgomery County stand out in terms of the percentage of Asian population 18 and over. The jurisdictions with the highest percentage of Democrats - Prince George’s and Baltimore City - have small Asian populations.\nSometimes joins look like they should work but don’t. Often this is due to the two columns you’re joining on having different data types: joining a  column to a  column, for example. Let’s walk through an example of that using some demographic data by zip code.\n\nmaryland_zcta &lt;- read_csv('data/maryland_zcta.csv')\n\nRows: 468 Columns: 40\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): FIRST_CLAS, FIRST_MTFC, FIRST_FUNC, REPORT_2_P, REPORT_9_P\ndbl (35): OBJECTID_1, ZCTA5CE10, FIRST_STAT, FIRST_GEOI, ZCTA5N, STATE, AREA...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(maryland_zcta)\n\nRows: 468\nColumns: 40\n$ OBJECTID_1 &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ ZCTA5CE10  &lt;dbl&gt; 20601, 20602, 20603, 20606, 20607, 20608, 20609, 20611, 206…\n$ FIRST_STAT &lt;dbl&gt; 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,…\n$ FIRST_GEOI &lt;dbl&gt; 2420601, 2420602, 2420603, 2420606, 2420607, 2420608, 24206…\n$ FIRST_CLAS &lt;chr&gt; \"B5\", \"B5\", \"B5\", \"B5\", \"B5\", \"B5\", \"B5\", \"B5\", \"B5\", \"B5\",…\n$ FIRST_MTFC &lt;chr&gt; \"G6350\", \"G6350\", \"G6350\", \"G6350\", \"G6350\", \"G6350\", \"G635…\n$ FIRST_FUNC &lt;chr&gt; \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\",…\n$ ZCTA5N     &lt;dbl&gt; 20601, 20602, 20603, 20606, 20607, 20608, 20609, 20611, 206…\n$ STATE      &lt;dbl&gt; 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,…\n$ AREALAND   &lt;dbl&gt; 115635266, 35830723, 44239637, 7501011, 54357590, 45583064,…\n$ AREAWATR   &lt;dbl&gt; 387684, 352762, 219356, 1248760, 448221, 5330329, 6602735, …\n$ POP100     &lt;dbl&gt; 24156, 24955, 28967, 431, 9802, 919, 1120, 1078, 261, 11860…\n$ HU100      &lt;dbl&gt; 8722, 9736, 10317, 230, 3504, 426, 554, 413, 142, 4424, 204…\n$ NHW        &lt;dbl&gt; 9785, 8466, 9625, 377, 2165, 438, 1009, 798, 245, 4044, 352…\n$ NHB        &lt;dbl&gt; 11146, 13054, 15025, 45, 6321, 453, 82, 215, 12, 6786, 32, …\n$ NHAI       &lt;dbl&gt; 155, 116, 98, 1, 33, 5, 2, 5, 0, 106, 2, 32, 3, 4, 38, 8, 1…\n$ NHA        &lt;dbl&gt; 880, 731, 1446, 4, 560, 2, 1, 10, 0, 186, 3, 165, 5, 1, 402…\n$ NHNH       &lt;dbl&gt; 11, 15, 24, 0, 3, 0, 1, 0, 0, 4, 1, 2, 0, 0, 4, 1, 0, 3, 1,…\n$ NHO        &lt;dbl&gt; 48, 58, 65, 0, 6, 0, 0, 0, 0, 8, 0, 1, 0, 3, 5, 8, 0, 5, 10…\n$ NHT        &lt;dbl&gt; 849, 999, 1091, 0, 234, 9, 15, 33, 1, 321, 13, 213, 14, 4, …\n$ HISP       &lt;dbl&gt; 1282, 1516, 1593, 4, 480, 12, 10, 17, 3, 405, 2, 244, 9, 7,…\n$ PNHW       &lt;dbl&gt; 40.5, 33.9, 33.2, 87.5, 22.1, 47.7, 90.1, 74.0, 93.9, 34.1,…\n$ PNHB       &lt;dbl&gt; 46.1, 52.3, 51.9, 10.4, 64.5, 49.3, 7.3, 19.9, 4.6, 57.2, 7…\n$ PNHAI      &lt;dbl&gt; 0.6, 0.5, 0.3, 0.2, 0.3, 0.5, 0.2, 0.5, 0.0, 0.9, 0.5, 0.5,…\n$ PNHA       &lt;dbl&gt; 3.6, 2.9, 5.0, 0.9, 5.7, 0.2, 0.1, 0.9, 0.0, 1.6, 0.7, 2.8,…\n$ PNHNH      &lt;dbl&gt; 0.0, 0.1, 0.1, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.2, 0.0,…\n$ PNHO       &lt;dbl&gt; 0.2, 0.2, 0.2, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0,…\n$ PNHT       &lt;dbl&gt; 3.5, 4.0, 3.8, 0.0, 2.4, 1.0, 1.3, 3.1, 0.4, 2.7, 3.2, 3.6,…\n$ PHISP      &lt;dbl&gt; 5.3, 6.1, 5.5, 0.9, 4.9, 1.3, 0.9, 1.6, 1.1, 3.4, 0.5, 4.2,…\n$ POP65_     &lt;dbl&gt; 1922, 1964, 1400, 108, 847, 173, 271, 129, 54, 1372, 73, 55…\n$ PCTPOP65_  &lt;dbl&gt; 8.0, 7.9, 4.8, 25.1, 8.6, 18.8, 24.2, 12.0, 20.7, 11.6, 18.…\n$ MEDAGE     &lt;dbl&gt; 37.3, 32.6, 34.5, 49.1, 40.9, 46.6, 47.6, 44.3, 47.3, 40.8,…\n$ VACNS      &lt;dbl&gt; 376, 769, 531, 15, 172, 39, 32, 22, 14, 249, 18, 158, 8, 18…\n$ PVACNS     &lt;dbl&gt; 4.3, 7.9, 5.1, 6.5, 4.9, 9.2, 5.8, 5.3, 9.9, 5.6, 8.8, 7.2,…\n$ PHOWN      &lt;dbl&gt; 71.1, 59.7, 73.8, 49.7, 83.1, 60.4, 44.8, 63.8, 38.3, 73.9,…\n$ PWOMORT    &lt;dbl&gt; 11.2, 9.0, 4.7, 39.3, 10.3, 28.2, 38.7, 21.8, 43.9, 17.4, 2…\n$ PRENT      &lt;dbl&gt; 19.9, 34.4, 22.6, 18.1, 7.4, 15.9, 27.0, 18.3, 31.7, 10.5, …\n$ PLT18SP    &lt;dbl&gt; 30.4, 43.6, 29.9, 31.2, 22.1, 14.1, 28.9, 24.5, 43.9, 26.7,…\n$ REPORT_2_P &lt;chr&gt; \"http://mdpgis.mdp.state.md.us/Census2010/PDF/00_SF1DP_2Pro…\n$ REPORT_9_P &lt;chr&gt; \"http://mdpgis.mdp.state.md.us/census2010/PDF/00_SF1_9PROFI…\n\n\nYou can see that ZCTA5N, the column representing the Zip Code Tabulation Area, is a numeric column. But should it be? Do we ever want to know the average zip code in Maryland? Zip codes and ZCTAs look like numbers but really are character columns. Let’s change that so that we can be sure to join them correctly with other data where the zip codes are not numbers. We’ll use mutate:\n\nmaryland_zcta &lt;- maryland_zcta |&gt; mutate(across(ZCTA5N, as.character))\n\nWhat’s happening here is that we’re telling R to take all of the values in the ZCTA5N column and make them “as.character”. If we wanted to change a column to numeric, we’d do “as.numeric”. When you join two dataframes, the join columns must be the same datatype.\nJoining datasets allows you to expand the range and sophistication of questions you’re able to ask. It is one of the most powerful tools in a journalist’s toolkit.",
    "crumbs": [
      "Combining and joining"
    ]
  },
  {
    "objectID": "merging.html#combining-data-stacking",
    "href": "merging.html#combining-data-stacking",
    "title": "Combining and joining",
    "section": "",
    "text": "Let’s say that we have Maryland county voter registration data from five different elections in five different files. They have the same record layout and the same number of counties (plus Baltimore City). We can combine them into a single dataframe.\nLet’s do what we need to import them properly. I’ve merged it all into one step for each of the datasets.\n\nlibrary(tidyverse)\n\n\ncounty_voters_2016 &lt;- read_csv(\"data/county_voters_2016.csv\")\n\nRows: 24 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): COUNTY\ndbl (8): YEAR, DEM, REP, LIB, GRN, UNA, OTH, TOTAL\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncounty_voters_2018 &lt;- read_csv(\"data/county_voters_2018.csv\")\n\nRows: 24 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): COUNTY\ndbl (8): YEAR, DEM, REP, LIB, GRN, UNA, OTH, TOTAL\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncounty_voters_2020 &lt;- read_csv(\"data/county_voters_2020.csv\")\n\nRows: 24 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): COUNTY\ndbl (8): YEAR, DEM, REP, GRN, LIB, OTH, UNA, TOTAL\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncounty_voters_2022 &lt;- read_csv(\"data/county_voters_2022.csv\")\n\nRows: 24 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): COUNTY\ndbl (8): YEAR, DEM, REP, GRN, LIB, UNA, OTH, TOTAL\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncounty_voters_2024 &lt;- read_csv(\"data/county_voters_2024.csv\")\n\nRows: 25 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): COUNTY\ndbl (7): YEAR, DEM, REP, LIB, UNA, OTH, TOTAL\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAll of these datasets have the same number of columns, all with the same names, so if we want to merge them together to compare them over time, we need to stack them together. The verb here, in R, is bind_rows. You tell the function what you want to combine and it does it, assuming that you’ve got column names in common containing identically formatted data.\nSince we have five dataframes, we’re going to need to pass them as a list, meaning they’ll be enclosed inside the list function.\n\ncounty_voters_combined &lt;- bind_rows(list(county_voters_2016, county_voters_2018, county_voters_2020, county_voters_2022, county_voters_2024))\n\nAnd boom, like that, we have 125 rows of data together instead of five dataframes. Now we can ask more interesting questions like how a county’s registration patterns have changed over time.\nThere are plenty of uses for bind_rows: any regularly updated data that comes in the same format like crime reports or award recipients or player game statistics. Or election results.",
    "crumbs": [
      "Combining and joining"
    ]
  },
  {
    "objectID": "merging.html#joining-data",
    "href": "merging.html#joining-data",
    "title": "Combining and joining",
    "section": "",
    "text": "More complicated is when you have two separate tables that are connected by a common element or elements. But there’s a verb for that, too: join.\nLet’s start by reading in some Maryland 2020 county population data:\n\nmaryland_population &lt;- read_csv('data/maryland_population_2020.csv')\n\nRows: 24 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): COUNTY\ndbl (1): POP2020\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nOne of the columns we have is called county, which is what we have in our county_voters_2020 dataframe.\nTo put the Maryland population data and voter registration data together, we need to use something called a join. There are different kinds of joins. It’s better if you think of two tables sitting next to each other. A left_join takes all the records from the left table and only the records that match in the right one. A right_join does the same thing. An inner_join takes only the records where they are equal. There’s one other join – a full_join which returns all rows of both, regardless of if there’s a match – but I’ve never once had a use for a full join.\nIn the best-case scenario, the two tables we want to join share a common column. In this case, both of our tables have a column called county that has the same characteristics: values in both look identical, including how they distinguish Baltimore City from Baltimore County. This is important, because joins work on exact matches.\nWe can do this join multiple ways and get a similar result. We can put the population file on the left and the registration data on the right and use a left join to get them all together. And we use by= to join by the correct column. I’m going to count the rows at the end. The reason I’m doing this is important: Rule 1 in joining data is having an idea of what you are expecting to get. So with a left join with population on the left, I have 24 rows, so I expect to get 24 rows when I’m done.\n\nmaryland_population |&gt; left_join(county_voters_2020, by=\"COUNTY\") |&gt; nrow()\n\n[1] 24\n\n\nRemove the nrow and run it again for yourself. By default, dplyr will do a “natural” join, where it’ll match all the matching columns in both tables. So if we take out the by, it’ll use all the common columns between the tables. That may not be right in every instance but let’s try it. If it works, we should get 24 rows.\n\nmaryland_population |&gt; left_join(county_voters_2020)\n\nJoining with `by = join_by(COUNTY)`\n\n\n# A tibble: 24 × 10\n   COUNTY           POP2020  YEAR    DEM    REP   GRN   LIB   OTH    UNA  TOTAL\n   &lt;chr&gt;              &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Allegany           68106  2020  12820  22530    74   204   434   7674  43736\n 2 Anne Arundel      588261  2020 174494 135457   564  1922  3017  90162 405616\n 3 Baltimore City    585708  2020 311610  30163   802   951  3709  52450 399685\n 4 Baltimore County  854535  2020 313870 142534   898  2227  6303 100576 566408\n 5 Calvert            92783  2020  24587  28181    89   332   617  14178  67984\n 6 Caroline           33293  2020   6629  10039    33    86   182   4208  21177\n 7 Carroll           172891  2020  33662  63967   155   670  1137  25770 125361\n 8 Cecil             103725  2020  21601  30880   103   341   784  15110  68819\n 9 Charles           166617  2020  72416  24711   112   349   865  19849 118302\n10 Dorchester         32531  2020   9848   8730    19    78   164   3348  22187\n# ℹ 14 more rows\n\n\nSince we only have one column in common between the two tables, the join only used that column. And we got the same answer. If we had more columns in common, you could see in your results columns with .X after them - that’s a sign of duplicative columns between two tables, and you may decide you don’t need both moving forward.\nLet’s save our joined data to a new dataframe, but this time let’s remove the select function so we don’t limit the columns to just three.\n\nmaryland_population_with_voters &lt;- maryland_population |&gt; left_join(county_voters_2020)\n\nJoining with `by = join_by(COUNTY)`\n\n\nNow, with our joined data, we can answer questions in a more useful way. But joins can do even more than just bring data together; they can include additional data to enable you to ask more sophisticated questions. Right now we have registered voters and total population. But we can do more.\nLet’s try adding more Maryland demographic data to the mix. Using a file describing the 18-and-over population (from which eligible voters come) from the state’s data catalog, we can read it into R:\n\nmaryland_demographics &lt;- read_csv('data/maryland_demographics.csv')\n\nRows: 24 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): NAME\ndbl (10): GEOCODE, pop_18_over, pop_one_race, pop_white, pop_black, pop_nati...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAgain, we can use a left_join to make our demographic data available. This time we’ll need to specify the two fields to join because they do not have identical names. We’ll use COUNTY from our population data and NAME from the demographic data, and the order matters - the first column is from the dataframe you name first.\n\nmaryland_population_with_voters_and_demographics &lt;- maryland_population_with_voters |&gt; left_join(maryland_demographics, by=c(\"COUNTY\"=\"NAME\"))\n\nNow we’ve got population data and demographic data by county. That means we can draw from both datasets in asking our questions. For example, we could see the counties with the highest 18+ Black population as a percentage of all population 18 and over and also the percentage of Democrats in that county.\nWe can get this by using mutate and arrange:\n\nmaryland_population_with_voters_and_demographics |&gt;\n  mutate(pct_black_18_plus = (pop_black/pop_18_over)*100, pct_dems = (DEM/TOTAL)*100) |&gt;\n  arrange(desc(pct_black_18_plus)) |&gt;\n  select(COUNTY, pct_black_18_plus, pct_dems)\n\n# A tibble: 24 × 3\n   COUNTY           pct_black_18_plus pct_dems\n   &lt;chr&gt;                        &lt;dbl&gt;    &lt;dbl&gt;\n 1 Prince George's               60.9     78.3\n 2 Baltimore City                56.3     78.0\n 3 Charles                       48.2     61.2\n 4 Somerset                      39.0     41.8\n 5 Baltimore County              28.8     55.4\n 6 Dorchester                    26.2     44.4\n 7 Wicomico                      25.6     42.3\n 8 Howard                        18.7     52.4\n 9 Montgomery                    18.1     61.0\n10 Anne Arundel                  17.4     43.0\n# ℹ 14 more rows\n\n\nIf you know Maryland political demographics, this result isn’t too surprising, but Somerset County - the state’s 2nd smallest in terms of population - stands out for its Black population, which is a greater percentage than Baltimore County and Montgomery County.\nLet’s change that to look at Asian population:\n\nmaryland_population_with_voters_and_demographics |&gt;\n  mutate(pct_asian_18_plus = (pop_asian/pop_18_over)*100, pct_dems = (DEM/TOTAL)*100) |&gt;\n  arrange(desc(pct_asian_18_plus)) |&gt;\n  select(COUNTY, pct_asian_18_plus, pct_dems)\n\n# A tibble: 24 × 3\n   COUNTY           pct_asian_18_plus pct_dems\n   &lt;chr&gt;                        &lt;dbl&gt;    &lt;dbl&gt;\n 1 Howard                       19.4      52.4\n 2 Montgomery                   16.0      61.0\n 3 Baltimore County              6.34     55.4\n 4 Frederick                     4.88     38.9\n 5 Prince George's               4.68     78.3\n 6 Anne Arundel                  4.52     43.0\n 7 Baltimore City                4.17     78.0\n 8 Charles                       3.55     61.2\n 9 Harford                       3.15     35.4\n10 St. Mary's                    3.13     35.7\n# ℹ 14 more rows\n\n\nHere, Howard and Montgomery County stand out in terms of the percentage of Asian population 18 and over. The jurisdictions with the highest percentage of Democrats - Prince George’s and Baltimore City - have small Asian populations.\nSometimes joins look like they should work but don’t. Often this is due to the two columns you’re joining on having different data types: joining a  column to a  column, for example. Let’s walk through an example of that using some demographic data by zip code.\n\nmaryland_zcta &lt;- read_csv('data/maryland_zcta.csv')\n\nRows: 468 Columns: 40\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): FIRST_CLAS, FIRST_MTFC, FIRST_FUNC, REPORT_2_P, REPORT_9_P\ndbl (35): OBJECTID_1, ZCTA5CE10, FIRST_STAT, FIRST_GEOI, ZCTA5N, STATE, AREA...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(maryland_zcta)\n\nRows: 468\nColumns: 40\n$ OBJECTID_1 &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ ZCTA5CE10  &lt;dbl&gt; 20601, 20602, 20603, 20606, 20607, 20608, 20609, 20611, 206…\n$ FIRST_STAT &lt;dbl&gt; 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,…\n$ FIRST_GEOI &lt;dbl&gt; 2420601, 2420602, 2420603, 2420606, 2420607, 2420608, 24206…\n$ FIRST_CLAS &lt;chr&gt; \"B5\", \"B5\", \"B5\", \"B5\", \"B5\", \"B5\", \"B5\", \"B5\", \"B5\", \"B5\",…\n$ FIRST_MTFC &lt;chr&gt; \"G6350\", \"G6350\", \"G6350\", \"G6350\", \"G6350\", \"G6350\", \"G635…\n$ FIRST_FUNC &lt;chr&gt; \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\",…\n$ ZCTA5N     &lt;dbl&gt; 20601, 20602, 20603, 20606, 20607, 20608, 20609, 20611, 206…\n$ STATE      &lt;dbl&gt; 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,…\n$ AREALAND   &lt;dbl&gt; 115635266, 35830723, 44239637, 7501011, 54357590, 45583064,…\n$ AREAWATR   &lt;dbl&gt; 387684, 352762, 219356, 1248760, 448221, 5330329, 6602735, …\n$ POP100     &lt;dbl&gt; 24156, 24955, 28967, 431, 9802, 919, 1120, 1078, 261, 11860…\n$ HU100      &lt;dbl&gt; 8722, 9736, 10317, 230, 3504, 426, 554, 413, 142, 4424, 204…\n$ NHW        &lt;dbl&gt; 9785, 8466, 9625, 377, 2165, 438, 1009, 798, 245, 4044, 352…\n$ NHB        &lt;dbl&gt; 11146, 13054, 15025, 45, 6321, 453, 82, 215, 12, 6786, 32, …\n$ NHAI       &lt;dbl&gt; 155, 116, 98, 1, 33, 5, 2, 5, 0, 106, 2, 32, 3, 4, 38, 8, 1…\n$ NHA        &lt;dbl&gt; 880, 731, 1446, 4, 560, 2, 1, 10, 0, 186, 3, 165, 5, 1, 402…\n$ NHNH       &lt;dbl&gt; 11, 15, 24, 0, 3, 0, 1, 0, 0, 4, 1, 2, 0, 0, 4, 1, 0, 3, 1,…\n$ NHO        &lt;dbl&gt; 48, 58, 65, 0, 6, 0, 0, 0, 0, 8, 0, 1, 0, 3, 5, 8, 0, 5, 10…\n$ NHT        &lt;dbl&gt; 849, 999, 1091, 0, 234, 9, 15, 33, 1, 321, 13, 213, 14, 4, …\n$ HISP       &lt;dbl&gt; 1282, 1516, 1593, 4, 480, 12, 10, 17, 3, 405, 2, 244, 9, 7,…\n$ PNHW       &lt;dbl&gt; 40.5, 33.9, 33.2, 87.5, 22.1, 47.7, 90.1, 74.0, 93.9, 34.1,…\n$ PNHB       &lt;dbl&gt; 46.1, 52.3, 51.9, 10.4, 64.5, 49.3, 7.3, 19.9, 4.6, 57.2, 7…\n$ PNHAI      &lt;dbl&gt; 0.6, 0.5, 0.3, 0.2, 0.3, 0.5, 0.2, 0.5, 0.0, 0.9, 0.5, 0.5,…\n$ PNHA       &lt;dbl&gt; 3.6, 2.9, 5.0, 0.9, 5.7, 0.2, 0.1, 0.9, 0.0, 1.6, 0.7, 2.8,…\n$ PNHNH      &lt;dbl&gt; 0.0, 0.1, 0.1, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.2, 0.0,…\n$ PNHO       &lt;dbl&gt; 0.2, 0.2, 0.2, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0,…\n$ PNHT       &lt;dbl&gt; 3.5, 4.0, 3.8, 0.0, 2.4, 1.0, 1.3, 3.1, 0.4, 2.7, 3.2, 3.6,…\n$ PHISP      &lt;dbl&gt; 5.3, 6.1, 5.5, 0.9, 4.9, 1.3, 0.9, 1.6, 1.1, 3.4, 0.5, 4.2,…\n$ POP65_     &lt;dbl&gt; 1922, 1964, 1400, 108, 847, 173, 271, 129, 54, 1372, 73, 55…\n$ PCTPOP65_  &lt;dbl&gt; 8.0, 7.9, 4.8, 25.1, 8.6, 18.8, 24.2, 12.0, 20.7, 11.6, 18.…\n$ MEDAGE     &lt;dbl&gt; 37.3, 32.6, 34.5, 49.1, 40.9, 46.6, 47.6, 44.3, 47.3, 40.8,…\n$ VACNS      &lt;dbl&gt; 376, 769, 531, 15, 172, 39, 32, 22, 14, 249, 18, 158, 8, 18…\n$ PVACNS     &lt;dbl&gt; 4.3, 7.9, 5.1, 6.5, 4.9, 9.2, 5.8, 5.3, 9.9, 5.6, 8.8, 7.2,…\n$ PHOWN      &lt;dbl&gt; 71.1, 59.7, 73.8, 49.7, 83.1, 60.4, 44.8, 63.8, 38.3, 73.9,…\n$ PWOMORT    &lt;dbl&gt; 11.2, 9.0, 4.7, 39.3, 10.3, 28.2, 38.7, 21.8, 43.9, 17.4, 2…\n$ PRENT      &lt;dbl&gt; 19.9, 34.4, 22.6, 18.1, 7.4, 15.9, 27.0, 18.3, 31.7, 10.5, …\n$ PLT18SP    &lt;dbl&gt; 30.4, 43.6, 29.9, 31.2, 22.1, 14.1, 28.9, 24.5, 43.9, 26.7,…\n$ REPORT_2_P &lt;chr&gt; \"http://mdpgis.mdp.state.md.us/Census2010/PDF/00_SF1DP_2Pro…\n$ REPORT_9_P &lt;chr&gt; \"http://mdpgis.mdp.state.md.us/census2010/PDF/00_SF1_9PROFI…\n\n\nYou can see that ZCTA5N, the column representing the Zip Code Tabulation Area, is a numeric column. But should it be? Do we ever want to know the average zip code in Maryland? Zip codes and ZCTAs look like numbers but really are character columns. Let’s change that so that we can be sure to join them correctly with other data where the zip codes are not numbers. We’ll use mutate:\n\nmaryland_zcta &lt;- maryland_zcta |&gt; mutate(across(ZCTA5N, as.character))\n\nWhat’s happening here is that we’re telling R to take all of the values in the ZCTA5N column and make them “as.character”. If we wanted to change a column to numeric, we’d do “as.numeric”. When you join two dataframes, the join columns must be the same datatype.\nJoining datasets allows you to expand the range and sophistication of questions you’re able to ask. It is one of the most powerful tools in a journalist’s toolkit.",
    "crumbs": [
      "Combining and joining"
    ]
  },
  {
    "objectID": "github.html",
    "href": "github.html",
    "title": "Using GitHub",
    "section": "",
    "text": "GitHub is a platform for managing and storing files, data and code built atop Git, a popular open source version control software. GitHub accounts are free and it’s easy to get started. The one prerequisite is that you have Git installed on your local computer. There are installers for Mac, Windows and Linux.\n\n\nVersion control is based on the ideas that you want to keep track of changes you make to a collection of files and that multiple people can work together without getting in each other’s way or having to do things in a set order. For individual users, it’s great for making sure that you always have your work.\nGitHub users work in what are known as repositories on their local computers and also push changes to a remote repository located on GitHub. That remote repository is key: if you lose your computer, you can fetch a version of your files from GitHub. If you want to work with someone else on the same files, you can each have a local copy, push changes to GitHub and then pull each others’ changes back to your local computers.\nSo, like Microsoft Word’s track changes but with a remote backup and multiple editors.\n\n\n\nAfter installing Git and signing up for a GitHub account, download and install GitHub Desktop. It will have you sign into your GitHub account and then you’ll have access to any existing repositories. If you don’t have any, that’s fine! You can make one locally.\nGitHub has good documentation for working in the Desktop app, and while the emphasis in this book will be on using GitHub for version control, it also supports recording issues (read: problems or questions) with your files, contributing to projects that aren’t yours and more.\n\n\n\nAlthough our focus is on the GitHub Desktop app, you can use Git and GitHub from your computer’s command line interface, and GitHub has a purpose-built command line client, too. GitHub can also serve as a publishing platform for many types of files, and entire websites are hosted on GitHub Pages.",
    "crumbs": [
      "Using GitHub"
    ]
  },
  {
    "objectID": "github.html#how-it-works",
    "href": "github.html#how-it-works",
    "title": "Using GitHub",
    "section": "",
    "text": "Version control is based on the ideas that you want to keep track of changes you make to a collection of files and that multiple people can work together without getting in each other’s way or having to do things in a set order. For individual users, it’s great for making sure that you always have your work.\nGitHub users work in what are known as repositories on their local computers and also push changes to a remote repository located on GitHub. That remote repository is key: if you lose your computer, you can fetch a version of your files from GitHub. If you want to work with someone else on the same files, you can each have a local copy, push changes to GitHub and then pull each others’ changes back to your local computers.\nSo, like Microsoft Word’s track changes but with a remote backup and multiple editors.",
    "crumbs": [
      "Using GitHub"
    ]
  },
  {
    "objectID": "github.html#getting-started",
    "href": "github.html#getting-started",
    "title": "Using GitHub",
    "section": "",
    "text": "After installing Git and signing up for a GitHub account, download and install GitHub Desktop. It will have you sign into your GitHub account and then you’ll have access to any existing repositories. If you don’t have any, that’s fine! You can make one locally.\nGitHub has good documentation for working in the Desktop app, and while the emphasis in this book will be on using GitHub for version control, it also supports recording issues (read: problems or questions) with your files, contributing to projects that aren’t yours and more.",
    "crumbs": [
      "Using GitHub"
    ]
  },
  {
    "objectID": "github.html#advanced-use",
    "href": "github.html#advanced-use",
    "title": "Using GitHub",
    "section": "",
    "text": "Although our focus is on the GitHub Desktop app, you can use Git and GitHub from your computer’s command line interface, and GitHub has a purpose-built command line client, too. GitHub can also serve as a publishing platform for many types of files, and entire websites are hosted on GitHub Pages.",
    "crumbs": [
      "Using GitHub"
    ]
  },
  {
    "objectID": "ethics.html",
    "href": "ethics.html",
    "title": "Ethics in data journalism",
    "section": "",
    "text": "This originally appeared on Open News in March 2013.\nIn 2009, a senior web editor asked me and another developer a question: could our development group build a new news application for Tampabay.com that displayed a gallery of mug shots? Stories about goofy crimes with strange mug shots were popular with readers. The vision, on the part of management, was a website that would display the mugshots collected every day from publicly available websites by two editors—well paid, professional editors with other responsibilities.\nNewsrooms are many things. Alive. Filled with energy. Fueled by stress, coffee and profanity. But they are also idea factories. Day after day, ideas come from everywhere. From reporters on the beat. From editors reading random things. From who knows where. Some of them are brilliant. Some would never work. Most need more people and time than are available. And some are dumber than anyone cares to admit.\nWe thought this idea was nuts. Why would we pay someone, let alone an editor, to fetch mug shots from the Internet? Couldn’t we do that with a scraper?\nIf only this were the most complex question we would face.\nBecause given enough time and enough creativity, scraping a mug shot website is easy. You need to recognize a pattern, parse some HTML and gather the pieces you need. At least that’s how it should work. Police agencies that put mugs online usually buy software from a vendor. Apparently, those vendors enjoy making horrific, non-standard, broken-in-interesting-and-unique-ways HTML. You’ll swear. A lot. But you’ll grind it out. And that’s part of the fun. Scraping isn’t any fun with clean, semantic, valid HTML. And scraping mug shot websites, by that definition, is tons of fun.\nThe complexity comes when you realize the data you are dealing with represent real people’s lives.\n\n\nThe first problem we faced, long before we actually had data, was that data has a life of its own. Because we were going to put this information in front of a big audience, Google was going to find it. That meant if we used our normal open door policy for the Googlebot, someone’s mug shot was going to be the first record in Google for their name, most likely. It would show up first because most people dont actively cultivate their name on the web for visibility in Google. It would show up first because we know how SEO works and they dont. It would show up first because our site would have more traffic than their site, and so Google would rank us higher.\nAnd that record in Google would exist as long as the URL did. Longer when you consider the cached versions Google keeps.\nThat was a problem because here are the things we could not know:\n\nWas this person wrongly arrested?\nWas this person innocent?\nWere the charges dropped against this person?\nDid this person lie about any of their information?\n\n\n\n\nSo it turned out to be very important to know the Googlebot. It’s your friend … until it isn’t. We went to our bosses and said words that no one had said to them before: we did not want Google to index these pages. In a news organization, the page view is the coin of the realm. It is — unfortunately — how many things are evaluated when the bosses ask if it was successful or not. So, with that in mind, Google is your friend. Google brings you traffic. Indeed, Google is your single largest referrer of traffic at a news organization, so you want to throw the doors open and make friends with the Googlebot.\nBut here we were, saying Google wasn’t our friend and that we needed to keep the Googlebot out. And, thankfully, our bosses listened to our argument. They too didn’t want to be the first result in Google for someone.\nSo, to make sure we were telling the Googlebot no, we used three lines of defense. We told it no in robots.txt and on individual pages as a meta tag, and we put the most interesting bits of data into a simple JavaScript wrapper that made it hard on the bot if the first two things failed.\nThe second solution had ramifications beyond the Googlebot. We decided that we were not trying to make a complete copy of the public record. That existed already. If you wanted to look at the actual public records, the sheriff’s offices in the area had websites and they were the official keeper of the record. We were making browsing those images easy, but we were not the public record.\nThat freedom had two consequences: it meant our scrapers could, at a certain point and given a number of failures, just give up on getting a mug. Data entered by humans will be flawed. There will be mistakes. Because of that, our code would have to try and deal with that. Well, there’s an infinite number of ways people can mess things up, so we decided that since we were not going to be an exact copy of the public record, we could deal with the most common failures and dump the rest. During testing, we were getting well over 98% of mugs without having to spend our lives coding for every possible variation of typo.\nThe second consequence of the decision actually came from the newspapers lawyers. They asked a question that dumbfounded us: How long are you keeping mugs? We never thought about it. Storage was cheap. We just assumed we’d keep them all. But, why should we do that? If we’re not a copy of the public record, we dont have to keep them. And, since we didnt know the result of each case, keeping them was really kind of pointless.\nSo, we asked around: How long does a misdemeanor case take to reach a judgement? The answer we got from various sources was about 60 days. From arrest to adjudication, it took about two months. So, at the 60 day mark, we deleted the data. We had no way of knowing if someone was guilty or innocent, so all of them had to go. We even called the script The Reaper.\nWe’d later learn that the practical impacts of this were nil. People looked at the day’s mugs and moved on. The amount of traffic a mug got after the day of arrest was nearly zero.\n\n\n\nThe life of your data matters. You have to ask yourself, Is it useful forever? Does it become harmful after a set time? We had to confront the real impact of deleting mugs after 60 days. People share them, potentially lengthening their lifetime long after they’ve fallen off the homepage. Delete them and that URL goes away.\nWe couldn’t stop people from sharing links on social media—and indeed probably didn’t want to stop them from doing it. Heck, we did it while we were building it. We kept IMing URLs to each other. And that’s how we realized we had a problem. All our work to minimize the impact on someone wrongly accused of a crime could be damaged by someone sharing a link on Facebook or Twitter.\nThere’s a difference between frictionless and unobstructed sharing and some reasonable constraints.\nWe couldn’t stop people from posting a mug on Facebook, but we didn’t have to make it easy and we didn’t have to put that mug front and center. So we blocked Facebook from using the mug as the thumbnail image on a shared link. And, after 60 days, the URL to the mug will throw a 404 page not found error. Because it’s gone.\nWe couldn’t block Google from memorializing someone’s arrest, only to let it live on forever on Facebook.\n\n\n\nThe last problem didn’t come until months later. And it came in the middle of the night. Two months after we launched, my phone rang at 1 a.m. This is never a good thing. It was my fellow developer, Jeremy Bowers, now with NPR, calling me from a hotel in Washington DC where he was supposed to appear in a wedding the next day. Amazon, which we were using for image hosting, was alerting him that our bandwidth bills had tripled on that day. And our traffic hadn’t changed.\nWhat was going on?\nAfter some digging, we found out that another developer had scraped our site—because we were so much easier to scrape than the Sheriff’s office sites—and had built a game out of our data called Pick the Perp. There were two problems with this: 1. The game was going viral on Digg (when it was still a thing) and Reddit. It was getting huge traffic. 2. That developer had hotlinked our images. He/she was serving them from our S3 account, which meant we were bearing the costs. And they were going up exponentially by the minute.\nWhat we didn’t realize when we launched, and what we figured out after Pick the Perp, was that we had become data provider, in a sense. We had done the hard work of getting the data out of a website and we put it into neat, semantic, easily digestible HTML. If you were after a stream of mugshots, why go through all the hassle of scraping four different sheriff’s office’s horrible HTML when you could just come get ours easily?\nWhoever built Pick the Perp, at least at the time, chose to use our site. But, in doing so, they also chose to hotlink images—use the URL of our S3 bucket, which cost us money—instead of hosting the images themselves.\nThat was a problem we hadn’t considered. People hotlink images all the time. And, until those images are deleted from our system, they’ll stay hotlinked somewhere.\nAmazon’s S3 has a system where you can attach a key to a file that expires after X period of time. In other words, the URL to your image only lasts 15 minutes, or an hour, or however long you decide, before it breaks. It gives you fine grained control over how long someone can use your image URL.\nSo at 3 a.m., after two hours of pulling our hair out, we figured out how to sync our image keys with our cache refreshes. So every 15 minutes, a url to an image expired and Pick the Perp came crashing down.\nWhile the Pick the Perp example is an easy one—it’s never cool to hotlink an image—it does raise an issue to consider. Because you are thinking carefully about how to build your app the right way doesn’t mean someone else will. And it doesn’t mean they won’t just go take your data from your site. So how could you deal with that? Make the data available as a download? Create an API that uses your same ethical constructs? Terms of service? All have pros and cons and are worth talking about before going forward.\n\n\n\nWe live in marvelous times. The web offers you no end of tools to make things on the web, to put data from here on there, to make information freely available. But, we’re an optimistic lot. Developers want to believe that their software is being used only for good. And most people will use it for good. But, there are times where the data you’re working with makes people uncomfortable. Indeed, much of journalism is about making people uncomfortable, publishing things that make people angry, or expose people who don’t want to be exposed.\nWhat I want you to think about, before you write a line of code, is what does it mean to put your data on the internet? What could happen, good and bad? What should you do to be responsible about it?\nBecause it can have consequences.\nOn Dec. 23, the Journal News in New York published a map of every legal gun permit holder in their home circulation county. It was a public record. They put it into Google Fusion Tables and Google dutifully geocoded the addresses. It was a short distance to publication from there.\nWithin days, angry gun owners had besieged the newspaper with complaints, saying the paper had given criminals directions to people’s houses where they’d find valuable guns to steal. They said the paper had violated their privacy. One outraged gun owner assembled a list of the paper’s staff, including their home addresses, telephone numbers, email addresses and other details. The paper hired armed security to stand watch at the paper.\nBy February, the New York state legislature removed handgun permits from the public record, citing the Journal News as the reason.\nThere’s no end of arguments to be had about this, but the simple fact is this: The reason people were angry was because you could click on a dot on the map and see a name and an address. In Fusion Tables, removing that info window would take two clicks.\nBecause you can put data on the web does not mean you should put data on the web. And there’s a difference between a record being “public” and “in front of a large audience.”\nSo before you write the first line of code, ask these questions:\n\nThis data is public, but is it widely available? And does making it widely available and easy to use change anything?\nShould this data be searchable in a search engine?\nDoes this data expose information someone has a reasonable expectation that it would remain at least semi-private?\nDoes this data change over time?\nDoes this data expire?\nWhat is my strategy to update or delete data?\nHow easy should it be to share this data on social media?\nHow should I deal with other people who want this data? API? Bulk download?\n\nYour answers to these questions will guide how you build your app. And hopefully, it’ll guide you to better decisions about how to build an app with ethics in mind.",
    "crumbs": [
      "Ethics in data journalism"
    ]
  },
  {
    "objectID": "ethics.html#problems",
    "href": "ethics.html#problems",
    "title": "Ethics in data journalism",
    "section": "",
    "text": "The first problem we faced, long before we actually had data, was that data has a life of its own. Because we were going to put this information in front of a big audience, Google was going to find it. That meant if we used our normal open door policy for the Googlebot, someone’s mug shot was going to be the first record in Google for their name, most likely. It would show up first because most people dont actively cultivate their name on the web for visibility in Google. It would show up first because we know how SEO works and they dont. It would show up first because our site would have more traffic than their site, and so Google would rank us higher.\nAnd that record in Google would exist as long as the URL did. Longer when you consider the cached versions Google keeps.\nThat was a problem because here are the things we could not know:\n\nWas this person wrongly arrested?\nWas this person innocent?\nWere the charges dropped against this person?\nDid this person lie about any of their information?",
    "crumbs": [
      "Ethics in data journalism"
    ]
  },
  {
    "objectID": "ethics.html#the-googlebot",
    "href": "ethics.html#the-googlebot",
    "title": "Ethics in data journalism",
    "section": "",
    "text": "So it turned out to be very important to know the Googlebot. It’s your friend … until it isn’t. We went to our bosses and said words that no one had said to them before: we did not want Google to index these pages. In a news organization, the page view is the coin of the realm. It is — unfortunately — how many things are evaluated when the bosses ask if it was successful or not. So, with that in mind, Google is your friend. Google brings you traffic. Indeed, Google is your single largest referrer of traffic at a news organization, so you want to throw the doors open and make friends with the Googlebot.\nBut here we were, saying Google wasn’t our friend and that we needed to keep the Googlebot out. And, thankfully, our bosses listened to our argument. They too didn’t want to be the first result in Google for someone.\nSo, to make sure we were telling the Googlebot no, we used three lines of defense. We told it no in robots.txt and on individual pages as a meta tag, and we put the most interesting bits of data into a simple JavaScript wrapper that made it hard on the bot if the first two things failed.\nThe second solution had ramifications beyond the Googlebot. We decided that we were not trying to make a complete copy of the public record. That existed already. If you wanted to look at the actual public records, the sheriff’s offices in the area had websites and they were the official keeper of the record. We were making browsing those images easy, but we were not the public record.\nThat freedom had two consequences: it meant our scrapers could, at a certain point and given a number of failures, just give up on getting a mug. Data entered by humans will be flawed. There will be mistakes. Because of that, our code would have to try and deal with that. Well, there’s an infinite number of ways people can mess things up, so we decided that since we were not going to be an exact copy of the public record, we could deal with the most common failures and dump the rest. During testing, we were getting well over 98% of mugs without having to spend our lives coding for every possible variation of typo.\nThe second consequence of the decision actually came from the newspapers lawyers. They asked a question that dumbfounded us: How long are you keeping mugs? We never thought about it. Storage was cheap. We just assumed we’d keep them all. But, why should we do that? If we’re not a copy of the public record, we dont have to keep them. And, since we didnt know the result of each case, keeping them was really kind of pointless.\nSo, we asked around: How long does a misdemeanor case take to reach a judgement? The answer we got from various sources was about 60 days. From arrest to adjudication, it took about two months. So, at the 60 day mark, we deleted the data. We had no way of knowing if someone was guilty or innocent, so all of them had to go. We even called the script The Reaper.\nWe’d later learn that the practical impacts of this were nil. People looked at the day’s mugs and moved on. The amount of traffic a mug got after the day of arrest was nearly zero.",
    "crumbs": [
      "Ethics in data journalism"
    ]
  },
  {
    "objectID": "ethics.html#data-lifetimes",
    "href": "ethics.html#data-lifetimes",
    "title": "Ethics in data journalism",
    "section": "",
    "text": "The life of your data matters. You have to ask yourself, Is it useful forever? Does it become harmful after a set time? We had to confront the real impact of deleting mugs after 60 days. People share them, potentially lengthening their lifetime long after they’ve fallen off the homepage. Delete them and that URL goes away.\nWe couldn’t stop people from sharing links on social media—and indeed probably didn’t want to stop them from doing it. Heck, we did it while we were building it. We kept IMing URLs to each other. And that’s how we realized we had a problem. All our work to minimize the impact on someone wrongly accused of a crime could be damaged by someone sharing a link on Facebook or Twitter.\nThere’s a difference between frictionless and unobstructed sharing and some reasonable constraints.\nWe couldn’t stop people from posting a mug on Facebook, but we didn’t have to make it easy and we didn’t have to put that mug front and center. So we blocked Facebook from using the mug as the thumbnail image on a shared link. And, after 60 days, the URL to the mug will throw a 404 page not found error. Because it’s gone.\nWe couldn’t block Google from memorializing someone’s arrest, only to let it live on forever on Facebook.",
    "crumbs": [
      "Ethics in data journalism"
    ]
  },
  {
    "objectID": "ethics.html#you-are-a-data-provider",
    "href": "ethics.html#you-are-a-data-provider",
    "title": "Ethics in data journalism",
    "section": "",
    "text": "The last problem didn’t come until months later. And it came in the middle of the night. Two months after we launched, my phone rang at 1 a.m. This is never a good thing. It was my fellow developer, Jeremy Bowers, now with NPR, calling me from a hotel in Washington DC where he was supposed to appear in a wedding the next day. Amazon, which we were using for image hosting, was alerting him that our bandwidth bills had tripled on that day. And our traffic hadn’t changed.\nWhat was going on?\nAfter some digging, we found out that another developer had scraped our site—because we were so much easier to scrape than the Sheriff’s office sites—and had built a game out of our data called Pick the Perp. There were two problems with this: 1. The game was going viral on Digg (when it was still a thing) and Reddit. It was getting huge traffic. 2. That developer had hotlinked our images. He/she was serving them from our S3 account, which meant we were bearing the costs. And they were going up exponentially by the minute.\nWhat we didn’t realize when we launched, and what we figured out after Pick the Perp, was that we had become data provider, in a sense. We had done the hard work of getting the data out of a website and we put it into neat, semantic, easily digestible HTML. If you were after a stream of mugshots, why go through all the hassle of scraping four different sheriff’s office’s horrible HTML when you could just come get ours easily?\nWhoever built Pick the Perp, at least at the time, chose to use our site. But, in doing so, they also chose to hotlink images—use the URL of our S3 bucket, which cost us money—instead of hosting the images themselves.\nThat was a problem we hadn’t considered. People hotlink images all the time. And, until those images are deleted from our system, they’ll stay hotlinked somewhere.\nAmazon’s S3 has a system where you can attach a key to a file that expires after X period of time. In other words, the URL to your image only lasts 15 minutes, or an hour, or however long you decide, before it breaks. It gives you fine grained control over how long someone can use your image URL.\nSo at 3 a.m., after two hours of pulling our hair out, we figured out how to sync our image keys with our cache refreshes. So every 15 minutes, a url to an image expired and Pick the Perp came crashing down.\nWhile the Pick the Perp example is an easy one—it’s never cool to hotlink an image—it does raise an issue to consider. Because you are thinking carefully about how to build your app the right way doesn’t mean someone else will. And it doesn’t mean they won’t just go take your data from your site. So how could you deal with that? Make the data available as a download? Create an API that uses your same ethical constructs? Terms of service? All have pros and cons and are worth talking about before going forward.",
    "crumbs": [
      "Ethics in data journalism"
    ]
  },
  {
    "objectID": "ethics.html#ethical-data",
    "href": "ethics.html#ethical-data",
    "title": "Ethics in data journalism",
    "section": "",
    "text": "We live in marvelous times. The web offers you no end of tools to make things on the web, to put data from here on there, to make information freely available. But, we’re an optimistic lot. Developers want to believe that their software is being used only for good. And most people will use it for good. But, there are times where the data you’re working with makes people uncomfortable. Indeed, much of journalism is about making people uncomfortable, publishing things that make people angry, or expose people who don’t want to be exposed.\nWhat I want you to think about, before you write a line of code, is what does it mean to put your data on the internet? What could happen, good and bad? What should you do to be responsible about it?\nBecause it can have consequences.\nOn Dec. 23, the Journal News in New York published a map of every legal gun permit holder in their home circulation county. It was a public record. They put it into Google Fusion Tables and Google dutifully geocoded the addresses. It was a short distance to publication from there.\nWithin days, angry gun owners had besieged the newspaper with complaints, saying the paper had given criminals directions to people’s houses where they’d find valuable guns to steal. They said the paper had violated their privacy. One outraged gun owner assembled a list of the paper’s staff, including their home addresses, telephone numbers, email addresses and other details. The paper hired armed security to stand watch at the paper.\nBy February, the New York state legislature removed handgun permits from the public record, citing the Journal News as the reason.\nThere’s no end of arguments to be had about this, but the simple fact is this: The reason people were angry was because you could click on a dot on the map and see a name and an address. In Fusion Tables, removing that info window would take two clicks.\nBecause you can put data on the web does not mean you should put data on the web. And there’s a difference between a record being “public” and “in front of a large audience.”\nSo before you write the first line of code, ask these questions:\n\nThis data is public, but is it widely available? And does making it widely available and easy to use change anything?\nShould this data be searchable in a search engine?\nDoes this data expose information someone has a reasonable expectation that it would remain at least semi-private?\nDoes this data change over time?\nDoes this data expire?\nWhat is my strategy to update or delete data?\nHow easy should it be to share this data on social media?\nHow should I deal with other people who want this data? API? Bulk download?\n\nYour answers to these questions will guide how you build your app. And hopefully, it’ll guide you to better decisions about how to build an app with ethics in mind.",
    "crumbs": [
      "Ethics in data journalism"
    ]
  },
  {
    "objectID": "data-cleaning-part-i.html",
    "href": "data-cleaning-part-i.html",
    "title": "Data Cleaning Part I: Data smells",
    "section": "",
    "text": "Any time you are given a dataset from anyone, you should immediately be suspicious. Is this data what I think it is? Does it include what I expect? Is there anything I need to know about it? Will it produce the information I expect?\nOne of the first things you should do is give it the smell test.\nFailure to give data the smell test can lead you to miss stories and get your butt kicked on a competitive story.\nWith data smells, we’re trying to find common mistakes in data. For more on data smells, read the GitHub wiki post that started it all. Some common data smells are:\n\nMissing data or missing values\nGaps in data\nWrong type of data\nOutliers\nSharp curves\nConflicting information within a dataset\nConflicting information across datasets\nWrongly derived data\nInternal inconsistency\nExternal inconsistency\nWrong spatial data\nUnusable data, including non-standard abbreviations, ambiguous data, extraneous data, inconsistent data\n\nNot all of these data smells are detectable in code. You may have to ask people about the data. You may have to compare it to another dataset yourself. Does the agency that uses the data produce reports from the data? Does your analysis match those reports? That will expose wrongly derived data, or wrong units, or mistakes you made with inclusion or exclusion.\nBut with several of these data smells, we can do them first, before we do anything else.\nWe’re going to examine several here as they apply to some Maryland state government payments data and Maryland state government grant & loan data.\n\n\nFirst, let’s look at Wrong Type Of Data.\nWe can sniff that out by looking at the output of readr.\nLet’s load the tidyverse.\n\n# Remove scientific notation\noptions(scipen=999)\n# Load the tidyverse\nlibrary(tidyverse)\n\nThis time, we’re going to load the data in a CSV format, which stands for comma separated values and is essentially a fancy structured text file. Each column in the csv is separated – “delimited” – by a comma from the next column.\nWe’re also going to introduce a new argument to our function that reads in the data, read_csv(), called “guess_max”. As R reads in the csv file, it will attempt to make some calls on what “data type” to assign to each field: number, character, date, and so on. The “guess_max” argument says: look at the values in the whatever number of rows we specify before deciding which data type to assign. In this case, we’ll pick 10.\n\n# Load the data\npayments &lt;- read_csv(\"data/State_of_Maryland_Payments_Data__FY2008_to_FY2024_20250115.csv\", guess_max=10)\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 369008 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): Agency Name, Vendor Name, Date, Category\ndbl (4): Fiscal Year, Vendor Zip, Amount, Fiscal Period\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nPay attention to the red warning that signals “one or more parsing issues.” It advises us to run the problems() function to see what went wrong. Let’s do that.\n\nproblems(payments)\n\n# A tibble: 369 × 5\n     row   col expected actual file                                             \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;                                            \n 1  9568     4 a double L4Z4B  /Users/dpwillis/code/datajournalismbook-maryland…\n 2 10117     4 a double L4Y1Z  /Users/dpwillis/code/datajournalismbook-maryland…\n 3 10476     4 a double 0LAND  /Users/dpwillis/code/datajournalismbook-maryland…\n 4 12308     4 a double H4S    /Users/dpwillis/code/datajournalismbook-maryland…\n 5 13084     4 a double N2G    /Users/dpwillis/code/datajournalismbook-maryland…\n 6 15549     4 a double GOR    /Users/dpwillis/code/datajournalismbook-maryland…\n 7 15772     4 a double N5V    /Users/dpwillis/code/datajournalismbook-maryland…\n 8 15844     4 a double T1Y    /Users/dpwillis/code/datajournalismbook-maryland…\n 9 15877     4 a double R2C    /Users/dpwillis/code/datajournalismbook-maryland…\n10 15973     4 a double L4L34  /Users/dpwillis/code/datajournalismbook-maryland…\n# ℹ 359 more rows\n\n\nIt produces a table of all the parsing problems. It has 369 rows, which means we have that some problems but not a huge number considering we have 369,000 rows. In almost every case here, the readr library has guessed that a given column was of a “double” data type – a number. It did it based on very limited information – only 10 rows. So, when it hit a value that looked like a date, or a character string, it didn’t know what to do. So it just didn’t read in that value correctly.\nThe easy way to fix this is to set the guess_max argument higher. It will take a little longer to load, but we’ll use every single row in the data set to guess the column type – all 322,138 of them.\n\npayments &lt;- read_csv(\"data/State_of_Maryland_Payments_Data__FY2008_to_FY2024_20250115.csv\", guess_max=369008)\n\nRows: 369008 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Agency Name, Vendor Name, Vendor Zip, Date, Category\ndbl (3): Fiscal Year, Amount, Fiscal Period\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNo parsing errors this time! You can see what the columns are using the glimpse function:\n\nglimpse(payments)\n\nRows: 369,008\nColumns: 8\n$ `Fiscal Year`   &lt;dbl&gt; 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, …\n$ `Agency Name`   &lt;chr&gt; \"AID TO UNIVERSITY OF MD MEDICAL SYSTEM\", \"BALTIMORE C…\n$ `Vendor Name`   &lt;chr&gt; \"U M M S\", \"3M PHJ3884\", \"4 IMPRINT\", \"A J STATIONERS\"…\n$ `Vendor Zip`    &lt;chr&gt; \"21273\", \"15250\", \"53201\", \"21226\", \"21228\", \"21210\", …\n$ Amount          &lt;dbl&gt; 9701191.00, 6212.00, 8312.96, 21810.09, 30195.57, 1002…\n$ `Fiscal Period` &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ Date            &lt;chr&gt; \"01/01/2008 12:00:00 AM\", \"01/01/2008 12:00:00 AM\", \"0…\n$ Category        &lt;chr&gt; \"Vendor Payment\", \"Vendor Payment\", \"Vendor Payment\", …\n\n\nThings that should be characters – like agency name, vendor name – are characters (chr). Things that should be numbers (dbl) – like amount and fiscal year – are numbers. We’ve seen before that sometimes dates aren’t defined as date datatypes by R - we can fix that using lubridate.\n\n\n\nThe second smell we can find in code is wrong spatial data. Spatial data means data that refers to some geography; in this dataset the only geographical element is the vendor’s zip code. Zip codes should be, at a minimum, five characters long (although composed of numbers, zip codes aren’t used as numbers).\nWe can check to see if any of the zip codes are less than five characters by using a function called str_length inside a filter:\n\npayments |&gt;\n  group_by(`Vendor Zip`) |&gt;\n  filter(str_length(`Vendor Zip`) &lt; 5) |&gt; \n  summarise(\n    count=n()\n  ) |&gt;\n  arrange(desc(count))\n\n# A tibble: 553 × 2\n   `Vendor Zip` count\n   &lt;chr&gt;        &lt;int&gt;\n 1 2241          2044\n 2 8873           434\n 3 2284           356\n 4 7921           296\n 5 4915           197\n 6 8650           195\n 7 7188           194\n 8 7101           191\n 9 1441           133\n10 7102           112\n# ℹ 543 more rows\n\n\nSo, yes, we definitely have some zip codes that are less than 5 characters long, which is not good, particularly because we don’t have any other geographical information (such as a state) that would tell us whether we’re missing a leading zero or some other character.\n\n\n\nLet’s now look at gaps in data. These often occur when you have a date or time element in your data, but there are other potential gaps, too. To illustrate those, we’re going to introduce some Maryland state grant and loan data from 2009 forward. Let’s load it and take a look:\n\nmd_grants_loans &lt;- read_csv(\"data/State_of_Maryland_Grant_and_Loan_Data__FY2009_to_FY2022_20250115.csv\")\n\nRows: 19482 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): Grantor, Grantee, Zip Code, Description, Category, Date\ndbl (3): Fiscal Year, Amount, Fiscal Period\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nEach row represents a recipient of state grant or loan, along with information about their location and the state agency that provided the money. When we talk about gaps, often they indicate the administrative rules. Here’s an example: let’s count the number of payments in each category (Grant or Loan) by year in this dataset:\n\nmd_grants_loans |&gt; \n  group_by(`Fiscal Year`, Category) |&gt; \n  summarize(count = n()) |&gt; \n  arrange(`Fiscal Year`)\n\n`summarise()` has grouped output by 'Fiscal Year'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 35 × 3\n# Groups:   Fiscal Year [15]\n   `Fiscal Year` Category count\n           &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt;\n 1          2009 Grant      819\n 2          2010 Grant      612\n 3          2010 Loan        76\n 4          2010 &lt;NA&gt;         1\n 5          2011 Grant      990\n 6          2011 Loan        45\n 7          2011 &lt;NA&gt;         1\n 8          2012 Grant     1100\n 9          2012 Loan        58\n10          2013 Grant      986\n# ℹ 25 more rows\n\n\nWe can see a couple of issues here: first, there is no loan data for FY 2009. That’s mentioned in the source page for the data. It’s good to be aware of all gaps in data, but they don’t always represent a problem. Second, and more problematic, there are a few records where the Category is NA - that data is missing. There also are some inconsistent values - there are 50 records in FY2013 with the category of “L” (probably loans) and one in FY 2017 that is listed as “Contract”.\n\n\n\nAny time you are going to focus on a column for analysis, you should check for unusual values. Are there any unusually large values or unusually small values? Are there any values that raise immediate questions about the data? Let’s look at the smallest amounts in the grants and loan data.\n\nmd_grants_loans |&gt; \n  arrange(Amount)\n\n# A tibble: 19,482 × 9\n   Grantor          Grantee `Zip Code` `Fiscal Year` Amount Description Category\n   &lt;chr&gt;            &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;   \n 1 Department of N… Land P… 21285               2009   60   Catherine … Grant   \n 2 Maryland Depart… UNITED… 21031               2012   96.0 FOOD SERVI… Grant   \n 3 Conservation Re… Easter… 21601               2018  186.  Rolling Vi… Grant   \n 4 Maryland Depart… FAMILY… 20877               2013  304   RTTT - EAR… Grant   \n 5 Maryland Depart… Washin… 21740               2017  361   Hold Harml… Grant   \n 6 Maryland Depart… THE CH… 21215-3211          2012  362.  FOOD SERVI… Grant   \n 7 Maryland Depart… ARCHDI… 21227               2012  379.  MARYLAND M… Grant   \n 8 Maryland Depart… HUMAN … 21158               2013  387   CASH FOR C… Grant   \n 9 Department of N… The Co… 22209               2012  402.  R Creighto… Grant   \n10 Governor's Offi… Court … 21204               2018  411.  Children's… Grant   \n# ℹ 19,472 more rows\n# ℹ 2 more variables: `Fiscal Period` &lt;dbl&gt;, Date &lt;chr&gt;\n\n\nThere are two grants for less than $100, which might not be problematic at all, but given that just two of 19,000 are for very small amounts you might wonder if there are suggested amounts for applicants and how tiny ones get evaluated compared to very large requests. As journalists, we should be skeptical of information put in front of us and ask why or what it says about the data itself.",
    "crumbs": [
      "Data Cleaning Part I: Data smells"
    ]
  },
  {
    "objectID": "data-cleaning-part-i.html#wrong-type",
    "href": "data-cleaning-part-i.html#wrong-type",
    "title": "Data Cleaning Part I: Data smells",
    "section": "",
    "text": "First, let’s look at Wrong Type Of Data.\nWe can sniff that out by looking at the output of readr.\nLet’s load the tidyverse.\n\n# Remove scientific notation\noptions(scipen=999)\n# Load the tidyverse\nlibrary(tidyverse)\n\nThis time, we’re going to load the data in a CSV format, which stands for comma separated values and is essentially a fancy structured text file. Each column in the csv is separated – “delimited” – by a comma from the next column.\nWe’re also going to introduce a new argument to our function that reads in the data, read_csv(), called “guess_max”. As R reads in the csv file, it will attempt to make some calls on what “data type” to assign to each field: number, character, date, and so on. The “guess_max” argument says: look at the values in the whatever number of rows we specify before deciding which data type to assign. In this case, we’ll pick 10.\n\n# Load the data\npayments &lt;- read_csv(\"data/State_of_Maryland_Payments_Data__FY2008_to_FY2024_20250115.csv\", guess_max=10)\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 369008 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): Agency Name, Vendor Name, Date, Category\ndbl (4): Fiscal Year, Vendor Zip, Amount, Fiscal Period\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nPay attention to the red warning that signals “one or more parsing issues.” It advises us to run the problems() function to see what went wrong. Let’s do that.\n\nproblems(payments)\n\n# A tibble: 369 × 5\n     row   col expected actual file                                             \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;                                            \n 1  9568     4 a double L4Z4B  /Users/dpwillis/code/datajournalismbook-maryland…\n 2 10117     4 a double L4Y1Z  /Users/dpwillis/code/datajournalismbook-maryland…\n 3 10476     4 a double 0LAND  /Users/dpwillis/code/datajournalismbook-maryland…\n 4 12308     4 a double H4S    /Users/dpwillis/code/datajournalismbook-maryland…\n 5 13084     4 a double N2G    /Users/dpwillis/code/datajournalismbook-maryland…\n 6 15549     4 a double GOR    /Users/dpwillis/code/datajournalismbook-maryland…\n 7 15772     4 a double N5V    /Users/dpwillis/code/datajournalismbook-maryland…\n 8 15844     4 a double T1Y    /Users/dpwillis/code/datajournalismbook-maryland…\n 9 15877     4 a double R2C    /Users/dpwillis/code/datajournalismbook-maryland…\n10 15973     4 a double L4L34  /Users/dpwillis/code/datajournalismbook-maryland…\n# ℹ 359 more rows\n\n\nIt produces a table of all the parsing problems. It has 369 rows, which means we have that some problems but not a huge number considering we have 369,000 rows. In almost every case here, the readr library has guessed that a given column was of a “double” data type – a number. It did it based on very limited information – only 10 rows. So, when it hit a value that looked like a date, or a character string, it didn’t know what to do. So it just didn’t read in that value correctly.\nThe easy way to fix this is to set the guess_max argument higher. It will take a little longer to load, but we’ll use every single row in the data set to guess the column type – all 322,138 of them.\n\npayments &lt;- read_csv(\"data/State_of_Maryland_Payments_Data__FY2008_to_FY2024_20250115.csv\", guess_max=369008)\n\nRows: 369008 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Agency Name, Vendor Name, Vendor Zip, Date, Category\ndbl (3): Fiscal Year, Amount, Fiscal Period\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNo parsing errors this time! You can see what the columns are using the glimpse function:\n\nglimpse(payments)\n\nRows: 369,008\nColumns: 8\n$ `Fiscal Year`   &lt;dbl&gt; 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, …\n$ `Agency Name`   &lt;chr&gt; \"AID TO UNIVERSITY OF MD MEDICAL SYSTEM\", \"BALTIMORE C…\n$ `Vendor Name`   &lt;chr&gt; \"U M M S\", \"3M PHJ3884\", \"4 IMPRINT\", \"A J STATIONERS\"…\n$ `Vendor Zip`    &lt;chr&gt; \"21273\", \"15250\", \"53201\", \"21226\", \"21228\", \"21210\", …\n$ Amount          &lt;dbl&gt; 9701191.00, 6212.00, 8312.96, 21810.09, 30195.57, 1002…\n$ `Fiscal Period` &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ Date            &lt;chr&gt; \"01/01/2008 12:00:00 AM\", \"01/01/2008 12:00:00 AM\", \"0…\n$ Category        &lt;chr&gt; \"Vendor Payment\", \"Vendor Payment\", \"Vendor Payment\", …\n\n\nThings that should be characters – like agency name, vendor name – are characters (chr). Things that should be numbers (dbl) – like amount and fiscal year – are numbers. We’ve seen before that sometimes dates aren’t defined as date datatypes by R - we can fix that using lubridate.",
    "crumbs": [
      "Data Cleaning Part I: Data smells"
    ]
  },
  {
    "objectID": "data-cleaning-part-i.html#wrong-spatial-data",
    "href": "data-cleaning-part-i.html#wrong-spatial-data",
    "title": "Data Cleaning Part I: Data smells",
    "section": "",
    "text": "The second smell we can find in code is wrong spatial data. Spatial data means data that refers to some geography; in this dataset the only geographical element is the vendor’s zip code. Zip codes should be, at a minimum, five characters long (although composed of numbers, zip codes aren’t used as numbers).\nWe can check to see if any of the zip codes are less than five characters by using a function called str_length inside a filter:\n\npayments |&gt;\n  group_by(`Vendor Zip`) |&gt;\n  filter(str_length(`Vendor Zip`) &lt; 5) |&gt; \n  summarise(\n    count=n()\n  ) |&gt;\n  arrange(desc(count))\n\n# A tibble: 553 × 2\n   `Vendor Zip` count\n   &lt;chr&gt;        &lt;int&gt;\n 1 2241          2044\n 2 8873           434\n 3 2284           356\n 4 7921           296\n 5 4915           197\n 6 8650           195\n 7 7188           194\n 8 7101           191\n 9 1441           133\n10 7102           112\n# ℹ 543 more rows\n\n\nSo, yes, we definitely have some zip codes that are less than 5 characters long, which is not good, particularly because we don’t have any other geographical information (such as a state) that would tell us whether we’re missing a leading zero or some other character.",
    "crumbs": [
      "Data Cleaning Part I: Data smells"
    ]
  },
  {
    "objectID": "data-cleaning-part-i.html#gaps-in-data-missing-data",
    "href": "data-cleaning-part-i.html#gaps-in-data-missing-data",
    "title": "Data Cleaning Part I: Data smells",
    "section": "",
    "text": "Let’s now look at gaps in data. These often occur when you have a date or time element in your data, but there are other potential gaps, too. To illustrate those, we’re going to introduce some Maryland state grant and loan data from 2009 forward. Let’s load it and take a look:\n\nmd_grants_loans &lt;- read_csv(\"data/State_of_Maryland_Grant_and_Loan_Data__FY2009_to_FY2022_20250115.csv\")\n\nRows: 19482 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): Grantor, Grantee, Zip Code, Description, Category, Date\ndbl (3): Fiscal Year, Amount, Fiscal Period\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nEach row represents a recipient of state grant or loan, along with information about their location and the state agency that provided the money. When we talk about gaps, often they indicate the administrative rules. Here’s an example: let’s count the number of payments in each category (Grant or Loan) by year in this dataset:\n\nmd_grants_loans |&gt; \n  group_by(`Fiscal Year`, Category) |&gt; \n  summarize(count = n()) |&gt; \n  arrange(`Fiscal Year`)\n\n`summarise()` has grouped output by 'Fiscal Year'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 35 × 3\n# Groups:   Fiscal Year [15]\n   `Fiscal Year` Category count\n           &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt;\n 1          2009 Grant      819\n 2          2010 Grant      612\n 3          2010 Loan        76\n 4          2010 &lt;NA&gt;         1\n 5          2011 Grant      990\n 6          2011 Loan        45\n 7          2011 &lt;NA&gt;         1\n 8          2012 Grant     1100\n 9          2012 Loan        58\n10          2013 Grant      986\n# ℹ 25 more rows\n\n\nWe can see a couple of issues here: first, there is no loan data for FY 2009. That’s mentioned in the source page for the data. It’s good to be aware of all gaps in data, but they don’t always represent a problem. Second, and more problematic, there are a few records where the Category is NA - that data is missing. There also are some inconsistent values - there are 50 records in FY2013 with the category of “L” (probably loans) and one in FY 2017 that is listed as “Contract”.",
    "crumbs": [
      "Data Cleaning Part I: Data smells"
    ]
  },
  {
    "objectID": "data-cleaning-part-i.html#unusual-outliers",
    "href": "data-cleaning-part-i.html#unusual-outliers",
    "title": "Data Cleaning Part I: Data smells",
    "section": "",
    "text": "Any time you are going to focus on a column for analysis, you should check for unusual values. Are there any unusually large values or unusually small values? Are there any values that raise immediate questions about the data? Let’s look at the smallest amounts in the grants and loan data.\n\nmd_grants_loans |&gt; \n  arrange(Amount)\n\n# A tibble: 19,482 × 9\n   Grantor          Grantee `Zip Code` `Fiscal Year` Amount Description Category\n   &lt;chr&gt;            &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;   \n 1 Department of N… Land P… 21285               2009   60   Catherine … Grant   \n 2 Maryland Depart… UNITED… 21031               2012   96.0 FOOD SERVI… Grant   \n 3 Conservation Re… Easter… 21601               2018  186.  Rolling Vi… Grant   \n 4 Maryland Depart… FAMILY… 20877               2013  304   RTTT - EAR… Grant   \n 5 Maryland Depart… Washin… 21740               2017  361   Hold Harml… Grant   \n 6 Maryland Depart… THE CH… 21215-3211          2012  362.  FOOD SERVI… Grant   \n 7 Maryland Depart… ARCHDI… 21227               2012  379.  MARYLAND M… Grant   \n 8 Maryland Depart… HUMAN … 21158               2013  387   CASH FOR C… Grant   \n 9 Department of N… The Co… 22209               2012  402.  R Creighto… Grant   \n10 Governor's Offi… Court … 21204               2018  411.  Children's… Grant   \n# ℹ 19,472 more rows\n# ℹ 2 more variables: `Fiscal Period` &lt;dbl&gt;, Date &lt;chr&gt;\n\n\nThere are two grants for less than $100, which might not be problematic at all, but given that just two of 19,000 are for very small amounts you might wonder if there are suggested amounts for applicants and how tiny ones get evaluated compared to very large requests. As journalists, we should be skeptical of information put in front of us and ask why or what it says about the data itself.",
    "crumbs": [
      "Data Cleaning Part I: Data smells"
    ]
  },
  {
    "objectID": "basicstats.html",
    "href": "basicstats.html",
    "title": "Basic Stats: Linear Regression and The T-Test",
    "section": "",
    "text": "Basic Stats: Linear Regression and The T-Test\nMany stories that rely on data use basic descriptive statistics: journalists often are calculating rates, creating rankings and computing averages. That’s often in service of finding patterns that could yield interesting stories, but there are useful ways to go beyond patterns and try to establish whether there is a relationship between two things and, if so, how strong that relationship is.\nHere’s an example: an elected official claims that more money spent on education leads to greater economic growth and people moving into the state. How can we test whether those ideas - education spending, economic growth and population growth - are related? How do we estimate how strong any relationship between them is? To do that, we’ll use two slightly more advanced statistical analysis methods to look for patterns: linear regression, to examine relationships, and a t.test, to confirm the statistical validity of an average between two groups. So, let’s do that here.\nWe’ll be using state-by-state data on education spending, economic activity and population growth from 2021, plus the presidential winner from the 2020 election.\nFirst, let’s load libraries. We’re going to load janitor, the tidyverse and a new package, corrr, which will help us do linear regression a bit easier than base R.\n\nlibrary(janitor)\nlibrary(tidyverse)\nlibrary(corrr)\n\nNow let’s load the data we’ll be using. It has five fields:\n\nstate\nper_pupil_2021: education spending per pupil in 2021.\ngdp_growth_2021: the percentage growth in Gross Domestic Product in 2021.\npop_change_2021: the percentage change in population in 2021.\nvote_2020: the winner of the state’s popular vote in the 2020 presidential election.\n\n\nstate_data &lt;- read_csv(\"data/regression_data.csv\")\n\nRows: 50 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): state, vote_2020\ndbl (3): per_pupil_2021, gdp_growth_2021, pop_change_2021\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstate_data\n\n# A tibble: 50 × 5\n   state       per_pupil_2021 gdp_growth_2021 pop_change_2021 vote_2020\n   &lt;chr&gt;                &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt; &lt;chr&gt;    \n 1 Alabama              10683             5.1            0.37 Trump    \n 2 Alaska               19540             0.3            0.17 Trump    \n 3 Arizona               9611             6.3            1.18 Biden    \n 4 Arkansas             11266             5.2            0.46 Trump    \n 5 California           14985             7.8           -0.91 Biden    \n 6 Colorado             12255             5.8            0.46 Biden    \n 7 Connecticut          22769             4.8            0.72 Biden    \n 8 Delaware             17448             3.4            1.28 Biden    \n 9 Florida              10401             8.4            1.1  Trump    \n10 Georgia              12145             7              0.54 Biden    \n# ℹ 40 more rows\n\n\n\n\nLinear Regression\nLet’s start with this question: did states that had the greatest population growth and see GDP growth (our economic activity measure)? We can answer it by examining the relationship or correlation between two variables, pop_change_2021 and gdp_growth_2021. How much do they move in tandem?\nLet’s start by plotting them to get a sense of the pattern.\n\nstate_data |&gt;\n  ggplot() +\n  geom_point(aes(x=pop_change_2021,y=gdp_growth_2021)) +\n  geom_smooth(aes(x=pop_change_2021,y=gdp_growth_2021), method=\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nIt’s a bit messy, but we can see something of a pattern here in the blob of dots. Generally, the dots are moving from the lower left (states with lower population growth) to upper right (states with higher population growth). The blue “line of best fit” shows the general direction of the relationship. It appears that those two things might be at least somewhat related.\nLet’s test another variable, the per-pupil education spending amount per state.\n\nstate_data |&gt;\n  ggplot() +\n  geom_point(aes(x=per_pupil_2021,y=gdp_growth_2021)) +\n  geom_smooth(aes(x=per_pupil_2021,y=gdp_growth_2021), method=\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThis one is messier. There may be a slight downward slope in this blob of dots, but it’s not quite as apparent. It seems less certain that there’s a relationship between these two variables.\nWe can be a bit more precise by calculating a statistic called the correlation coefficient, also called “r”. r is a value between 1 and -1. An r of 1 indicates a strong positive correlation.\nAn increase in air temperature and air conditioning use at home is strongly-positively correlated: the hotter it gets, the more we have to use air conditioning. If we were to plot those two variables, we might not get 1, but we’d get close to it.\nAn r of -1 indicates a strong negative correlation. An increase in temperature and home heating use is strongly negatively correlated: the hotter it gets, the less heat we use indoors. We might not hit -1, but we’d probably get close to it.\nA correlation of 0 indicates no relationship.\nAll r values will fall somewhere on this scale, and how to interpret them isn’t always straightforward. They’re best used to give general guidance when exploring patterns.\nWe can calculate r with a function from the corrr package called “correlate()”. First, we remove the non-numeric values from our state_data (state and vote_2020), then we correlate.\n\nstate_data |&gt;\n  select(-state, -vote_2020) |&gt;\n  correlate()\n\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\n\n# A tibble: 3 × 4\n  term            per_pupil_2021 gdp_growth_2021 pop_change_2021\n  &lt;chr&gt;                    &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;\n1 per_pupil_2021         NA              -0.0599          -0.411\n2 gdp_growth_2021        -0.0599         NA                0.211\n3 pop_change_2021        -0.411           0.211           NA    \n\n\nThe table this function produces generally confirms our interpretation of the two graphs above. The relationship between a state’s gdp_growth_2021 and per-pupil education spending is slightly negative at -0.06 (on a scale of -1 to 1), and the relationship isn’t particularly strong. That’s why the second graphic above was messier than the first.\nThe relationship between population growth and GDP growth seems slightly positive at 0.21 (on a scale of -1 to 1), but there are a lot of dots falling outside that line of best fit in our scatterplot.\nThe relationship between a state’s per-pupil education spending and population growth is a bit stronger (in a negative direction), if moderate, -.41 (on a scale of -1 to 1). Is this finding statistically valid? We can get a general sense of that by calculating the p-value of this correlation, a test of statistical significance. For that, we can use the cor.test function.\n\ncor.test(state_data$per_pupil_2021, state_data$pop_change_2021)\n\n\n    Pearson's product-moment correlation\n\ndata:  state_data$per_pupil_2021 and state_data$pop_change_2021\nt = -3.1256, df = 48, p-value = 0.00301\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.6187516 -0.1500577\nsample estimates:\n       cor \n-0.4112291 \n\n\nThis output is quite a bit uglier, but for our purposes there are two key pieces of information from this chunk of unfamiliar words. First, it shows the correlation calculated above: r -0.4112291. Two, it shows the p-value, which is 0.00301. That’s pretty low, as far as p-values go, which indicates that there’s a very slim chance that our finding is a statistical aberration; that is, that it happened solely by chance.\nNow let’s test another one, the relationship between the per_pupil_2021 and gdp_growth_2021.\n\ncor.test(state_data$per_pupil_2021, state_data$gdp_growth_2021)\n\n\n    Pearson's product-moment correlation\n\ndata:  state_data$per_pupil_2021 and state_data$gdp_growth_2021\nt = -0.41572, df = 48, p-value = 0.6795\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.3326975  0.2221550\nsample estimates:\n        cor \n-0.05989652 \n\n\nAgain, it shows our r value of -0.06, which was weaker. And the p-value here is a much larger 0.6795. That indicates a much greater chance of our finding being a statistical aberration, high enough that I wouldn’t rely on its validity.\np &lt; .05 is accepted in many scientific disciplines – and by many data journalists – as the cutoff for statistical significance. But there’s heated debate about that level, and some academics question whether p-values should be relied on so heavily.\nAnd to be clear, a low p-value does not prove that we’ve found what we set out to find. There’s nothing on this graph or in the regression model output that proves that education spending alone has a direct impact on GDP growth or population change. It’s entirely possible that there’s some other variable – or variables – not considered here that explain this pattern.\nAll we know is that we’ve been able to explore the relationship between multiple variables, and the results can help us develop better questions or provide some context to the elected official’s claims. One of the best uses of techniques like regression and significance testing is to see just how much stock we should put into a claim or conclusion.\n\n\nT-tests\nLet’s suppose we want to ask a related set of questions: did states won by Donald Trump have higher than average GDP growth than states won by Joe Biden? Or did states won by Biden have greater education spending than those won by Trump?\nWe can do this because, in our data, we have a column with two possible categorical values, Biden or Trump, for each state.\nWe could just calculate the averages like we’re used to doing.\n\nstate_data |&gt;\n  group_by(vote_2020) |&gt;\n  summarise(\n    mean_gdp_growth = mean(gdp_growth_2021),\n    mean_per_pupil_education = mean(per_pupil_2021),\n    mean_pop_growth = mean(pop_change_2021)\n  )\n\n# A tibble: 2 × 4\n  vote_2020 mean_gdp_growth mean_per_pupil_education mean_pop_growth\n  &lt;chr&gt;               &lt;dbl&gt;                    &lt;dbl&gt;           &lt;dbl&gt;\n1 Biden                5.87                   16579.           0.177\n2 Trump                4.30                   12379.           0.550\n\n\nExamining this, it appears that in all three categories there’s a difference.\nThe average GDP growth was higher in Biden states than Trump states (5.8% to 4.3%). The average amount of per-pupil education spending in Biden states was larger – $16,578 – than Trump states – $12,378. And states that backed Trump saw higher population growth in 2021. Should we report these as meaningful findings?\nA t-test can help us answer that question. It can tell us where there’s a statistically significant difference between the means of two groups. Have we found a real difference, or have we chanced upon a statistical aberration? Let’s see by calculating it for the population growth.\n\nt.test(pop_change_2021 ~ vote_2020, data = state_data)\n\n\n    Welch Two Sample t-test\n\ndata:  pop_change_2021 by vote_2020\nt = -1.828, df = 45.476, p-value = 0.07411\nalternative hypothesis: true difference in means between group Biden and group Trump is not equal to 0\n95 percent confidence interval:\n -0.78342389  0.03782389\nsample estimates:\nmean in group Biden mean in group Trump \n             0.1768              0.5496 \n\n\nWe see our two means, for Trump and Biden, the same as we calculated above. The t-value is approximately -1.8, the p-value here is 0.07411, both of which should give us pause that that we’ve found something meaningful. More on t-tests here\nLet’s try per-pupil education spending.\n\nt.test(per_pupil_2021 ~ vote_2020, data = state_data)\n\n\n    Welch Two Sample t-test\n\ndata:  per_pupil_2021 by vote_2020\nt = 4.2494, df = 39.029, p-value = 0.0001289\nalternative hypothesis: true difference in means between group Biden and group Trump is not equal to 0\n95 percent confidence interval:\n 2200.814 6199.026\nsample estimates:\nmean in group Biden mean in group Trump \n           16578.80            12378.88 \n\n\nThis is more promising. T is much stronger – about 4.2 – and the p-value is a very low 0.0001. Both of these should give us assurance that we’ve found something statistically meaningful. Again, this doesn’t prove that spending more on education means that voters there will back Democrats for president, or that voting for Donald Trump led to population growth. It just suggests there’s a pattern worth following up on.",
    "crumbs": [
      "Basic Stats: Linear Regression and The T-Test"
    ]
  },
  {
    "objectID": "aggregates.html",
    "href": "aggregates.html",
    "title": "Aggregates",
    "section": "",
    "text": "R is a statistical programming language that is purpose built for data analysis.\nBase R does a lot, but there are a mountain of external libraries that do things to make R better/easier/more fully featured. We already installed the tidyverse – or you should have if you followed the instructions for the last assignment – which isn’t exactly a library, but a collection of libraries. Together, they make up the Tidyverse. Individually, they are extraordinarily useful for what they do. We can load them all at once using the tidyverse name, or we can load them individually. Let’s start with individually.\nThe two libraries we are going to need for this assignment are readr and dplyr. The library readr reads different types of data in. For this assignment, we’re going to read in csv data or Comma Separated Values data. That’s data that has a comma between each column of data.\nThen we’re going to use dplyr to analyze it.\nTo use a library, you need to import it. Good practice – one I’m going to insist on – is that you put all your library steps at the top of your notebooks.\nThat code looks like this:\n\nlibrary(readr)\n\nTo load them both, you need to do this:\n\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nBut, because those two libraries – and several others that we’re going to use over the course of this class – are so commonly used, there’s a shortcut to loading all of the libraries we’ll need:\n\nlibrary(tidyverse)\n\nYou can keep doing that for as many libraries as you need.\n\n\n\nThe first thing we need to do is get some data to work with. We do that by reading it in. In our case, we’re going to read a datatable from an “rds” file, which is a format for storing data with R. Later in the course, we’ll more frequently work with a format called a CSV. A CSV is a stripped down version of a spreadsheet you might open in a program like Excel, in which each column is separated by a comma. RDS files are less common when getting data from other people. But reading in CSVs is less foolproof than reading in rds files, so for now we’ll work with rds.\nThe rds file we’re going to read in contains information about classes offered at the University of Maryland since 2017.\nSo step 1 is to import the data. The code to import the data looks like this:\numd_courses &lt;- read_rds(\"umd_courses.rds\")\nLet’s unpack that.\nThe first part – umd_courses – is the name of a variable.\nA variable is just a name that we’ll use to refer to some more complex thing. In this case, the more complex thing is the data we’re importing into R that will be stored as a dataframe, which is one way R stores data.\nWe can call this variable whatever we want. The variable name doesn’t matter, technically. We could use any word. You could use your first name, if you like. Generally, though, we want to give variables names that are descriptive of the thing they refer to. Which is why we’re calling this one umd_courses. Variable names, by convention are one word all lower case (or two or more words connected by an underscore). You can end a variable with a number, but you can’t start one with a number.\nThe &lt;- bit, you’ll recall from the basics, is the variable assignment operator. It’s how we know we’re assigning something to a word. Think of the arrow as saying “Take everything on the right of this arrow and stuff it into the thing on the left.” So we’re creating an empty vessel called umd_courses and stuffing all this data into it.\nread_rds() is a function, one that only works when we’ve loaded the tidyverse. A function is a little bit of computer code that takes in information and follows a series of pre-determined steps and spits it back out. A recipe to make pizza is a kind of function. We might call it make_pizza().\nThe function does one thing. It takes a preset collection of ingredients – flour, water, oil, cheese, tomato, salt – and passes them through each step outlined in a recipe, in order. Things like: mix flour and water and oil, knead, let it sit, roll it out, put tomato sauce and cheese on it, bake it in an oven, then take it out.\nThe output of our make pizza() function is a finished pie.\nWe’ll make use of a lot of pre-written functions from the tidyverse and other packages, and even write some of our own. Back to this line of code:\numd_courses &lt;- read_rds(\"umd_courses.rds\")\nInside of the read_rds() function, we’ve put the name of the file we want to load. Things we put inside of function, to customize what the function does, are called arguments.\nThe easiest thing to do, if you are confused about how to find your data, is to put your data in the same folder as as your notebook (you’ll have to save that notebook first). If you do that, then you just need to put the name of the file in there (maryland_winred.rds). If you put your data in a folder called “data” that sits next to your data notebook, your function would instead look like this:\n\numd_courses &lt;- read_rds(\"data/umd_courses.rds\")\n\nIn this data set, each row represents a course offered at UMD by a department during a specific term, with some other information optionally included, like seats in the class, the instructor(s) and a description.\nAfter loading the data, it’s a good idea to get a sense of its shape. What does it look like? There are several ways we can examine it.\nBy looking in the R Studio environment window, we can see the number of rows (called “obs.”, which is short for observations), and the number of columns (called variables). We can double click on the dataframe name in the environment window, and explore it like a spreadsheet.\nThere are several useful functions for getting a sense of the dataset right in our markdown document.\nIf we run glimpse(umd_courses), it will give us a list of the columns, the data type for each column and and the first few values for each column.\n\nglimpse(umd_courses)\n\nRows: 79,366\nColumns: 9\n$ id             &lt;chr&gt; \"LING889\", \"LING200\", \"LING689\", \"LING499\", \"LING899\", …\n$ title          &lt;chr&gt; \"Directed Research\", \"Introductory Linguistics\", \"Indep…\n$ description    &lt;chr&gt; NA, \"Credit only granted for: HESP120 or LING200.\\nAddi…\n$ term           &lt;dbl&gt; 202112, 202112, 202112, 202112, 202112, 202112, 202112,…\n$ department     &lt;chr&gt; \"Linguistics\", \"Linguistics\", \"Linguistics\", \"Linguisti…\n$ sections       &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 1, 1, 8, 7, 2, 1, 0…\n$ instructors    &lt;chr&gt; NA, \"Michelle Morrison\", NA, NA, NA, NA, NA, NA, NA, NA…\n$ seats          &lt;dbl&gt; 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 30, 45, 5, 1, 34, 29, 6,…\n$ syllabus_count &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n\n\nIf we type head(umd_courses), it will print out the columns and the first six rows of data.\n\nhead(umd_courses)\n\n# A tibble: 6 × 9\n  id      title         description   term department sections instructors seats\n  &lt;chr&gt;   &lt;chr&gt;         &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n1 LING889 Directed Res…  &lt;NA&gt;       202112 Linguisti…        0 &lt;NA&gt;            0\n2 LING200 Introductory… \"Credit on… 202112 Linguisti…        1 Michelle M…    20\n3 LING689 Independent …  &lt;NA&gt;       202112 Linguisti…        0 &lt;NA&gt;            0\n4 LING499 Directed Stu…  &lt;NA&gt;       202112 Linguisti…        0 &lt;NA&gt;            0\n5 LING899 Doctoral Dis…  &lt;NA&gt;       202112 Linguisti…        0 &lt;NA&gt;            0\n6 LING698 Directed Stu…  &lt;NA&gt;       202112 Linguisti…        0 &lt;NA&gt;            0\n# ℹ 1 more variable: syllabus_count &lt;dbl&gt;\n\n\nWe can also click on the data name in the R Studio environment window to explore it interactively.\n\n\n\nSo what if we wanted to know how many classes were offered in each term?\nTo do that by hand, we’d have to take each of the 60,672 individual rows (or observations or records) and sort them into a pile. We’d put them in groups – one for each term – and then count them.\ndplyr has a group by function in it that does just this. A massive amount of data analysis involves grouping like things together and then doing simple things like counting them, or averaging them together. So it’s a good place to start.\nSo to do this, we’ll take our dataset and we’ll introduce a new operator: |&gt;. The best way to read that operator, in my opinion, is to interpret that as “and then do this.” This is called the “pipe operator” and it’s a huge part of writing R statements. So much so that there’s a keyboard shortcut for this: cmd-shift-m on the Mac and ctrl-shift-m on Windows. In order to enable this shortcut, you’ll need to set an option under Tools -&gt; Global Options, in the “Code” section. Make sure you check the box labeled “Use native pipe operator” and then click “Apply”, like so:\n\nDon’t like that character? R also has one that does the same thing: %&gt;%. They both work.\nWe’re going to establish a pattern that will come up again and again throughout this book: data |&gt; function. In English: take your data set and then do this specific action to it.\nThe first step of every analysis starts with the data being used. Then we apply functions to the data.\nIn our case, the pattern that you’ll use many, many times is: data |&gt; group_by(COLUMN NAME) |&gt; summarize(VARIABLE NAME = AGGREGATE FUNCTION(COLUMN NAME))\nIn our dataset, the column with term information is called “term”\nHere’s the code to count the number of courses in each term:\n\numd_courses |&gt;\n  group_by(term) |&gt;\n  summarise(\n    count_classes = n()\n  )\n\n# A tibble: 29 × 2\n     term count_classes\n    &lt;dbl&gt;         &lt;int&gt;\n 1 201712           487\n 2 201801          4479\n 3 201805          1231\n 4 201808          4465\n 5 201905          1183\n 6 201908          4537\n 7 201912           516\n 8 202001          4648\n 9 202005          1357\n10 202008          4508\n# ℹ 19 more rows\n\n\nSo let’s walk through that.\nWe start with our dataset – umd_courses – and then we tell it to group the data by a given field in the data. In this case, we wanted to group together all the terms, signified by the field name term, which you could get from using the glimpse() function. After we group the data, we need to count them up.\nIn dplyr, we use the summarize() function, which can do alot more than just count things.\nInside the parentheses in summarize, we set up the summaries we want. In this case, we just want a count of the number of classes for each term grouping. The line of code count_classes = n(), says create a new field, called count_classes and set it equal to n(). n() is a function that counts the number of rows or records in each group. Why the letter n? The letter n is a common symbol used to denote a count of something. The number of things (or rows or observations or records) in a dataset? Statisticians call it n. There are n number of classes in this dataset.\nWhen we run that, we get a list of terms with a count next to them. But it’s not in any order.\nSo we’ll add another “and then do this” symbol – |&gt; – and use a new function called arrange(). Arrange does what you think it does – it arranges data in order. By default, it’s in ascending order – smallest to largest. But if we want to know the term with the most classes, we need to sort it in descending order. That looks like this:\n\numd_courses |&gt;\n  group_by(term) |&gt;\n  summarise(\n    count_classes = n()\n  ) |&gt;\n  arrange(desc(count_classes))\n\n# A tibble: 29 × 2\n     term count_classes\n    &lt;dbl&gt;         &lt;int&gt;\n 1 202408          5052\n 2 202208          4975\n 3 202401          4959\n 4 202501          4893\n 5 202308          4849\n 6 202001          4648\n 7 202301          4578\n 8 202108          4545\n 9 201908          4537\n10 202008          4508\n# ℹ 19 more rows\n\n\nThe term labeled 202408, representing the Fall 2024 term, has the most classes.\nWe can, if we want, group by more than one thing. The courses data contains a column detailing the department.\nWe can group by “term” and “department” to see how many courses each department offered in each term. We’ll sort by department and term.\n\numd_courses |&gt;\n  group_by(term, department) |&gt;\n  summarise(\n    count_classes = n()\n  ) |&gt;\n  arrange(term, department)\n\n`summarise()` has grouped output by 'term'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4,270 × 3\n# Groups:   term [29]\n     term department                                   count_classes\n    &lt;dbl&gt; &lt;chr&gt;                                                &lt;int&gt;\n 1 201712 African American Studies                                 6\n 2 201712 Agricultural and Resource Economics                      1\n 3 201712 American Studies                                         4\n 4 201712 Animal Science                                           5\n 5 201712 Anthropology                                            25\n 6 201712 Applied Mathematics & Scientific Computation             3\n 7 201712 Arabic                                                   1\n 8 201712 Art History & Archaeology                                3\n 9 201712 Art Studio                                              13\n10 201712 Asian American Studies                                   1\n# ℹ 4,260 more rows\n\n\n\n\n\nIn the last example, we grouped like records together and counted them, but there’s so much more we can do to summarize each group.\nLet’s say we wanted to know the total number of seats offered in each term? For that, we could use the sum() function to add up all of the values in the column “seats”. We put the column we want to total – “seats” – inside the sum() function sum(seats). Note that we can simply add a new summarize function here, keeping our count_classes field in our output table.\n\numd_courses |&gt;\n  group_by(term) |&gt;\n  summarise(\n    count_classes = n(),\n    total_seats = sum(seats)\n  ) |&gt;\n  arrange(desc(total_seats))\n\n# A tibble: 29 × 3\n     term count_classes total_seats\n    &lt;dbl&gt;         &lt;int&gt;       &lt;dbl&gt;\n 1 202408          5052      241341\n 2 202208          4975      230796\n 3 202308          4849      229128\n 4 202108          4545      223321\n 5 202401          4959      220040\n 6 202008          4508      217314\n 7 201908          4537      217075\n 8 201808          4465      208839\n 9 202301          4578      207465\n10 202201          4493      206075\n# ℹ 19 more rows\n\n\nWe can also calculate the average number of seats for each department – the mean – and the number that sits at the midpoint of our data – the median.\n\numd_courses |&gt;\n  group_by(term) |&gt;\n  summarise(\n    count_classes = n(),\n    total_seats = sum(seats),\n    mean_seats = mean(seats),\n    median_seats = median(seats)\n  ) |&gt;\n  arrange(desc(total_seats))\n\n# A tibble: 29 × 5\n     term count_classes total_seats mean_seats median_seats\n    &lt;dbl&gt;         &lt;int&gt;       &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1 202408          5052      241341       47.8           23\n 2 202208          4975      230796       46.4           22\n 3 202308          4849      229128       47.3           24\n 4 202108          4545      223321       49.1           25\n 5 202401          4959      220040       44.4           24\n 6 202008          4508      217314       48.2           22\n 7 201908          4537      217075       47.8           24\n 8 201808          4465      208839       46.8           20\n 9 202301          4578      207465       45.3           25\n10 202201          4493      206075       45.9           24\n# ℹ 19 more rows\n\n\nWe see something interesting here. The mean number of seats is higher than the median number in most cases, but the difference isn’t always huge. In some cases the mean gets skewed by larger or lower amounts. Examining both the median – which is less sensitive to extreme values – and the mean – which is more sensitive to extreme values – gives you a clearer picture of the composition of the data.\nWhat about the highest and lowest number of seats for each department? For that, we can use the min() and max() functions.\n\numd_courses |&gt;\n  group_by(department) |&gt;\n  summarise(\n    count_classes = n(),\n    total_seats = sum(seats),\n    mean_seats = mean(seats),\n    median_seats = median(seats),\n    min_seats = min(seats),\n    max_seats = max(seats)\n  ) |&gt;\n  arrange(desc(total_seats))\n\n# A tibble: 222 × 7\n   department        count_classes total_seats mean_seats median_seats min_seats\n   &lt;chr&gt;                     &lt;int&gt;       &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n 1 Business and Man…          2498      212114       84.9           45         0\n 2 Computer Science           1410      157586      112.            45         0\n 3 Mathematics                1340      135107      101.            27         0\n 4 English                    2054      130166       63.4           23         0\n 5 Biological Scien…          1564      124339       79.5           30         0\n 6 Information Stud…          1613      115974       71.9           38         0\n 7 Chemistry                   875      106586      122.            36         0\n 8 Engineering Scie…           809      103095      127.            45         0\n 9 Economics                  1441       88650       61.5           35         0\n10 Communication              1780       87846       49.4           30         0\n# ℹ 212 more rows\n# ℹ 1 more variable: max_seats &lt;dbl&gt;\n\n\nFrom this, we can see that every department has courses with 0 seats listed. We’ll need to account for those in any analysis, probably by excluding them. The variation in mean and median seats is interesting, too: look at Mathematics and English compared to Computer Science and Engineering, for example. These patterns aren’t random: departments make choices about the courses they offer.\nIt would be interesting to see what the course with the largest number of seats is. To do that, we could simply take our original data set and sort it from highest to lowest on seats\n\numd_courses |&gt;\n  arrange(desc(seats))\n\n# A tibble: 79,366 × 9\n   id      title        description   term department sections instructors seats\n   &lt;chr&gt;   &lt;chr&gt;        &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n 1 UNIV100 The Student… \"Credit on… 201808 Universit…      144 Valerie Go…  2876\n 2 UNIV100 The Student… \"Credit on… 202108 Universit…      131 Tori Shay;…  2685\n 3 UNIV100 The Student… \"Credit on… 202208 Universit…      127 Tori Shay;…  2575\n 4 ENES140 Discovering… \"Additiona… 202201 Engineeri…       10 James Green  2500\n 5 ENES140 Discovering… \"Additiona… 202501 Engineeri…       10 James Green  2500\n 6 ENES140 Discovering… \"Additiona… 202101 Engineeri…       10 James Green  2500\n 7 ENES140 Discovering… \"Additiona… 202401 Engineeri…       10 James Green  2500\n 8 ENES140 Discovering… \"Additiona… 202001 Engineeri…       10 James Green  2500\n 9 ENES140 Discovering… \"Additiona… 201801 Engineeri…       10 James Green  2500\n10 ENES140 Discovering… \"Additiona… 202301 Engineeri…       10 James Green  2500\n# ℹ 79,356 more rows\n# ℹ 1 more variable: syllabus_count &lt;dbl&gt;\n\n\nUNIV100 and ENES140 have a LOT of seats. Are they virtual classes? Hybrid? What else could we ask of this data?",
    "crumbs": [
      "Aggregates"
    ]
  },
  {
    "objectID": "aggregates.html#libraries",
    "href": "aggregates.html#libraries",
    "title": "Aggregates",
    "section": "",
    "text": "R is a statistical programming language that is purpose built for data analysis.\nBase R does a lot, but there are a mountain of external libraries that do things to make R better/easier/more fully featured. We already installed the tidyverse – or you should have if you followed the instructions for the last assignment – which isn’t exactly a library, but a collection of libraries. Together, they make up the Tidyverse. Individually, they are extraordinarily useful for what they do. We can load them all at once using the tidyverse name, or we can load them individually. Let’s start with individually.\nThe two libraries we are going to need for this assignment are readr and dplyr. The library readr reads different types of data in. For this assignment, we’re going to read in csv data or Comma Separated Values data. That’s data that has a comma between each column of data.\nThen we’re going to use dplyr to analyze it.\nTo use a library, you need to import it. Good practice – one I’m going to insist on – is that you put all your library steps at the top of your notebooks.\nThat code looks like this:\n\nlibrary(readr)\n\nTo load them both, you need to do this:\n\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nBut, because those two libraries – and several others that we’re going to use over the course of this class – are so commonly used, there’s a shortcut to loading all of the libraries we’ll need:\n\nlibrary(tidyverse)\n\nYou can keep doing that for as many libraries as you need.",
    "crumbs": [
      "Aggregates"
    ]
  },
  {
    "objectID": "aggregates.html#importing-data",
    "href": "aggregates.html#importing-data",
    "title": "Aggregates",
    "section": "",
    "text": "The first thing we need to do is get some data to work with. We do that by reading it in. In our case, we’re going to read a datatable from an “rds” file, which is a format for storing data with R. Later in the course, we’ll more frequently work with a format called a CSV. A CSV is a stripped down version of a spreadsheet you might open in a program like Excel, in which each column is separated by a comma. RDS files are less common when getting data from other people. But reading in CSVs is less foolproof than reading in rds files, so for now we’ll work with rds.\nThe rds file we’re going to read in contains information about classes offered at the University of Maryland since 2017.\nSo step 1 is to import the data. The code to import the data looks like this:\numd_courses &lt;- read_rds(\"umd_courses.rds\")\nLet’s unpack that.\nThe first part – umd_courses – is the name of a variable.\nA variable is just a name that we’ll use to refer to some more complex thing. In this case, the more complex thing is the data we’re importing into R that will be stored as a dataframe, which is one way R stores data.\nWe can call this variable whatever we want. The variable name doesn’t matter, technically. We could use any word. You could use your first name, if you like. Generally, though, we want to give variables names that are descriptive of the thing they refer to. Which is why we’re calling this one umd_courses. Variable names, by convention are one word all lower case (or two or more words connected by an underscore). You can end a variable with a number, but you can’t start one with a number.\nThe &lt;- bit, you’ll recall from the basics, is the variable assignment operator. It’s how we know we’re assigning something to a word. Think of the arrow as saying “Take everything on the right of this arrow and stuff it into the thing on the left.” So we’re creating an empty vessel called umd_courses and stuffing all this data into it.\nread_rds() is a function, one that only works when we’ve loaded the tidyverse. A function is a little bit of computer code that takes in information and follows a series of pre-determined steps and spits it back out. A recipe to make pizza is a kind of function. We might call it make_pizza().\nThe function does one thing. It takes a preset collection of ingredients – flour, water, oil, cheese, tomato, salt – and passes them through each step outlined in a recipe, in order. Things like: mix flour and water and oil, knead, let it sit, roll it out, put tomato sauce and cheese on it, bake it in an oven, then take it out.\nThe output of our make pizza() function is a finished pie.\nWe’ll make use of a lot of pre-written functions from the tidyverse and other packages, and even write some of our own. Back to this line of code:\numd_courses &lt;- read_rds(\"umd_courses.rds\")\nInside of the read_rds() function, we’ve put the name of the file we want to load. Things we put inside of function, to customize what the function does, are called arguments.\nThe easiest thing to do, if you are confused about how to find your data, is to put your data in the same folder as as your notebook (you’ll have to save that notebook first). If you do that, then you just need to put the name of the file in there (maryland_winred.rds). If you put your data in a folder called “data” that sits next to your data notebook, your function would instead look like this:\n\numd_courses &lt;- read_rds(\"data/umd_courses.rds\")\n\nIn this data set, each row represents a course offered at UMD by a department during a specific term, with some other information optionally included, like seats in the class, the instructor(s) and a description.\nAfter loading the data, it’s a good idea to get a sense of its shape. What does it look like? There are several ways we can examine it.\nBy looking in the R Studio environment window, we can see the number of rows (called “obs.”, which is short for observations), and the number of columns (called variables). We can double click on the dataframe name in the environment window, and explore it like a spreadsheet.\nThere are several useful functions for getting a sense of the dataset right in our markdown document.\nIf we run glimpse(umd_courses), it will give us a list of the columns, the data type for each column and and the first few values for each column.\n\nglimpse(umd_courses)\n\nRows: 79,366\nColumns: 9\n$ id             &lt;chr&gt; \"LING889\", \"LING200\", \"LING689\", \"LING499\", \"LING899\", …\n$ title          &lt;chr&gt; \"Directed Research\", \"Introductory Linguistics\", \"Indep…\n$ description    &lt;chr&gt; NA, \"Credit only granted for: HESP120 or LING200.\\nAddi…\n$ term           &lt;dbl&gt; 202112, 202112, 202112, 202112, 202112, 202112, 202112,…\n$ department     &lt;chr&gt; \"Linguistics\", \"Linguistics\", \"Linguistics\", \"Linguisti…\n$ sections       &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 1, 1, 8, 7, 2, 1, 0…\n$ instructors    &lt;chr&gt; NA, \"Michelle Morrison\", NA, NA, NA, NA, NA, NA, NA, NA…\n$ seats          &lt;dbl&gt; 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 30, 45, 5, 1, 34, 29, 6,…\n$ syllabus_count &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n\n\nIf we type head(umd_courses), it will print out the columns and the first six rows of data.\n\nhead(umd_courses)\n\n# A tibble: 6 × 9\n  id      title         description   term department sections instructors seats\n  &lt;chr&gt;   &lt;chr&gt;         &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n1 LING889 Directed Res…  &lt;NA&gt;       202112 Linguisti…        0 &lt;NA&gt;            0\n2 LING200 Introductory… \"Credit on… 202112 Linguisti…        1 Michelle M…    20\n3 LING689 Independent …  &lt;NA&gt;       202112 Linguisti…        0 &lt;NA&gt;            0\n4 LING499 Directed Stu…  &lt;NA&gt;       202112 Linguisti…        0 &lt;NA&gt;            0\n5 LING899 Doctoral Dis…  &lt;NA&gt;       202112 Linguisti…        0 &lt;NA&gt;            0\n6 LING698 Directed Stu…  &lt;NA&gt;       202112 Linguisti…        0 &lt;NA&gt;            0\n# ℹ 1 more variable: syllabus_count &lt;dbl&gt;\n\n\nWe can also click on the data name in the R Studio environment window to explore it interactively.",
    "crumbs": [
      "Aggregates"
    ]
  },
  {
    "objectID": "aggregates.html#group-by-and-count",
    "href": "aggregates.html#group-by-and-count",
    "title": "Aggregates",
    "section": "",
    "text": "So what if we wanted to know how many classes were offered in each term?\nTo do that by hand, we’d have to take each of the 60,672 individual rows (or observations or records) and sort them into a pile. We’d put them in groups – one for each term – and then count them.\ndplyr has a group by function in it that does just this. A massive amount of data analysis involves grouping like things together and then doing simple things like counting them, or averaging them together. So it’s a good place to start.\nSo to do this, we’ll take our dataset and we’ll introduce a new operator: |&gt;. The best way to read that operator, in my opinion, is to interpret that as “and then do this.” This is called the “pipe operator” and it’s a huge part of writing R statements. So much so that there’s a keyboard shortcut for this: cmd-shift-m on the Mac and ctrl-shift-m on Windows. In order to enable this shortcut, you’ll need to set an option under Tools -&gt; Global Options, in the “Code” section. Make sure you check the box labeled “Use native pipe operator” and then click “Apply”, like so:\n\nDon’t like that character? R also has one that does the same thing: %&gt;%. They both work.\nWe’re going to establish a pattern that will come up again and again throughout this book: data |&gt; function. In English: take your data set and then do this specific action to it.\nThe first step of every analysis starts with the data being used. Then we apply functions to the data.\nIn our case, the pattern that you’ll use many, many times is: data |&gt; group_by(COLUMN NAME) |&gt; summarize(VARIABLE NAME = AGGREGATE FUNCTION(COLUMN NAME))\nIn our dataset, the column with term information is called “term”\nHere’s the code to count the number of courses in each term:\n\numd_courses |&gt;\n  group_by(term) |&gt;\n  summarise(\n    count_classes = n()\n  )\n\n# A tibble: 29 × 2\n     term count_classes\n    &lt;dbl&gt;         &lt;int&gt;\n 1 201712           487\n 2 201801          4479\n 3 201805          1231\n 4 201808          4465\n 5 201905          1183\n 6 201908          4537\n 7 201912           516\n 8 202001          4648\n 9 202005          1357\n10 202008          4508\n# ℹ 19 more rows\n\n\nSo let’s walk through that.\nWe start with our dataset – umd_courses – and then we tell it to group the data by a given field in the data. In this case, we wanted to group together all the terms, signified by the field name term, which you could get from using the glimpse() function. After we group the data, we need to count them up.\nIn dplyr, we use the summarize() function, which can do alot more than just count things.\nInside the parentheses in summarize, we set up the summaries we want. In this case, we just want a count of the number of classes for each term grouping. The line of code count_classes = n(), says create a new field, called count_classes and set it equal to n(). n() is a function that counts the number of rows or records in each group. Why the letter n? The letter n is a common symbol used to denote a count of something. The number of things (or rows or observations or records) in a dataset? Statisticians call it n. There are n number of classes in this dataset.\nWhen we run that, we get a list of terms with a count next to them. But it’s not in any order.\nSo we’ll add another “and then do this” symbol – |&gt; – and use a new function called arrange(). Arrange does what you think it does – it arranges data in order. By default, it’s in ascending order – smallest to largest. But if we want to know the term with the most classes, we need to sort it in descending order. That looks like this:\n\numd_courses |&gt;\n  group_by(term) |&gt;\n  summarise(\n    count_classes = n()\n  ) |&gt;\n  arrange(desc(count_classes))\n\n# A tibble: 29 × 2\n     term count_classes\n    &lt;dbl&gt;         &lt;int&gt;\n 1 202408          5052\n 2 202208          4975\n 3 202401          4959\n 4 202501          4893\n 5 202308          4849\n 6 202001          4648\n 7 202301          4578\n 8 202108          4545\n 9 201908          4537\n10 202008          4508\n# ℹ 19 more rows\n\n\nThe term labeled 202408, representing the Fall 2024 term, has the most classes.\nWe can, if we want, group by more than one thing. The courses data contains a column detailing the department.\nWe can group by “term” and “department” to see how many courses each department offered in each term. We’ll sort by department and term.\n\numd_courses |&gt;\n  group_by(term, department) |&gt;\n  summarise(\n    count_classes = n()\n  ) |&gt;\n  arrange(term, department)\n\n`summarise()` has grouped output by 'term'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4,270 × 3\n# Groups:   term [29]\n     term department                                   count_classes\n    &lt;dbl&gt; &lt;chr&gt;                                                &lt;int&gt;\n 1 201712 African American Studies                                 6\n 2 201712 Agricultural and Resource Economics                      1\n 3 201712 American Studies                                         4\n 4 201712 Animal Science                                           5\n 5 201712 Anthropology                                            25\n 6 201712 Applied Mathematics & Scientific Computation             3\n 7 201712 Arabic                                                   1\n 8 201712 Art History & Archaeology                                3\n 9 201712 Art Studio                                              13\n10 201712 Asian American Studies                                   1\n# ℹ 4,260 more rows",
    "crumbs": [
      "Aggregates"
    ]
  },
  {
    "objectID": "aggregates.html#other-summarization-methods-summing-mean-median-min-and-max",
    "href": "aggregates.html#other-summarization-methods-summing-mean-median-min-and-max",
    "title": "Aggregates",
    "section": "",
    "text": "In the last example, we grouped like records together and counted them, but there’s so much more we can do to summarize each group.\nLet’s say we wanted to know the total number of seats offered in each term? For that, we could use the sum() function to add up all of the values in the column “seats”. We put the column we want to total – “seats” – inside the sum() function sum(seats). Note that we can simply add a new summarize function here, keeping our count_classes field in our output table.\n\numd_courses |&gt;\n  group_by(term) |&gt;\n  summarise(\n    count_classes = n(),\n    total_seats = sum(seats)\n  ) |&gt;\n  arrange(desc(total_seats))\n\n# A tibble: 29 × 3\n     term count_classes total_seats\n    &lt;dbl&gt;         &lt;int&gt;       &lt;dbl&gt;\n 1 202408          5052      241341\n 2 202208          4975      230796\n 3 202308          4849      229128\n 4 202108          4545      223321\n 5 202401          4959      220040\n 6 202008          4508      217314\n 7 201908          4537      217075\n 8 201808          4465      208839\n 9 202301          4578      207465\n10 202201          4493      206075\n# ℹ 19 more rows\n\n\nWe can also calculate the average number of seats for each department – the mean – and the number that sits at the midpoint of our data – the median.\n\numd_courses |&gt;\n  group_by(term) |&gt;\n  summarise(\n    count_classes = n(),\n    total_seats = sum(seats),\n    mean_seats = mean(seats),\n    median_seats = median(seats)\n  ) |&gt;\n  arrange(desc(total_seats))\n\n# A tibble: 29 × 5\n     term count_classes total_seats mean_seats median_seats\n    &lt;dbl&gt;         &lt;int&gt;       &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1 202408          5052      241341       47.8           23\n 2 202208          4975      230796       46.4           22\n 3 202308          4849      229128       47.3           24\n 4 202108          4545      223321       49.1           25\n 5 202401          4959      220040       44.4           24\n 6 202008          4508      217314       48.2           22\n 7 201908          4537      217075       47.8           24\n 8 201808          4465      208839       46.8           20\n 9 202301          4578      207465       45.3           25\n10 202201          4493      206075       45.9           24\n# ℹ 19 more rows\n\n\nWe see something interesting here. The mean number of seats is higher than the median number in most cases, but the difference isn’t always huge. In some cases the mean gets skewed by larger or lower amounts. Examining both the median – which is less sensitive to extreme values – and the mean – which is more sensitive to extreme values – gives you a clearer picture of the composition of the data.\nWhat about the highest and lowest number of seats for each department? For that, we can use the min() and max() functions.\n\numd_courses |&gt;\n  group_by(department) |&gt;\n  summarise(\n    count_classes = n(),\n    total_seats = sum(seats),\n    mean_seats = mean(seats),\n    median_seats = median(seats),\n    min_seats = min(seats),\n    max_seats = max(seats)\n  ) |&gt;\n  arrange(desc(total_seats))\n\n# A tibble: 222 × 7\n   department        count_classes total_seats mean_seats median_seats min_seats\n   &lt;chr&gt;                     &lt;int&gt;       &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n 1 Business and Man…          2498      212114       84.9           45         0\n 2 Computer Science           1410      157586      112.            45         0\n 3 Mathematics                1340      135107      101.            27         0\n 4 English                    2054      130166       63.4           23         0\n 5 Biological Scien…          1564      124339       79.5           30         0\n 6 Information Stud…          1613      115974       71.9           38         0\n 7 Chemistry                   875      106586      122.            36         0\n 8 Engineering Scie…           809      103095      127.            45         0\n 9 Economics                  1441       88650       61.5           35         0\n10 Communication              1780       87846       49.4           30         0\n# ℹ 212 more rows\n# ℹ 1 more variable: max_seats &lt;dbl&gt;\n\n\nFrom this, we can see that every department has courses with 0 seats listed. We’ll need to account for those in any analysis, probably by excluding them. The variation in mean and median seats is interesting, too: look at Mathematics and English compared to Computer Science and Engineering, for example. These patterns aren’t random: departments make choices about the courses they offer.\nIt would be interesting to see what the course with the largest number of seats is. To do that, we could simply take our original data set and sort it from highest to lowest on seats\n\numd_courses |&gt;\n  arrange(desc(seats))\n\n# A tibble: 79,366 × 9\n   id      title        description   term department sections instructors seats\n   &lt;chr&gt;   &lt;chr&gt;        &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n 1 UNIV100 The Student… \"Credit on… 201808 Universit…      144 Valerie Go…  2876\n 2 UNIV100 The Student… \"Credit on… 202108 Universit…      131 Tori Shay;…  2685\n 3 UNIV100 The Student… \"Credit on… 202208 Universit…      127 Tori Shay;…  2575\n 4 ENES140 Discovering… \"Additiona… 202201 Engineeri…       10 James Green  2500\n 5 ENES140 Discovering… \"Additiona… 202501 Engineeri…       10 James Green  2500\n 6 ENES140 Discovering… \"Additiona… 202101 Engineeri…       10 James Green  2500\n 7 ENES140 Discovering… \"Additiona… 202401 Engineeri…       10 James Green  2500\n 8 ENES140 Discovering… \"Additiona… 202001 Engineeri…       10 James Green  2500\n 9 ENES140 Discovering… \"Additiona… 201801 Engineeri…       10 James Green  2500\n10 ENES140 Discovering… \"Additiona… 202301 Engineeri…       10 James Green  2500\n# ℹ 79,356 more rows\n# ℹ 1 more variable: syllabus_count &lt;dbl&gt;\n\n\nUNIV100 and ENES140 have a LOT of seats. Are they virtual classes? Hybrid? What else could we ask of this data?",
    "crumbs": [
      "Aggregates"
    ]
  },
  {
    "objectID": "ai_and_llms.html",
    "href": "ai_and_llms.html",
    "title": "AI and Data Journalism",
    "section": "",
    "text": "The first thing to know about the large language models that have attracted so much attention, money and coverage is this: they are not fact machines.\n\n\n\n\n\n\n\n\n\nBut they are - mostly - very useful for people who write code and for those trying to work through complex problems. That’s you. At its core, what a large language model does is predict the next word in a phrase or sentence. They are probabilistic prediction machines based on a huge set of training data. This chapter goes through some tasks and examples using LLMs.\n\n\nWe’ll be using a service called Groq for the examples here. You should sign up for a free account and create an API key. Make sure you copy that key. We’ll also need to install an R package to handle the responses:\n\n# install.packages(\"devtools\")\ndevtools::install_github(\"heurekalabsco/axolotr\")\n\nSkipping install of 'axolotr' from a github remote, the SHA1 (1447e0f7) has not changed since last install.\n  Use `force = TRUE` to force installation\n\n\nThen we can load that library and, using your API key, setup your credentials:\n\nlibrary(axolotr)\n\ncreate_credentials(GROQ_API_KEY = \"YOUR API KEY HERE\")\n\nCredentials updated successfully. Please restart your R session for changes to take effect.\n\n\nSee that “Please restart your R session for changes to take effect.”? Go ahead and do that; you’ll need to rerun the library() function above.\nLet’s make sure that worked. We’ll be using the Llama 3.1 model released by Meta.\n\ngroq_response &lt;- axolotr::ask(\n  prompt = \"Give me five names for a pet lemur\",\n  model = \"llama-3.1-8b-instant\"\n)\n\nError in Groq API call: \n\ngroq_response\n\nNULL\n\n\nI guess you’re getting a lemur?\n\n\n\nThere are at least three good uses of AI in data journalism:\n\nturning unstructured information into data\nhelping with code debugging and explanation\nbrainstorming about strategies for data analysis and visualization\n\nIf you’ve tried to use a large language model to actually do data analysis, it can work, but often the results can be frustrating. Think of AI as a potentially useful assistant for the work you’re doing. If you have a clear idea of the question you want to ask or the direction you want to go, they can help. If you don’t have a clear idea or question, they probably will be less helpful. Let’s go over a quick example of each use.\n\n\nNews organizations are sitting on a trove of valuable raw materials - the words, images, audio and video that they produce every day. We can (hopefully) search it, but search doesn’t always deliver meaning, let alone elevate patterns. For that, often it helps to turn that information into structured data. Let’s look at an example involving my friend Tyson Evans, who recently celebrated his 10th wedding anniversary. You can read about his wedding in The New York Times.\nThis announcement is a story, but it’s also data - or it should be.\n\n\n\n\n\n\n\n\n\nWhat if we could extract those highlighted portions of the text into, say, a CSV file? That’s something that LLMs are pretty good at. Let’s give it a shot using the full text of that announcement:\n\ntext = \"Gabriela Nunes Herman and Tyson Charles Evans were married Saturday at the home of their friends Marcy Gringlas and Joel Greenberg in Chilmark, Mass. Rachel Been, a friend of the couple who received a one-day solemnization certificate from Massachusetts, officiated. The bride, 33, will continue to use her name professionally. She is a Brooklyn-based freelance photographer for magazines and newspapers. She graduated from Wesleyan University in Middletown, Conn. She is a daughter of Dr. Talia N. Herman of Brookline, Mass., and Jeffrey N. Herman of Cambridge, Mass. The bride’s father is a lawyer and the executive vice president of DecisionQuest, a national trial consulting firm in Boston. Her mother is a senior primary care internist at Harvard Vanguard Medical Associates, a practice in Boston. The groom, 31, is a deputy editor of interactive news at The New York Times and an adjunct professor at the Columbia University Graduate School of Journalism. He graduated from the University of California, Los Angeles. He is the son of Carmen K. Evans of Climax Springs, Mo., and Frank J. Evans of St. Joseph, Mo. The groom’s father retired as the president of UPCO, a national retailer of pet supplies in St. Joseph.\"\n\nevans_response &lt;- axolotr::ask(\n  prompt = paste(\"Given the following text, extract information into a CSV file with the following structure with no yapping: celebrant1,celebrant2,location,officiant,celebrant1_age,celebrant2_age,celebrant1_parent1,celebrant1_parent2,celebrant2_parent1,celebrant2_parent2\", text),\n  model = \"llama-3.1-8b-instant\"\n)\n\nError in Groq API call: \n\nevans_response\n\nNULL\n\n\nA brief word about that “no yapping” bit; it’s a way to tell your friendly LLM to cut down on the chattiness in its response. What we care about is the data, not the narrative. And look at the results: without even providing an example or saying that the text described a wedding, the LLM did a solid job. Now imagine if you could do this with hundreds or thousands of similar announcements. You’ve just built a database.\n\n\n\nWhen you’re writing code and run into error messages, you should read them. But if they do not make sense to you, you can ask an LLM to do some translation, which is another great use case for AI. As with any debugging exercise, you should provide some context, things like “Using R and the tidyverse …” and describing what you’re trying to do, but you also can ask LLMs to explain an error message in a different way. Here’s an example:\n\ndebug_response &lt;- axolotr::ask(\n  prompt = \"Explain the following R error message using brief, simple language and suggest a single fix. I am using the tidyverse library: could not find function '|&gt;'\",\n  model = \"llama-3.1-8b-instant\"\n)\n\nError in Groq API call: \n\ndebug_response\n\nNULL\n\n\nThe trouble is that if you run that several times, it will give you slightly different answers. Not fact machines. But you should be able to try some of the suggested solutions and see if any of them work. An even better use could be to pass in working code that you’re not fully understanding and ask the LLM to explain it to you.\n\n\n\nLet’s say that you have some data that you want to interview, but aren’t sure how to proceed. LLMs can provide some direction, but you may not want to follow their directions exactly. You shouldn’t accept their judgments uncritically; you’ll still need to think for yourself. Here’s an example of how that might go:\n\nidea_response &lt;- axolotr::ask(\n  prompt = \"I have a CSV file of daily data on campus police incidents, including the type of incident, building location and time. Using R and the tidyverse, suggest some ways that I could find patterns in the data. Use the new-style pipe operator (|&gt;) in any code examples\",\n  model = \"llama-3.1-8b-instant\"\n)\n\nError in Groq API call: \n\nidea_response\n\nNULL\n\n\nNote that the column names may not match your data; the LLM is making predictions about your data, so you could provide the column names.\n\n\n\nWell, you can try, but I’m not confident you’ll like the results. As this story from The Pudding makes clear, the potential for using LLMs to not just assist with but perform data analysis is real. What will make the difference is how much context you can provide and how clear your ideas and questions are. You still have to do the work.",
    "crumbs": [
      "AI and Data Journalism"
    ]
  },
  {
    "objectID": "ai_and_llms.html#setup",
    "href": "ai_and_llms.html#setup",
    "title": "AI and Data Journalism",
    "section": "",
    "text": "We’ll be using a service called Groq for the examples here. You should sign up for a free account and create an API key. Make sure you copy that key. We’ll also need to install an R package to handle the responses:\n\n# install.packages(\"devtools\")\ndevtools::install_github(\"heurekalabsco/axolotr\")\n\nSkipping install of 'axolotr' from a github remote, the SHA1 (1447e0f7) has not changed since last install.\n  Use `force = TRUE` to force installation\n\n\nThen we can load that library and, using your API key, setup your credentials:\n\nlibrary(axolotr)\n\ncreate_credentials(GROQ_API_KEY = \"YOUR API KEY HERE\")\n\nCredentials updated successfully. Please restart your R session for changes to take effect.\n\n\nSee that “Please restart your R session for changes to take effect.”? Go ahead and do that; you’ll need to rerun the library() function above.\nLet’s make sure that worked. We’ll be using the Llama 3.1 model released by Meta.\n\ngroq_response &lt;- axolotr::ask(\n  prompt = \"Give me five names for a pet lemur\",\n  model = \"llama-3.1-8b-instant\"\n)\n\nError in Groq API call: \n\ngroq_response\n\nNULL\n\n\nI guess you’re getting a lemur?",
    "crumbs": [
      "AI and Data Journalism"
    ]
  },
  {
    "objectID": "ai_and_llms.html#three-uses-of-ai-in-data-journalism",
    "href": "ai_and_llms.html#three-uses-of-ai-in-data-journalism",
    "title": "AI and Data Journalism",
    "section": "",
    "text": "There are at least three good uses of AI in data journalism:\n\nturning unstructured information into data\nhelping with code debugging and explanation\nbrainstorming about strategies for data analysis and visualization\n\nIf you’ve tried to use a large language model to actually do data analysis, it can work, but often the results can be frustrating. Think of AI as a potentially useful assistant for the work you’re doing. If you have a clear idea of the question you want to ask or the direction you want to go, they can help. If you don’t have a clear idea or question, they probably will be less helpful. Let’s go over a quick example of each use.\n\n\nNews organizations are sitting on a trove of valuable raw materials - the words, images, audio and video that they produce every day. We can (hopefully) search it, but search doesn’t always deliver meaning, let alone elevate patterns. For that, often it helps to turn that information into structured data. Let’s look at an example involving my friend Tyson Evans, who recently celebrated his 10th wedding anniversary. You can read about his wedding in The New York Times.\nThis announcement is a story, but it’s also data - or it should be.\n\n\n\n\n\n\n\n\n\nWhat if we could extract those highlighted portions of the text into, say, a CSV file? That’s something that LLMs are pretty good at. Let’s give it a shot using the full text of that announcement:\n\ntext = \"Gabriela Nunes Herman and Tyson Charles Evans were married Saturday at the home of their friends Marcy Gringlas and Joel Greenberg in Chilmark, Mass. Rachel Been, a friend of the couple who received a one-day solemnization certificate from Massachusetts, officiated. The bride, 33, will continue to use her name professionally. She is a Brooklyn-based freelance photographer for magazines and newspapers. She graduated from Wesleyan University in Middletown, Conn. She is a daughter of Dr. Talia N. Herman of Brookline, Mass., and Jeffrey N. Herman of Cambridge, Mass. The bride’s father is a lawyer and the executive vice president of DecisionQuest, a national trial consulting firm in Boston. Her mother is a senior primary care internist at Harvard Vanguard Medical Associates, a practice in Boston. The groom, 31, is a deputy editor of interactive news at The New York Times and an adjunct professor at the Columbia University Graduate School of Journalism. He graduated from the University of California, Los Angeles. He is the son of Carmen K. Evans of Climax Springs, Mo., and Frank J. Evans of St. Joseph, Mo. The groom’s father retired as the president of UPCO, a national retailer of pet supplies in St. Joseph.\"\n\nevans_response &lt;- axolotr::ask(\n  prompt = paste(\"Given the following text, extract information into a CSV file with the following structure with no yapping: celebrant1,celebrant2,location,officiant,celebrant1_age,celebrant2_age,celebrant1_parent1,celebrant1_parent2,celebrant2_parent1,celebrant2_parent2\", text),\n  model = \"llama-3.1-8b-instant\"\n)\n\nError in Groq API call: \n\nevans_response\n\nNULL\n\n\nA brief word about that “no yapping” bit; it’s a way to tell your friendly LLM to cut down on the chattiness in its response. What we care about is the data, not the narrative. And look at the results: without even providing an example or saying that the text described a wedding, the LLM did a solid job. Now imagine if you could do this with hundreds or thousands of similar announcements. You’ve just built a database.\n\n\n\nWhen you’re writing code and run into error messages, you should read them. But if they do not make sense to you, you can ask an LLM to do some translation, which is another great use case for AI. As with any debugging exercise, you should provide some context, things like “Using R and the tidyverse …” and describing what you’re trying to do, but you also can ask LLMs to explain an error message in a different way. Here’s an example:\n\ndebug_response &lt;- axolotr::ask(\n  prompt = \"Explain the following R error message using brief, simple language and suggest a single fix. I am using the tidyverse library: could not find function '|&gt;'\",\n  model = \"llama-3.1-8b-instant\"\n)\n\nError in Groq API call: \n\ndebug_response\n\nNULL\n\n\nThe trouble is that if you run that several times, it will give you slightly different answers. Not fact machines. But you should be able to try some of the suggested solutions and see if any of them work. An even better use could be to pass in working code that you’re not fully understanding and ask the LLM to explain it to you.\n\n\n\nLet’s say that you have some data that you want to interview, but aren’t sure how to proceed. LLMs can provide some direction, but you may not want to follow their directions exactly. You shouldn’t accept their judgments uncritically; you’ll still need to think for yourself. Here’s an example of how that might go:\n\nidea_response &lt;- axolotr::ask(\n  prompt = \"I have a CSV file of daily data on campus police incidents, including the type of incident, building location and time. Using R and the tidyverse, suggest some ways that I could find patterns in the data. Use the new-style pipe operator (|&gt;) in any code examples\",\n  model = \"llama-3.1-8b-instant\"\n)\n\nError in Groq API call: \n\nidea_response\n\nNULL\n\n\nNote that the column names may not match your data; the LLM is making predictions about your data, so you could provide the column names.\n\n\n\nWell, you can try, but I’m not confident you’ll like the results. As this story from The Pudding makes clear, the potential for using LLMs to not just assist with but perform data analysis is real. What will make the difference is how much context you can provide and how clear your ideas and questions are. You still have to do the work.",
    "crumbs": [
      "AI and Data Journalism"
    ]
  },
  {
    "objectID": "census.html",
    "href": "census.html",
    "title": "Intro to APIs: The Census",
    "section": "",
    "text": "There is truly an astonishing amount of data collected by the US Census Bureau. First, there’s the Census that most people know – the every 10 year census. That’s the one mandated by the Constitution where the government attempts to count every person in the US. It’s a mind-boggling feat to even try, and billions get spent on it. That data is used first for determining how many representatives each state gets in Congress. From there, the Census gets used to divide up billions of dollars of federal spending.\nTo answer the questions the government needs to do that, a ton of data gets collected. That, unfortunately, means the Census is exceedingly complicated to work with. The good news is, the Census has an API – an application programming interface. What that means is we can get data directly through the Census Bureau via calls over the internet.\nLet’s demonstrate.\nWe’re going to use a library called tidycensus which makes calls to the Census API in a very tidy way, and gives you back tidy data. That means we don’t have to go through the process of importing the data from a file. I can’t tell you how amazing this is, speaking from experience. The documentation for this library is here. Another R library for working with Census APIs (there is more than one) is this one from Hannah Recht, a journalist with Kaiser Health News.\nFirst we need to install tidycensus using the console: install.packages(\"tidycensus\", dependencies = TRUE). You also should install the sf and rgdal packages.\n\nlibrary(tidyverse)\nlibrary(tidycensus)\n\nTo use the API, you need an API key. To get that, you need to apply for an API key with the Census Bureau. It takes a few minutes and you need to activate your key via email. Once you have your key, you need to set that for this session. Just FYI: Your key is your key. Do not share it around.\n\ncensus_api_key(\"YOUR KEY HERE\", install=TRUE)\n\nThe two main functions in tidycensus are get_decennial, which retrieves data from the 2000 and 2010 Censuses (and soon the 2020 Census), and get_acs, which pulls data from the American Community Survey, a between-Censuses annual survey that provides estimates, not hard counts, but asks more detailed questions. If you’re new to Census data, there’s a very good set of slides from Kyle Walker, the creator of tidycensus, and he’s working on a book that you can read for free online.\nIt’s important to keep in mind that Census data represents people - you, your neighbors and total strangers. It also requires some level of definitions, especially about race & ethnicity, that may or may not match how you define yourself or how others define themselves.\nSo to give you some idea of how complicated the data is, let’s pull up just one file from the decennial Census. We’ll use Summary File 1, or SF1. That has the major population and housing stuff.\n\nsf1 &lt;- load_variables(2010, \"sf1\", cache = TRUE)\n\nsf1\n\n# A tibble: 8,959 × 3\n   name    label                                concept         \n   &lt;chr&gt;   &lt;chr&gt;                                &lt;chr&gt;           \n 1 H001001 Total                                HOUSING UNITS   \n 2 H002001 Total                                URBAN AND RURAL \n 3 H002002 Total!!Urban                         URBAN AND RURAL \n 4 H002003 Total!!Urban!!Inside urbanized areas URBAN AND RURAL \n 5 H002004 Total!!Urban!!Inside urban clusters  URBAN AND RURAL \n 6 H002005 Total!!Rural                         URBAN AND RURAL \n 7 H002006 Total!!Not defined for this file     URBAN AND RURAL \n 8 H003001 Total                                OCCUPANCY STATUS\n 9 H003002 Total!!Occupied                      OCCUPANCY STATUS\n10 H003003 Total!!Vacant                        OCCUPANCY STATUS\n# ℹ 8,949 more rows\n\n\nNote: There are thousands of variables in SF1. That’s not a typo. Open it in your environment by double clicking. As you scroll down, you’ll get an idea of what you’ve got to choose from.\nIf you think that’s crazy, try the SF3 file from 2000.\n\nsf3 &lt;- load_variables(2000, \"sf3\", cache = TRUE)\n\nsf3\n\n# A tibble: 16,520 × 3\n   name    label                                       concept                  \n   &lt;chr&gt;   &lt;chr&gt;                                       &lt;chr&gt;                    \n 1 H001001 Total                                       HOUSING UNITS [1]        \n 2 H002001 Total                                       UNWEIGHTED SAMPLE HOUSIN…\n 3 H002002 Total!!Occupied                             UNWEIGHTED SAMPLE HOUSIN…\n 4 H002003 Total!!Vacant                               UNWEIGHTED SAMPLE HOUSIN…\n 5 H003001 Total                                       100-PERCENT COUNT OF HOU…\n 6 H004001 Percent of occupied housing units in sample PERCENT OF HOUSING UNITS…\n 7 H004002 Percent of vacant housing units in sample   PERCENT OF HOUSING UNITS…\n 8 H005001 Total                                       URBAN AND RURAL [7]      \n 9 H005002 Total!!Urban                                URBAN AND RURAL [7]      \n10 H005003 Total!!Urban!!Inside urbanized areas        URBAN AND RURAL [7]      \n# ℹ 16,510 more rows\n\n\nYes. That’s more than 16,000 variables to choose from. I told you. Astonishing.\nSo let’s try to answer a question using the Census. What is the fastest growing state since 2000?\nTo answer this, we need to pull the total population by state in each of the decennial census. Here’s 2000.\n\np00 &lt;- get_decennial(geography = \"state\", variables = \"P001001\", year = 2000)\n\nNow 2010.\n\np10 &lt;- get_decennial(geography = \"state\", variables = \"P001001\", year = 2010)\n\nLet’s take a peek at 2010.\n\np10\n\nAs you can see, we have a GEOID, NAME, then variable and value. Variable and value are going to be the same. Because those are named the same thing, to merge them together, we need to rename them.\n\np10 |&gt; select(GEOID, NAME, value) |&gt; rename(Population2010=value) -&gt; p2010\n\np00 |&gt; select(GEOID, NAME, value) |&gt; rename(Population2000=value) -&gt; p2000\n\nNow we join the data together.\n\nalldata &lt;- p2000 |&gt; inner_join(p2010)\n\nAnd now we calculate the percent change.\n\nalldata |&gt; mutate(change = ((Population2010-Population2000)/Population2000)*100) |&gt; arrange(desc(change))\n\nAnd just like that: Nevada.\nYou may be asking: hey, wasn’t there a 2020 Census? Where’s that data? The answer is that it’s coming, slowly - the Census Bureau has a schedule of releases.\n\n\nIn 2010, the Census Bureau replaced SF3 with the American Community Survey. The Good News is that the data would be updated on a rolling basis. The bad news is that it’s more complicated because it’s more like survey data with a large sample. That means there’s margins of error and confidence intervals to worry about. By default, using get_acs fetches data from the 5-year estimates (currently 2016-2020), but you can specify 1-year estimates for jurisdictions with at least 65,000 people (many counties and cities).\nHere’s an example using the 5-year ACS estimates:\nWhat is Maryland’s richest county?\nWe can measure this by median household income. That variable is B19013_001, so we can get that data like this (I’m narrowing it to the top 20 for simplicity):\n\nmd &lt;- get_acs(geography = \"county\",\n              variables = c(medincome = \"B19013_001\"),\n              state = \"MD\",\n              year = 2020)\n\nGetting data from the 2016-2020 5-year ACS\n\nmd &lt;- md |&gt; arrange(desc(estimate)) |&gt; top_n(20, estimate)\n\nmd\n\n# A tibble: 20 × 5\n   GEOID NAME                             variable  estimate   moe\n   &lt;chr&gt; &lt;chr&gt;                            &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n 1 24027 Howard County, Maryland          medincome   124042  3448\n 2 24009 Calvert County, Maryland         medincome   112696  3287\n 3 24031 Montgomery County, Maryland      medincome   111812  1361\n 4 24017 Charles County, Maryland         medincome   103678  1654\n 5 24003 Anne Arundel County, Maryland    medincome   103225  1817\n 6 24021 Frederick County, Maryland       medincome   100685  1927\n 7 24013 Carroll County, Maryland         medincome    99569  3051\n 8 24035 Queen Anne's County, Maryland    medincome    96467  4785\n 9 24037 St. Mary's County, Maryland      medincome    95864  3872\n10 24025 Harford County, Maryland         medincome    94003  2398\n11 24033 Prince George's County, Maryland medincome    86994   865\n12 24015 Cecil County, Maryland           medincome    79415  3479\n13 24005 Baltimore County, Maryland       medincome    78724  1641\n14 24041 Talbot County, Maryland          medincome    73102  4031\n15 24047 Worcester County, Maryland       medincome    65396  3856\n16 24043 Washington County, Maryland      medincome    63510  1930\n17 24045 Wicomico County, Maryland        medincome    60366  2437\n18 24029 Kent County, Maryland            medincome    60208  5185\n19 24011 Caroline County, Maryland        medincome    59042  4215\n20 24023 Garrett County, Maryland         medincome    54542  3487\n\n\nHoward, Calvert, Montgomery, Anne Arundel, Charles. What do they all have in common? Lots of suburban flight from DC and Baltimore. But do the margins of error let us say one county is richer than the other. We can find this out visually using error bars. Don’t worry much about the code here – we’ll cover that soon enough.\n\nmd |&gt;\n  mutate(NAME = gsub(\" County, Maryland\", \"\", NAME)) |&gt;\n  ggplot(aes(x = estimate, y = reorder(NAME, estimate))) +\n  geom_errorbarh(aes(xmin = estimate - moe, xmax = estimate + moe)) +\n  geom_point(color = \"red\") +\n  labs(title = \"Household income by county in Maryland\",\n       subtitle = \"2016-2020 American Community Survey\",\n       y = \"\",\n       x = \"ACS estimate (bars represent margin of error)\")\n\n\n\n\n\n\n\n\nAs you can see, some of the error bars are quite wide. Some are narrow. But if the bars overlap, it means the difference between the two counties is within the margin of error, and the differences aren’t statistically significant. So is the difference between Calvert and Montgomery significant? Nope. Is the difference between Howard and everyone else significant? Yes it is.\nLet’s ask another question of the ACS – did any counties lose income from the time of the global financial crisis to the current 5-year window?\nLet’s re-label our first household income data.\n\nmd20 &lt;- get_acs(geography = \"county\",\n              variables = c(medincome = \"B19013_001\"),\n              state = \"MD\",\n              year = 2020)\n\nGetting data from the 2016-2020 5-year ACS\n\n\nAnd now we grab the 2010 median household income.\n\nmd10 &lt;- get_acs(geography = \"county\",\n              variables = c(medincome = \"B19013_001\"),\n              state = \"MD\",\n              year = 2010)\n\nGetting data from the 2006-2010 5-year ACS\n\n\nWhat I’m going to do next is a lot, but each step is simple. I’m going to join the data together, so each county has one line of data. Then I’m going to rename some fields that repeat. Then I’m going to calculate the minimium and maximum value of the estimate using the margin of error. That’ll help me later. After that, I’m going to calculate a perent change and sort it by that change.\n\nmd10 |&gt;\n  inner_join(md20, by=c(\"GEOID\", \"NAME\")) |&gt;\n  rename(estimate2010=estimate.x, estimate2020=estimate.y) |&gt;\n  mutate(min2010 = estimate2010-moe.x, max2010 = estimate2010+moe.x, min2020 = estimate2020-moe.y, max2020 = estimate2020+moe.y) |&gt;\n  select(-variable.x, -variable.y, -moe.x, -moe.y) |&gt;\n  mutate(change = ((estimate2020-estimate2010)/estimate2010)*100) |&gt;\n  arrange(change)\n\n# A tibble: 24 × 9\n   GEOID NAME   estimate2010 estimate2020 min2010 max2010 min2020 max2020 change\n   &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 24011 Carol…        58799        59042   56740   60858   54827   63257  0.413\n 2 24039 Somer…        42443        44980   39092   45794   40432   49528  5.98 \n 3 24041 Talbo…        63017        73102   60081   65953   69071   77133 16.0  \n 4 24017 Charl…        88825       103678   87268   90382  102024  105332 16.7  \n 5 24019 Dorch…        45151        52799   43470   46832   49020   56578 16.9  \n 6 24047 Worce…        55487        65396   52749   58225   61540   69252 17.9  \n 7 24045 Wicom…        50752        60366   49313   52191   57929   62803 18.9  \n 8 24035 Queen…        81096        96467   78068   84124   91682  101252 19.0  \n 9 24023 Garre…        45760        54542   43729   47791   51055   58029 19.2  \n10 24031 Montg…        93373       111812   92535   94211  110451  113173 19.7  \n# ℹ 14 more rows\n\n\nSo according to this, Somerset and Caroline counties had the smallest change between 2010 and 2020, while all other jurisdictions saw double-digit percentage increases.\nBut did they?\nLook at the min and max values for both. Is the change statistically significant?\nThe ACS data has lots of variables, just like the decennial Census does. To browse them, you can do this:\n\nv20 &lt;- load_variables(2020, \"acs5\", cache=TRUE)\n\nAnd then view v20 to see what kinds of variables are available via the API.\n\n\n\nAlthough one of the chief strengths of tidycensus is that it offers a, well, tidy display of Census data, it also has the ability to view multiple variables spread across columns. This can be useful for creating percentages and comparing multiple variables.\n\n\n\nYou’ll notice that we’ve used arrange to sort the results of tidycensus functions, although that’s done after we create a new variable to hold the data. There’s another way to use arrange that you should know about, one that you can use for exploratory analysis. An example using median household income from 2020:\n\nmd20 &lt;- get_acs(geography = \"county\",\n              variables = c(medincome = \"B19013_001\"),\n              state = \"MD\",\n              year = 2020)\n\nGetting data from the 2016-2020 5-year ACS\n\narrange(md20, desc(estimate))\n\n# A tibble: 24 × 5\n   GEOID NAME                          variable  estimate   moe\n   &lt;chr&gt; &lt;chr&gt;                         &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n 1 24027 Howard County, Maryland       medincome   124042  3448\n 2 24009 Calvert County, Maryland      medincome   112696  3287\n 3 24031 Montgomery County, Maryland   medincome   111812  1361\n 4 24017 Charles County, Maryland      medincome   103678  1654\n 5 24003 Anne Arundel County, Maryland medincome   103225  1817\n 6 24021 Frederick County, Maryland    medincome   100685  1927\n 7 24013 Carroll County, Maryland      medincome    99569  3051\n 8 24035 Queen Anne's County, Maryland medincome    96467  4785\n 9 24037 St. Mary's County, Maryland   medincome    95864  3872\n10 24025 Harford County, Maryland      medincome    94003  2398\n# ℹ 14 more rows\n\n\nIn this case we don’t save the sorted results to a variable, we can just see the output in the console.\n\n\n\nCombining Census data with other types of data really unlocks its power for journalism. In this case, we’ll use some 2019-20 data from Maryland high schools, including the number of graduates and the number of students who enrolled in college the next fall semester, plus the percentage. We also have the zip code of the high school, which will enable us to bring in Census data. Let’s load the CSV file and make sure the zip code is a character column, because that’s what we’ll need to join.\n\nmd_high_schools &lt;- read_csv(\"data/md_high_schools.csv\") |&gt;\n  mutate(zip = as.character(zip))\n\nRows: 201 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): cohort_year, local_school_system, school\ndbl (4): zip, high_school_graduates, enrollment, percent_of_high_school_grad...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe Census Bureau has Zip Code Tabulation Areas - they are mostly identical to what we know as zip codes, with some small differences. In most cases we can use them interchangeably. Let’s get ZCTA household income data from the ACS using tidycensus for each zip code in md_high_schools. Run the following codeblock to retrieve household income data for all Maryland ZCTAs first, then join them to md_high_schools using the GEOID column. We’ll also create min and max values and drop any rows where we don’t have income estimates. What’s one question we could ask of this data now that we have enrollment percentages and household income for the high school’s zip code? What might make the answer harder to interpret?\n\nmd_zipcodes &lt;- get_acs(geography=\"zcta\", variables = \"B19013_001\", state='MD', year=2019)\n\nGetting data from the 2015-2019 5-year ACS\n\nmd_high_schools_with_acs &lt;- md_high_schools |&gt; left_join(md_zipcodes, by=c('zip'='GEOID'))\nmd_high_schools_with_acs &lt;- md_high_schools_with_acs |&gt; mutate(min=(estimate-moe), max=(estimate+moe)) |&gt; drop_na()\n\nNow let’s look at the average household income for high schools that have 60% or more graduates immediately enrolling in college, and then look at those with less than 40%. What do you see here?\n\nmd_high_schools_with_acs |&gt;\n  filter(percent_of_high_school_graduates_enrolled &gt;= 60) |&gt; \n  summarize(avg_income = mean(estimate), count =n())\n\n# A tibble: 1 × 2\n  avg_income count\n       &lt;dbl&gt; &lt;int&gt;\n1    111448.    50\n\nmd_high_schools_with_acs |&gt;\n  filter(percent_of_high_school_graduates_enrolled &lt;= 40) |&gt; \n  summarize(avg_income = mean(estimate), count = n())\n\n# A tibble: 1 × 2\n  avg_income count\n       &lt;dbl&gt; &lt;int&gt;\n1     69252.    76",
    "crumbs": [
      "Intro to APIs: The Census"
    ]
  },
  {
    "objectID": "census.html#the-acs",
    "href": "census.html#the-acs",
    "title": "Intro to APIs: The Census",
    "section": "",
    "text": "In 2010, the Census Bureau replaced SF3 with the American Community Survey. The Good News is that the data would be updated on a rolling basis. The bad news is that it’s more complicated because it’s more like survey data with a large sample. That means there’s margins of error and confidence intervals to worry about. By default, using get_acs fetches data from the 5-year estimates (currently 2016-2020), but you can specify 1-year estimates for jurisdictions with at least 65,000 people (many counties and cities).\nHere’s an example using the 5-year ACS estimates:\nWhat is Maryland’s richest county?\nWe can measure this by median household income. That variable is B19013_001, so we can get that data like this (I’m narrowing it to the top 20 for simplicity):\n\nmd &lt;- get_acs(geography = \"county\",\n              variables = c(medincome = \"B19013_001\"),\n              state = \"MD\",\n              year = 2020)\n\nGetting data from the 2016-2020 5-year ACS\n\nmd &lt;- md |&gt; arrange(desc(estimate)) |&gt; top_n(20, estimate)\n\nmd\n\n# A tibble: 20 × 5\n   GEOID NAME                             variable  estimate   moe\n   &lt;chr&gt; &lt;chr&gt;                            &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n 1 24027 Howard County, Maryland          medincome   124042  3448\n 2 24009 Calvert County, Maryland         medincome   112696  3287\n 3 24031 Montgomery County, Maryland      medincome   111812  1361\n 4 24017 Charles County, Maryland         medincome   103678  1654\n 5 24003 Anne Arundel County, Maryland    medincome   103225  1817\n 6 24021 Frederick County, Maryland       medincome   100685  1927\n 7 24013 Carroll County, Maryland         medincome    99569  3051\n 8 24035 Queen Anne's County, Maryland    medincome    96467  4785\n 9 24037 St. Mary's County, Maryland      medincome    95864  3872\n10 24025 Harford County, Maryland         medincome    94003  2398\n11 24033 Prince George's County, Maryland medincome    86994   865\n12 24015 Cecil County, Maryland           medincome    79415  3479\n13 24005 Baltimore County, Maryland       medincome    78724  1641\n14 24041 Talbot County, Maryland          medincome    73102  4031\n15 24047 Worcester County, Maryland       medincome    65396  3856\n16 24043 Washington County, Maryland      medincome    63510  1930\n17 24045 Wicomico County, Maryland        medincome    60366  2437\n18 24029 Kent County, Maryland            medincome    60208  5185\n19 24011 Caroline County, Maryland        medincome    59042  4215\n20 24023 Garrett County, Maryland         medincome    54542  3487\n\n\nHoward, Calvert, Montgomery, Anne Arundel, Charles. What do they all have in common? Lots of suburban flight from DC and Baltimore. But do the margins of error let us say one county is richer than the other. We can find this out visually using error bars. Don’t worry much about the code here – we’ll cover that soon enough.\n\nmd |&gt;\n  mutate(NAME = gsub(\" County, Maryland\", \"\", NAME)) |&gt;\n  ggplot(aes(x = estimate, y = reorder(NAME, estimate))) +\n  geom_errorbarh(aes(xmin = estimate - moe, xmax = estimate + moe)) +\n  geom_point(color = \"red\") +\n  labs(title = \"Household income by county in Maryland\",\n       subtitle = \"2016-2020 American Community Survey\",\n       y = \"\",\n       x = \"ACS estimate (bars represent margin of error)\")\n\n\n\n\n\n\n\n\nAs you can see, some of the error bars are quite wide. Some are narrow. But if the bars overlap, it means the difference between the two counties is within the margin of error, and the differences aren’t statistically significant. So is the difference between Calvert and Montgomery significant? Nope. Is the difference between Howard and everyone else significant? Yes it is.\nLet’s ask another question of the ACS – did any counties lose income from the time of the global financial crisis to the current 5-year window?\nLet’s re-label our first household income data.\n\nmd20 &lt;- get_acs(geography = \"county\",\n              variables = c(medincome = \"B19013_001\"),\n              state = \"MD\",\n              year = 2020)\n\nGetting data from the 2016-2020 5-year ACS\n\n\nAnd now we grab the 2010 median household income.\n\nmd10 &lt;- get_acs(geography = \"county\",\n              variables = c(medincome = \"B19013_001\"),\n              state = \"MD\",\n              year = 2010)\n\nGetting data from the 2006-2010 5-year ACS\n\n\nWhat I’m going to do next is a lot, but each step is simple. I’m going to join the data together, so each county has one line of data. Then I’m going to rename some fields that repeat. Then I’m going to calculate the minimium and maximum value of the estimate using the margin of error. That’ll help me later. After that, I’m going to calculate a perent change and sort it by that change.\n\nmd10 |&gt;\n  inner_join(md20, by=c(\"GEOID\", \"NAME\")) |&gt;\n  rename(estimate2010=estimate.x, estimate2020=estimate.y) |&gt;\n  mutate(min2010 = estimate2010-moe.x, max2010 = estimate2010+moe.x, min2020 = estimate2020-moe.y, max2020 = estimate2020+moe.y) |&gt;\n  select(-variable.x, -variable.y, -moe.x, -moe.y) |&gt;\n  mutate(change = ((estimate2020-estimate2010)/estimate2010)*100) |&gt;\n  arrange(change)\n\n# A tibble: 24 × 9\n   GEOID NAME   estimate2010 estimate2020 min2010 max2010 min2020 max2020 change\n   &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 24011 Carol…        58799        59042   56740   60858   54827   63257  0.413\n 2 24039 Somer…        42443        44980   39092   45794   40432   49528  5.98 \n 3 24041 Talbo…        63017        73102   60081   65953   69071   77133 16.0  \n 4 24017 Charl…        88825       103678   87268   90382  102024  105332 16.7  \n 5 24019 Dorch…        45151        52799   43470   46832   49020   56578 16.9  \n 6 24047 Worce…        55487        65396   52749   58225   61540   69252 17.9  \n 7 24045 Wicom…        50752        60366   49313   52191   57929   62803 18.9  \n 8 24035 Queen…        81096        96467   78068   84124   91682  101252 19.0  \n 9 24023 Garre…        45760        54542   43729   47791   51055   58029 19.2  \n10 24031 Montg…        93373       111812   92535   94211  110451  113173 19.7  \n# ℹ 14 more rows\n\n\nSo according to this, Somerset and Caroline counties had the smallest change between 2010 and 2020, while all other jurisdictions saw double-digit percentage increases.\nBut did they?\nLook at the min and max values for both. Is the change statistically significant?\nThe ACS data has lots of variables, just like the decennial Census does. To browse them, you can do this:\n\nv20 &lt;- load_variables(2020, \"acs5\", cache=TRUE)\n\nAnd then view v20 to see what kinds of variables are available via the API.",
    "crumbs": [
      "Intro to APIs: The Census"
    ]
  },
  {
    "objectID": "census.html#wide-results",
    "href": "census.html#wide-results",
    "title": "Intro to APIs: The Census",
    "section": "",
    "text": "Although one of the chief strengths of tidycensus is that it offers a, well, tidy display of Census data, it also has the ability to view multiple variables spread across columns. This can be useful for creating percentages and comparing multiple variables.",
    "crumbs": [
      "Intro to APIs: The Census"
    ]
  },
  {
    "objectID": "census.html#sorting-results",
    "href": "census.html#sorting-results",
    "title": "Intro to APIs: The Census",
    "section": "",
    "text": "You’ll notice that we’ve used arrange to sort the results of tidycensus functions, although that’s done after we create a new variable to hold the data. There’s another way to use arrange that you should know about, one that you can use for exploratory analysis. An example using median household income from 2020:\n\nmd20 &lt;- get_acs(geography = \"county\",\n              variables = c(medincome = \"B19013_001\"),\n              state = \"MD\",\n              year = 2020)\n\nGetting data from the 2016-2020 5-year ACS\n\narrange(md20, desc(estimate))\n\n# A tibble: 24 × 5\n   GEOID NAME                          variable  estimate   moe\n   &lt;chr&gt; &lt;chr&gt;                         &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n 1 24027 Howard County, Maryland       medincome   124042  3448\n 2 24009 Calvert County, Maryland      medincome   112696  3287\n 3 24031 Montgomery County, Maryland   medincome   111812  1361\n 4 24017 Charles County, Maryland      medincome   103678  1654\n 5 24003 Anne Arundel County, Maryland medincome   103225  1817\n 6 24021 Frederick County, Maryland    medincome   100685  1927\n 7 24013 Carroll County, Maryland      medincome    99569  3051\n 8 24035 Queen Anne's County, Maryland medincome    96467  4785\n 9 24037 St. Mary's County, Maryland   medincome    95864  3872\n10 24025 Harford County, Maryland      medincome    94003  2398\n# ℹ 14 more rows\n\n\nIn this case we don’t save the sorted results to a variable, we can just see the output in the console.",
    "crumbs": [
      "Intro to APIs: The Census"
    ]
  },
  {
    "objectID": "census.html#combining-with-other-data",
    "href": "census.html#combining-with-other-data",
    "title": "Intro to APIs: The Census",
    "section": "",
    "text": "Combining Census data with other types of data really unlocks its power for journalism. In this case, we’ll use some 2019-20 data from Maryland high schools, including the number of graduates and the number of students who enrolled in college the next fall semester, plus the percentage. We also have the zip code of the high school, which will enable us to bring in Census data. Let’s load the CSV file and make sure the zip code is a character column, because that’s what we’ll need to join.\n\nmd_high_schools &lt;- read_csv(\"data/md_high_schools.csv\") |&gt;\n  mutate(zip = as.character(zip))\n\nRows: 201 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): cohort_year, local_school_system, school\ndbl (4): zip, high_school_graduates, enrollment, percent_of_high_school_grad...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe Census Bureau has Zip Code Tabulation Areas - they are mostly identical to what we know as zip codes, with some small differences. In most cases we can use them interchangeably. Let’s get ZCTA household income data from the ACS using tidycensus for each zip code in md_high_schools. Run the following codeblock to retrieve household income data for all Maryland ZCTAs first, then join them to md_high_schools using the GEOID column. We’ll also create min and max values and drop any rows where we don’t have income estimates. What’s one question we could ask of this data now that we have enrollment percentages and household income for the high school’s zip code? What might make the answer harder to interpret?\n\nmd_zipcodes &lt;- get_acs(geography=\"zcta\", variables = \"B19013_001\", state='MD', year=2019)\n\nGetting data from the 2015-2019 5-year ACS\n\nmd_high_schools_with_acs &lt;- md_high_schools |&gt; left_join(md_zipcodes, by=c('zip'='GEOID'))\nmd_high_schools_with_acs &lt;- md_high_schools_with_acs |&gt; mutate(min=(estimate-moe), max=(estimate+moe)) |&gt; drop_na()\n\nNow let’s look at the average household income for high schools that have 60% or more graduates immediately enrolling in college, and then look at those with less than 40%. What do you see here?\n\nmd_high_schools_with_acs |&gt;\n  filter(percent_of_high_school_graduates_enrolled &gt;= 60) |&gt; \n  summarize(avg_income = mean(estimate), count =n())\n\n# A tibble: 1 × 2\n  avg_income count\n       &lt;dbl&gt; &lt;int&gt;\n1    111448.    50\n\nmd_high_schools_with_acs |&gt;\n  filter(percent_of_high_school_graduates_enrolled &lt;= 40) |&gt; \n  summarize(avg_income = mean(estimate), count = n())\n\n# A tibble: 1 × 2\n  avg_income count\n       &lt;dbl&gt; &lt;int&gt;\n1     69252.    76",
    "crumbs": [
      "Intro to APIs: The Census"
    ]
  },
  {
    "objectID": "data-cleaning-part-ii.html",
    "href": "data-cleaning-part-ii.html",
    "title": "Data Cleaning Part II: Janitor",
    "section": "",
    "text": "The necessary bane of every data journalist’s existence is data cleaning.\nEvery developer, every data system, every agency, they all have opinions about how data gets collected. Some decisions make sense from the outside. Some decisions are based entirely on internal politics: who is creating the data, how they are creating it, why they are creating it. Is it automated? Is it manual? Are data normalized? Are there free form fields where users can just type into or does the system restrict them to choices?\nYour journalistic questions – what you want the data to tell you – are almost never part of that equation.\nSo cleaning data is the process of fixing issues in your data so you can answer the questions you want to answer. Data cleaning is a critical step that you can’t skip. A standard metric is that 80 percent of the time working with data will be spent cleaning and verifying data, and 20 percent the more exciting parts like analysis and visualization.\nThe tidyverse has a lot of built-in tools for data cleaning. We’re also going to make use of a new library, called janitor that has a bunch of great functions for cleaning data. Let’s load those now.\n\nlibrary(tidyverse)\nlibrary(janitor)\n\nLet’s continue with our Maryland grants and loans data that we worked with in the previous chapter.\n\nmd_grants_loans &lt;- read_csv(\"data/State_of_Maryland_Grant_and_Loan_Data__FY2009_to_FY2022_20250115.csv\")\n\nRows: 19482 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): Grantor, Grantee, Zip Code, Description, Category, Date\ndbl (3): Fiscal Year, Amount, Fiscal Period\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThere are a number of issues with this data set that might get in the way of asking questions and receiving accurate answers. They are:\n\nThe column names have spaces in them. This isn’t a deal-breaker, as we used this dataframe previously. But it does require that you do some things differently when writing code, and ideally you don’t want spaces in your column names.\nInconsistent capitalization across multiple columns. Sometimes the grantee is capitalized, and other times not. Portions of the grantor name are sometimes capitalized. This issue will ruin your ability to count and add things using those columns.\nThe zip field mixes five digit ZIP codes and nine digit ZIP codes, and some of the records include spaces. If we wanted to group and count the number of loans in a given ZIP code, this inconsistency would not let us do that correctly.\nThe category column is inconsistent and has some missing values.\n\nLet’s get cleaning. Our goal will be to build up one block of code that does all the necessary cleaning in order to answer this question: which zip code has gotten the most amount of money from the Maryland Tourism Board?\n\n\nOne of the first places we can start with cleaning data is cleaning the column names (or headers).\nEvery system has their own way of recording headers, and every developer has their own thoughts of what a good idea is within it. R is most happy when headers are lower case, without special characters.\nIf column headers start with a number, or have a space in between two words, you have to set them off with backticks when using them in a function. Generally speaking, we want one word (or words separated by an underscore), all lowercase, that don’t start with numbers.\nThe janitor library makes fixing headers trivially simple with the function clean_names()\n\n# cleaning function\ncleaned_md_grants_loans &lt;- md_grants_loans |&gt;\n  clean_names()\n\n# display the cleaned dataset\ncleaned_md_grants_loans\n\n# A tibble: 19,482 × 9\n   grantor              grantee zip_code fiscal_year amount description category\n   &lt;chr&gt;                &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;   \n 1 Commerce/Maryland T… PRINCE… 20772           2017 1.28e5 Maryland T… Grant   \n 2 Department of Healt… Associ… 21201           2010 9.34e4 Minority O… Grant   \n 3 Maryland Department… WESTED… 94107-1…        2014 1.61e6 GRANTS FOR… Grant   \n 4 Department of Healt… Maryla… 21075           2010 2.14e5 Babies Bor… Grant   \n 5 Department of Natur… Anacos… 20710           2017 1.74e5 Payments m… Grant   \n 6 Department of Busin… Washin… 21740           2009 5.59e4 grant fund… Grant   \n 7 Boards and Commissi… Domest… 21045           2014 1.72e5 Domestic V… Grant   \n 8 MD Small Business D… Dacore… 20601           2018 1.04e5 Maryland S… Loan    \n 9 Maryland Higher Edu… Mount … 21727           2015 1.75e6 Sellinger … Grant   \n10 Department of Busin… Olney … 20830           2010 2.16e5 Grant fund… Grant   \n# ℹ 19,472 more rows\n# ℹ 2 more variables: fiscal_period &lt;dbl&gt;, date &lt;chr&gt;\n\n\nThis function changed Zip Code to zip_code and generally got rid of capital letters and replaced spaces with underscores. If we wanted to rename a column, we can use a tidyverse function rename() to do that. Let’s change grantor to source as an example. NOTE: when using rename(), the new name comes first.\n\n# cleaning function\ncleaned_md_grants_loans &lt;- md_grants_loans |&gt;\n  clean_names() |&gt; \n  rename(source = grantor)\n\n# display the cleaned dataset\ncleaned_md_grants_loans\n\n# A tibble: 19,482 × 9\n   source grantee zip_code fiscal_year amount description category fiscal_period\n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt;\n 1 Comme… PRINCE… 20772           2017 1.28e5 Maryland T… Grant                1\n 2 Depar… Associ… 21201           2010 9.34e4 Minority O… Grant                1\n 3 Maryl… WESTED… 94107-1…        2014 1.61e6 GRANTS FOR… Grant                1\n 4 Depar… Maryla… 21075           2010 2.14e5 Babies Bor… Grant                1\n 5 Depar… Anacos… 20710           2017 1.74e5 Payments m… Grant                1\n 6 Depar… Washin… 21740           2009 5.59e4 grant fund… Grant                1\n 7 Board… Domest… 21045           2014 1.72e5 Domestic V… Grant                1\n 8 MD Sm… Dacore… 20601           2018 1.04e5 Maryland S… Loan                 1\n 9 Maryl… Mount … 21727           2015 1.75e6 Sellinger … Grant                1\n10 Depar… Olney … 20830           2010 2.16e5 Grant fund… Grant                1\n# ℹ 19,472 more rows\n# ℹ 1 more variable: date &lt;chr&gt;\n\n\n\n\n\nRight now the source, grantee and description columns have inconsistent capitalization. We can fix that using a mutate statement and a function that changes the case of text called str_to_upper(). We’ll use the same columns, overwriting what’s in there since all we’re doing is changing case.\n\n# cleaning function\ncleaned_md_grants_loans &lt;- md_grants_loans |&gt;\n  clean_names() |&gt; \n  rename(source = grantor) |&gt; \n  mutate(source = str_to_upper(source), grantee = str_to_upper(grantee), description = str_to_upper(description))\n\n# display the cleaned dataset\ncleaned_md_grants_loans\n\n# A tibble: 19,482 × 9\n   source grantee zip_code fiscal_year amount description category fiscal_period\n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt;\n 1 COMME… PRINCE… 20772           2017 1.28e5 MARYLAND T… Grant                1\n 2 DEPAR… ASSOCI… 21201           2010 9.34e4 MINORITY O… Grant                1\n 3 MARYL… WESTED… 94107-1…        2014 1.61e6 GRANTS FOR… Grant                1\n 4 DEPAR… MARYLA… 21075           2010 2.14e5 BABIES BOR… Grant                1\n 5 DEPAR… ANACOS… 20710           2017 1.74e5 PAYMENTS M… Grant                1\n 6 DEPAR… WASHIN… 21740           2009 5.59e4 GRANT FUND… Grant                1\n 7 BOARD… DOMEST… 21045           2014 1.72e5 DOMESTIC V… Grant                1\n 8 MD SM… DACORE… 20601           2018 1.04e5 MARYLAND S… Loan                 1\n 9 MARYL… MOUNT … 21727           2015 1.75e6 SELLINGER … Grant                1\n10 DEPAR… OLNEY … 20830           2010 2.16e5 GRANT FUND… Grant                1\n# ℹ 19,472 more rows\n# ℹ 1 more variable: date &lt;chr&gt;\n\n\nWhat this does is make it so that using group_by will result in fewer rows due to inconsistent capitalization. It won’t fix misspellings, but working off a single case style definitely helps.\n\n\n\nOne of the most difficult problems to fix in data is duplicate records in the data. They can creep in with bad joins, bad data entry practices, mistakes – all kinds of reasons. A duplicated record isn’t always there because of an error, but you need to know if it’s there before making that determination.\nSo the question is, do we have any records repeated?\nHere we’ll use a function called get_dupes from the janitor library to check for fully repeated records in our cleaned data set.\n\ncleaned_md_grants_loans |&gt;\n  get_dupes()\n\nNo variable names specified - using all columns.\n\n\n# A tibble: 58 × 10\n   source grantee zip_code fiscal_year amount description category fiscal_period\n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt;\n 1 BOARD… CASA C… 21740           2009 6.62e4 SERVICES T… Grant                1\n 2 BOARD… CASA C… 21740           2009 6.62e4 SERVICES T… Grant                1\n 3 BOARD… FAMILY… 21218           2009 2.18e5 PRE-ADJUDI… Grant                1\n 4 BOARD… FAMILY… 21218           2009 2.18e5 PRE-ADJUDI… Grant                1\n 5 BOARD… HEARTL… 21705           2009 5.32e4 UNDERSERVE… Grant                1\n 6 BOARD… HEARTL… 21705           2009 5.32e4 UNDERSERVE… Grant                1\n 7 BOARD… MARYLA… 21012           2009 9.72e4 CAPACITY B… Grant                1\n 8 BOARD… MARYLA… 21012           2009 9.72e4 CAPACITY B… Grant                1\n 9 BOARD… MARYLA… 21012           2009 9.72e4 SEXUAL ASS… Grant                1\n10 BOARD… MARYLA… 21012           2009 9.72e4 SEXUAL ASS… Grant                1\n# ℹ 48 more rows\n# ℹ 2 more variables: date &lt;chr&gt;, dupe_count &lt;int&gt;\n\n\nAnd the answer is … maybe? Because the original dataset doesn’t have a unique identifier for each grant, it’s possible that we have duplicates here, as many as 58. If we could confirm that these actually are duplicates, we can fix this by adding the function distinct() to our cleaning script. This will keep only one copy of each unique record in our table. But we’d need to confirm that first.\n\n# cleaning function\ncleaned_md_grants_loans &lt;- md_grants_loans |&gt;\n  clean_names() |&gt; \n  rename(source = grantor) |&gt; \n  mutate(source = str_to_upper(source), grantee = str_to_upper(grantee), description = str_to_upper(description)) |&gt; \n  distinct()\n\n# display the cleaned dataset\ncleaned_md_grants_loans\n\n# A tibble: 19,453 × 9\n   source grantee zip_code fiscal_year amount description category fiscal_period\n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt;\n 1 COMME… PRINCE… 20772           2017 1.28e5 MARYLAND T… Grant                1\n 2 DEPAR… ASSOCI… 21201           2010 9.34e4 MINORITY O… Grant                1\n 3 MARYL… WESTED… 94107-1…        2014 1.61e6 GRANTS FOR… Grant                1\n 4 DEPAR… MARYLA… 21075           2010 2.14e5 BABIES BOR… Grant                1\n 5 DEPAR… ANACOS… 20710           2017 1.74e5 PAYMENTS M… Grant                1\n 6 DEPAR… WASHIN… 21740           2009 5.59e4 GRANT FUND… Grant                1\n 7 BOARD… DOMEST… 21045           2014 1.72e5 DOMESTIC V… Grant                1\n 8 MD SM… DACORE… 20601           2018 1.04e5 MARYLAND S… Loan                 1\n 9 MARYL… MOUNT … 21727           2015 1.75e6 SELLINGER … Grant                1\n10 DEPAR… OLNEY … 20830           2010 2.16e5 GRANT FUND… Grant                1\n# ℹ 19,443 more rows\n# ℹ 1 more variable: date &lt;chr&gt;\n\n\n\n\n\nThe rest of the problems with this data set all have to do with inconsistent format of values in a few of the columns. To fix these problems, we’re going to make use of mutate() in concert with “string functions” – special functions that allow us to clean up columns stored as character strings. The tidyverse package stringr has lots of useful string functions, more than we’ll learn in this chapter.\nLet’s start by cleaning up the zip field. Remember, some of the rows had a five-digit ZIP code, while others had a nine-digit ZIP code, separated by a hyphen or not.\nWe’re going to write code that tells R to make a new column for our zips, keeping the first five digits on the left, and get rid of anything after that by using mutate() in concert with str_sub(), from the stringr package.\n\n# cleaning function\ncleaned_md_grants_loans &lt;- md_grants_loans |&gt;\n  clean_names() |&gt; \n  rename(source = grantor) |&gt; \n  mutate(source = str_to_upper(source), grantee = str_to_upper(grantee), description = str_to_upper(description)) |&gt; \n  distinct() |&gt;\n  mutate(zip5 = str_sub(zip_code, start=1L, end=5L))\n\n\n# display the cleaned dataset\ncleaned_md_grants_loans\n\n# A tibble: 19,453 × 10\n   source grantee zip_code fiscal_year amount description category fiscal_period\n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt;\n 1 COMME… PRINCE… 20772           2017 1.28e5 MARYLAND T… Grant                1\n 2 DEPAR… ASSOCI… 21201           2010 9.34e4 MINORITY O… Grant                1\n 3 MARYL… WESTED… 94107-1…        2014 1.61e6 GRANTS FOR… Grant                1\n 4 DEPAR… MARYLA… 21075           2010 2.14e5 BABIES BOR… Grant                1\n 5 DEPAR… ANACOS… 20710           2017 1.74e5 PAYMENTS M… Grant                1\n 6 DEPAR… WASHIN… 21740           2009 5.59e4 GRANT FUND… Grant                1\n 7 BOARD… DOMEST… 21045           2014 1.72e5 DOMESTIC V… Grant                1\n 8 MD SM… DACORE… 20601           2018 1.04e5 MARYLAND S… Loan                 1\n 9 MARYL… MOUNT … 21727           2015 1.75e6 SELLINGER … Grant                1\n10 DEPAR… OLNEY … 20830           2010 2.16e5 GRANT FUND… Grant                1\n# ℹ 19,443 more rows\n# ℹ 2 more variables: date &lt;chr&gt;, zip5 &lt;chr&gt;\n\n\nLet’s break down that last line of code. It says: take the value in each zip column and extract the first character on the left (1L) through the fifth character on the left (5L), and then use that five-digit zip to populate a new zip5 column.\nIf we arrange the zip5 column we can see that there are some non-digits in there, so let’s make those NA. For that, we’re going to use case_when(), a function that let’s us say if a value meets a certain condition, then change it, and if it doesn’t, don’t change it.\n\n# cleaning function\ncleaned_md_grants_loans &lt;- md_grants_loans |&gt;\n  clean_names() |&gt; \n  rename(source = grantor) |&gt; \n  mutate(source = str_to_upper(source), grantee = str_to_upper(grantee), description = str_to_upper(description)) |&gt; \n  distinct() |&gt;\n  mutate(zip5 = str_sub(zip_code, start=1L, end=5L)) |&gt;\n  mutate(zip5 = case_when(\n    zip5 == \"Vario\" ~ NA,\n    zip5 == \"UB7 O\" ~ NA,\n    zip5 == \"UB7 \" ~ NA,\n    .default = zip5\n  ))\n\n# display the cleaned dataset\ncleaned_md_grants_loans\n\n# A tibble: 19,453 × 10\n   source grantee zip_code fiscal_year amount description category fiscal_period\n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt;\n 1 COMME… PRINCE… 20772           2017 1.28e5 MARYLAND T… Grant                1\n 2 DEPAR… ASSOCI… 21201           2010 9.34e4 MINORITY O… Grant                1\n 3 MARYL… WESTED… 94107-1…        2014 1.61e6 GRANTS FOR… Grant                1\n 4 DEPAR… MARYLA… 21075           2010 2.14e5 BABIES BOR… Grant                1\n 5 DEPAR… ANACOS… 20710           2017 1.74e5 PAYMENTS M… Grant                1\n 6 DEPAR… WASHIN… 21740           2009 5.59e4 GRANT FUND… Grant                1\n 7 BOARD… DOMEST… 21045           2014 1.72e5 DOMESTIC V… Grant                1\n 8 MD SM… DACORE… 20601           2018 1.04e5 MARYLAND S… Loan                 1\n 9 MARYL… MOUNT … 21727           2015 1.75e6 SELLINGER … Grant                1\n10 DEPAR… OLNEY … 20830           2010 2.16e5 GRANT FUND… Grant                1\n# ℹ 19,443 more rows\n# ℹ 2 more variables: date &lt;chr&gt;, zip5 &lt;chr&gt;\n\n\nThat last bit is a little complex, so let’s break it down.\nWhat the code above says, in English, is this: Look at all the values in the zip5 column. If the value is “Vario”, then (that’s what the “~” means, then) replace it with NA. Same for the other variations. If it’s anything other than that (that’s what “TRUE” means, otherwise), then keep the existing value in that column.\nInstead of specifying the exact value, we can also solve the problem by using something more generalizable, using a function called str_detect(), which allows us to search parts of words.\nThe second line of our case_when() function below now says, in English: look in the city column. If you find that one of the values starts with “UB7” (the “^” symbol means “starts with”), then (the tilde ~ means then) change it to NA.\n\n# cleaning function\ncleaned_md_grants_loans &lt;- md_grants_loans |&gt;\n  clean_names() |&gt; \n  rename(source = grantor) |&gt; \n  mutate(source = str_to_upper(source), grantee = str_to_upper(grantee), description = str_to_upper(description)) |&gt; \n  distinct() |&gt;\n  mutate(zip5 = str_sub(zip_code, start=1L, end=5L)) |&gt;\n  mutate(zip5 = case_when(\n    zip5 == \"Vario\" ~ NA,\n    str_detect(zip5, \"^UB7\") ~ NA,\n    .default = zip5\n  ))\n\n# display the cleaned dataset\ncleaned_md_grants_loans\n\n# A tibble: 19,453 × 10\n   source grantee zip_code fiscal_year amount description category fiscal_period\n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt;\n 1 COMME… PRINCE… 20772           2017 1.28e5 MARYLAND T… Grant                1\n 2 DEPAR… ASSOCI… 21201           2010 9.34e4 MINORITY O… Grant                1\n 3 MARYL… WESTED… 94107-1…        2014 1.61e6 GRANTS FOR… Grant                1\n 4 DEPAR… MARYLA… 21075           2010 2.14e5 BABIES BOR… Grant                1\n 5 DEPAR… ANACOS… 20710           2017 1.74e5 PAYMENTS M… Grant                1\n 6 DEPAR… WASHIN… 21740           2009 5.59e4 GRANT FUND… Grant                1\n 7 BOARD… DOMEST… 21045           2014 1.72e5 DOMESTIC V… Grant                1\n 8 MD SM… DACORE… 20601           2018 1.04e5 MARYLAND S… Loan                 1\n 9 MARYL… MOUNT … 21727           2015 1.75e6 SELLINGER … Grant                1\n10 DEPAR… OLNEY … 20830           2010 2.16e5 GRANT FUND… Grant                1\n# ℹ 19,443 more rows\n# ℹ 2 more variables: date &lt;chr&gt;, zip5 &lt;chr&gt;\n\n\nWe’ve gotten the source and zip code data as clean as we can, and now we can answer our question: which zip code has gotten the most amount of money from the Maryland Tourism Board? A good rule of thumb is that you should only spend time cleaning fields that are critical to the specific analysis you want to do.\n\ncleaned_md_grants_loans |&gt; \n  filter(source == 'COMMERCE/MARYLAND TOURISM BOARD') |&gt; \n  group_by(zip5) |&gt; \n  summarize(total_amount = sum(amount)) |&gt; \n  arrange(desc(total_amount))\n\n# A tibble: 8 × 2\n  zip5  total_amount\n  &lt;chr&gt;        &lt;dbl&gt;\n1 21202       648069\n2 21601       400000\n3 20005       250000\n4 21401       190570\n5 20772       128041\n6 21701       116708\n7 21740        63070\n8 21014        52490\n\n\nWhy, it’s downtown Baltimore, including the Inner Harbor area.",
    "crumbs": [
      "Data Cleaning Part II: Janitor"
    ]
  },
  {
    "objectID": "data-cleaning-part-ii.html#cleaning-headers",
    "href": "data-cleaning-part-ii.html#cleaning-headers",
    "title": "Data Cleaning Part II: Janitor",
    "section": "",
    "text": "One of the first places we can start with cleaning data is cleaning the column names (or headers).\nEvery system has their own way of recording headers, and every developer has their own thoughts of what a good idea is within it. R is most happy when headers are lower case, without special characters.\nIf column headers start with a number, or have a space in between two words, you have to set them off with backticks when using them in a function. Generally speaking, we want one word (or words separated by an underscore), all lowercase, that don’t start with numbers.\nThe janitor library makes fixing headers trivially simple with the function clean_names()\n\n# cleaning function\ncleaned_md_grants_loans &lt;- md_grants_loans |&gt;\n  clean_names()\n\n# display the cleaned dataset\ncleaned_md_grants_loans\n\n# A tibble: 19,482 × 9\n   grantor              grantee zip_code fiscal_year amount description category\n   &lt;chr&gt;                &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;   \n 1 Commerce/Maryland T… PRINCE… 20772           2017 1.28e5 Maryland T… Grant   \n 2 Department of Healt… Associ… 21201           2010 9.34e4 Minority O… Grant   \n 3 Maryland Department… WESTED… 94107-1…        2014 1.61e6 GRANTS FOR… Grant   \n 4 Department of Healt… Maryla… 21075           2010 2.14e5 Babies Bor… Grant   \n 5 Department of Natur… Anacos… 20710           2017 1.74e5 Payments m… Grant   \n 6 Department of Busin… Washin… 21740           2009 5.59e4 grant fund… Grant   \n 7 Boards and Commissi… Domest… 21045           2014 1.72e5 Domestic V… Grant   \n 8 MD Small Business D… Dacore… 20601           2018 1.04e5 Maryland S… Loan    \n 9 Maryland Higher Edu… Mount … 21727           2015 1.75e6 Sellinger … Grant   \n10 Department of Busin… Olney … 20830           2010 2.16e5 Grant fund… Grant   \n# ℹ 19,472 more rows\n# ℹ 2 more variables: fiscal_period &lt;dbl&gt;, date &lt;chr&gt;\n\n\nThis function changed Zip Code to zip_code and generally got rid of capital letters and replaced spaces with underscores. If we wanted to rename a column, we can use a tidyverse function rename() to do that. Let’s change grantor to source as an example. NOTE: when using rename(), the new name comes first.\n\n# cleaning function\ncleaned_md_grants_loans &lt;- md_grants_loans |&gt;\n  clean_names() |&gt; \n  rename(source = grantor)\n\n# display the cleaned dataset\ncleaned_md_grants_loans\n\n# A tibble: 19,482 × 9\n   source grantee zip_code fiscal_year amount description category fiscal_period\n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt;\n 1 Comme… PRINCE… 20772           2017 1.28e5 Maryland T… Grant                1\n 2 Depar… Associ… 21201           2010 9.34e4 Minority O… Grant                1\n 3 Maryl… WESTED… 94107-1…        2014 1.61e6 GRANTS FOR… Grant                1\n 4 Depar… Maryla… 21075           2010 2.14e5 Babies Bor… Grant                1\n 5 Depar… Anacos… 20710           2017 1.74e5 Payments m… Grant                1\n 6 Depar… Washin… 21740           2009 5.59e4 grant fund… Grant                1\n 7 Board… Domest… 21045           2014 1.72e5 Domestic V… Grant                1\n 8 MD Sm… Dacore… 20601           2018 1.04e5 Maryland S… Loan                 1\n 9 Maryl… Mount … 21727           2015 1.75e6 Sellinger … Grant                1\n10 Depar… Olney … 20830           2010 2.16e5 Grant fund… Grant                1\n# ℹ 19,472 more rows\n# ℹ 1 more variable: date &lt;chr&gt;",
    "crumbs": [
      "Data Cleaning Part II: Janitor"
    ]
  },
  {
    "objectID": "data-cleaning-part-ii.html#changing-capitalization",
    "href": "data-cleaning-part-ii.html#changing-capitalization",
    "title": "Data Cleaning Part II: Janitor",
    "section": "",
    "text": "Right now the source, grantee and description columns have inconsistent capitalization. We can fix that using a mutate statement and a function that changes the case of text called str_to_upper(). We’ll use the same columns, overwriting what’s in there since all we’re doing is changing case.\n\n# cleaning function\ncleaned_md_grants_loans &lt;- md_grants_loans |&gt;\n  clean_names() |&gt; \n  rename(source = grantor) |&gt; \n  mutate(source = str_to_upper(source), grantee = str_to_upper(grantee), description = str_to_upper(description))\n\n# display the cleaned dataset\ncleaned_md_grants_loans\n\n# A tibble: 19,482 × 9\n   source grantee zip_code fiscal_year amount description category fiscal_period\n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt;\n 1 COMME… PRINCE… 20772           2017 1.28e5 MARYLAND T… Grant                1\n 2 DEPAR… ASSOCI… 21201           2010 9.34e4 MINORITY O… Grant                1\n 3 MARYL… WESTED… 94107-1…        2014 1.61e6 GRANTS FOR… Grant                1\n 4 DEPAR… MARYLA… 21075           2010 2.14e5 BABIES BOR… Grant                1\n 5 DEPAR… ANACOS… 20710           2017 1.74e5 PAYMENTS M… Grant                1\n 6 DEPAR… WASHIN… 21740           2009 5.59e4 GRANT FUND… Grant                1\n 7 BOARD… DOMEST… 21045           2014 1.72e5 DOMESTIC V… Grant                1\n 8 MD SM… DACORE… 20601           2018 1.04e5 MARYLAND S… Loan                 1\n 9 MARYL… MOUNT … 21727           2015 1.75e6 SELLINGER … Grant                1\n10 DEPAR… OLNEY … 20830           2010 2.16e5 GRANT FUND… Grant                1\n# ℹ 19,472 more rows\n# ℹ 1 more variable: date &lt;chr&gt;\n\n\nWhat this does is make it so that using group_by will result in fewer rows due to inconsistent capitalization. It won’t fix misspellings, but working off a single case style definitely helps.",
    "crumbs": [
      "Data Cleaning Part II: Janitor"
    ]
  },
  {
    "objectID": "data-cleaning-part-ii.html#duplicates",
    "href": "data-cleaning-part-ii.html#duplicates",
    "title": "Data Cleaning Part II: Janitor",
    "section": "",
    "text": "One of the most difficult problems to fix in data is duplicate records in the data. They can creep in with bad joins, bad data entry practices, mistakes – all kinds of reasons. A duplicated record isn’t always there because of an error, but you need to know if it’s there before making that determination.\nSo the question is, do we have any records repeated?\nHere we’ll use a function called get_dupes from the janitor library to check for fully repeated records in our cleaned data set.\n\ncleaned_md_grants_loans |&gt;\n  get_dupes()\n\nNo variable names specified - using all columns.\n\n\n# A tibble: 58 × 10\n   source grantee zip_code fiscal_year amount description category fiscal_period\n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt;\n 1 BOARD… CASA C… 21740           2009 6.62e4 SERVICES T… Grant                1\n 2 BOARD… CASA C… 21740           2009 6.62e4 SERVICES T… Grant                1\n 3 BOARD… FAMILY… 21218           2009 2.18e5 PRE-ADJUDI… Grant                1\n 4 BOARD… FAMILY… 21218           2009 2.18e5 PRE-ADJUDI… Grant                1\n 5 BOARD… HEARTL… 21705           2009 5.32e4 UNDERSERVE… Grant                1\n 6 BOARD… HEARTL… 21705           2009 5.32e4 UNDERSERVE… Grant                1\n 7 BOARD… MARYLA… 21012           2009 9.72e4 CAPACITY B… Grant                1\n 8 BOARD… MARYLA… 21012           2009 9.72e4 CAPACITY B… Grant                1\n 9 BOARD… MARYLA… 21012           2009 9.72e4 SEXUAL ASS… Grant                1\n10 BOARD… MARYLA… 21012           2009 9.72e4 SEXUAL ASS… Grant                1\n# ℹ 48 more rows\n# ℹ 2 more variables: date &lt;chr&gt;, dupe_count &lt;int&gt;\n\n\nAnd the answer is … maybe? Because the original dataset doesn’t have a unique identifier for each grant, it’s possible that we have duplicates here, as many as 58. If we could confirm that these actually are duplicates, we can fix this by adding the function distinct() to our cleaning script. This will keep only one copy of each unique record in our table. But we’d need to confirm that first.\n\n# cleaning function\ncleaned_md_grants_loans &lt;- md_grants_loans |&gt;\n  clean_names() |&gt; \n  rename(source = grantor) |&gt; \n  mutate(source = str_to_upper(source), grantee = str_to_upper(grantee), description = str_to_upper(description)) |&gt; \n  distinct()\n\n# display the cleaned dataset\ncleaned_md_grants_loans\n\n# A tibble: 19,453 × 9\n   source grantee zip_code fiscal_year amount description category fiscal_period\n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt;\n 1 COMME… PRINCE… 20772           2017 1.28e5 MARYLAND T… Grant                1\n 2 DEPAR… ASSOCI… 21201           2010 9.34e4 MINORITY O… Grant                1\n 3 MARYL… WESTED… 94107-1…        2014 1.61e6 GRANTS FOR… Grant                1\n 4 DEPAR… MARYLA… 21075           2010 2.14e5 BABIES BOR… Grant                1\n 5 DEPAR… ANACOS… 20710           2017 1.74e5 PAYMENTS M… Grant                1\n 6 DEPAR… WASHIN… 21740           2009 5.59e4 GRANT FUND… Grant                1\n 7 BOARD… DOMEST… 21045           2014 1.72e5 DOMESTIC V… Grant                1\n 8 MD SM… DACORE… 20601           2018 1.04e5 MARYLAND S… Loan                 1\n 9 MARYL… MOUNT … 21727           2015 1.75e6 SELLINGER … Grant                1\n10 DEPAR… OLNEY … 20830           2010 2.16e5 GRANT FUND… Grant                1\n# ℹ 19,443 more rows\n# ℹ 1 more variable: date &lt;chr&gt;",
    "crumbs": [
      "Data Cleaning Part II: Janitor"
    ]
  },
  {
    "objectID": "data-cleaning-part-ii.html#cleaning-strings",
    "href": "data-cleaning-part-ii.html#cleaning-strings",
    "title": "Data Cleaning Part II: Janitor",
    "section": "",
    "text": "The rest of the problems with this data set all have to do with inconsistent format of values in a few of the columns. To fix these problems, we’re going to make use of mutate() in concert with “string functions” – special functions that allow us to clean up columns stored as character strings. The tidyverse package stringr has lots of useful string functions, more than we’ll learn in this chapter.\nLet’s start by cleaning up the zip field. Remember, some of the rows had a five-digit ZIP code, while others had a nine-digit ZIP code, separated by a hyphen or not.\nWe’re going to write code that tells R to make a new column for our zips, keeping the first five digits on the left, and get rid of anything after that by using mutate() in concert with str_sub(), from the stringr package.\n\n# cleaning function\ncleaned_md_grants_loans &lt;- md_grants_loans |&gt;\n  clean_names() |&gt; \n  rename(source = grantor) |&gt; \n  mutate(source = str_to_upper(source), grantee = str_to_upper(grantee), description = str_to_upper(description)) |&gt; \n  distinct() |&gt;\n  mutate(zip5 = str_sub(zip_code, start=1L, end=5L))\n\n\n# display the cleaned dataset\ncleaned_md_grants_loans\n\n# A tibble: 19,453 × 10\n   source grantee zip_code fiscal_year amount description category fiscal_period\n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt;\n 1 COMME… PRINCE… 20772           2017 1.28e5 MARYLAND T… Grant                1\n 2 DEPAR… ASSOCI… 21201           2010 9.34e4 MINORITY O… Grant                1\n 3 MARYL… WESTED… 94107-1…        2014 1.61e6 GRANTS FOR… Grant                1\n 4 DEPAR… MARYLA… 21075           2010 2.14e5 BABIES BOR… Grant                1\n 5 DEPAR… ANACOS… 20710           2017 1.74e5 PAYMENTS M… Grant                1\n 6 DEPAR… WASHIN… 21740           2009 5.59e4 GRANT FUND… Grant                1\n 7 BOARD… DOMEST… 21045           2014 1.72e5 DOMESTIC V… Grant                1\n 8 MD SM… DACORE… 20601           2018 1.04e5 MARYLAND S… Loan                 1\n 9 MARYL… MOUNT … 21727           2015 1.75e6 SELLINGER … Grant                1\n10 DEPAR… OLNEY … 20830           2010 2.16e5 GRANT FUND… Grant                1\n# ℹ 19,443 more rows\n# ℹ 2 more variables: date &lt;chr&gt;, zip5 &lt;chr&gt;\n\n\nLet’s break down that last line of code. It says: take the value in each zip column and extract the first character on the left (1L) through the fifth character on the left (5L), and then use that five-digit zip to populate a new zip5 column.\nIf we arrange the zip5 column we can see that there are some non-digits in there, so let’s make those NA. For that, we’re going to use case_when(), a function that let’s us say if a value meets a certain condition, then change it, and if it doesn’t, don’t change it.\n\n# cleaning function\ncleaned_md_grants_loans &lt;- md_grants_loans |&gt;\n  clean_names() |&gt; \n  rename(source = grantor) |&gt; \n  mutate(source = str_to_upper(source), grantee = str_to_upper(grantee), description = str_to_upper(description)) |&gt; \n  distinct() |&gt;\n  mutate(zip5 = str_sub(zip_code, start=1L, end=5L)) |&gt;\n  mutate(zip5 = case_when(\n    zip5 == \"Vario\" ~ NA,\n    zip5 == \"UB7 O\" ~ NA,\n    zip5 == \"UB7 \" ~ NA,\n    .default = zip5\n  ))\n\n# display the cleaned dataset\ncleaned_md_grants_loans\n\n# A tibble: 19,453 × 10\n   source grantee zip_code fiscal_year amount description category fiscal_period\n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt;\n 1 COMME… PRINCE… 20772           2017 1.28e5 MARYLAND T… Grant                1\n 2 DEPAR… ASSOCI… 21201           2010 9.34e4 MINORITY O… Grant                1\n 3 MARYL… WESTED… 94107-1…        2014 1.61e6 GRANTS FOR… Grant                1\n 4 DEPAR… MARYLA… 21075           2010 2.14e5 BABIES BOR… Grant                1\n 5 DEPAR… ANACOS… 20710           2017 1.74e5 PAYMENTS M… Grant                1\n 6 DEPAR… WASHIN… 21740           2009 5.59e4 GRANT FUND… Grant                1\n 7 BOARD… DOMEST… 21045           2014 1.72e5 DOMESTIC V… Grant                1\n 8 MD SM… DACORE… 20601           2018 1.04e5 MARYLAND S… Loan                 1\n 9 MARYL… MOUNT … 21727           2015 1.75e6 SELLINGER … Grant                1\n10 DEPAR… OLNEY … 20830           2010 2.16e5 GRANT FUND… Grant                1\n# ℹ 19,443 more rows\n# ℹ 2 more variables: date &lt;chr&gt;, zip5 &lt;chr&gt;\n\n\nThat last bit is a little complex, so let’s break it down.\nWhat the code above says, in English, is this: Look at all the values in the zip5 column. If the value is “Vario”, then (that’s what the “~” means, then) replace it with NA. Same for the other variations. If it’s anything other than that (that’s what “TRUE” means, otherwise), then keep the existing value in that column.\nInstead of specifying the exact value, we can also solve the problem by using something more generalizable, using a function called str_detect(), which allows us to search parts of words.\nThe second line of our case_when() function below now says, in English: look in the city column. If you find that one of the values starts with “UB7” (the “^” symbol means “starts with”), then (the tilde ~ means then) change it to NA.\n\n# cleaning function\ncleaned_md_grants_loans &lt;- md_grants_loans |&gt;\n  clean_names() |&gt; \n  rename(source = grantor) |&gt; \n  mutate(source = str_to_upper(source), grantee = str_to_upper(grantee), description = str_to_upper(description)) |&gt; \n  distinct() |&gt;\n  mutate(zip5 = str_sub(zip_code, start=1L, end=5L)) |&gt;\n  mutate(zip5 = case_when(\n    zip5 == \"Vario\" ~ NA,\n    str_detect(zip5, \"^UB7\") ~ NA,\n    .default = zip5\n  ))\n\n# display the cleaned dataset\ncleaned_md_grants_loans\n\n# A tibble: 19,453 × 10\n   source grantee zip_code fiscal_year amount description category fiscal_period\n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt;\n 1 COMME… PRINCE… 20772           2017 1.28e5 MARYLAND T… Grant                1\n 2 DEPAR… ASSOCI… 21201           2010 9.34e4 MINORITY O… Grant                1\n 3 MARYL… WESTED… 94107-1…        2014 1.61e6 GRANTS FOR… Grant                1\n 4 DEPAR… MARYLA… 21075           2010 2.14e5 BABIES BOR… Grant                1\n 5 DEPAR… ANACOS… 20710           2017 1.74e5 PAYMENTS M… Grant                1\n 6 DEPAR… WASHIN… 21740           2009 5.59e4 GRANT FUND… Grant                1\n 7 BOARD… DOMEST… 21045           2014 1.72e5 DOMESTIC V… Grant                1\n 8 MD SM… DACORE… 20601           2018 1.04e5 MARYLAND S… Loan                 1\n 9 MARYL… MOUNT … 21727           2015 1.75e6 SELLINGER … Grant                1\n10 DEPAR… OLNEY … 20830           2010 2.16e5 GRANT FUND… Grant                1\n# ℹ 19,443 more rows\n# ℹ 2 more variables: date &lt;chr&gt;, zip5 &lt;chr&gt;\n\n\nWe’ve gotten the source and zip code data as clean as we can, and now we can answer our question: which zip code has gotten the most amount of money from the Maryland Tourism Board? A good rule of thumb is that you should only spend time cleaning fields that are critical to the specific analysis you want to do.\n\ncleaned_md_grants_loans |&gt; \n  filter(source == 'COMMERCE/MARYLAND TOURISM BOARD') |&gt; \n  group_by(zip5) |&gt; \n  summarize(total_amount = sum(amount)) |&gt; \n  arrange(desc(total_amount))\n\n# A tibble: 8 × 2\n  zip5  total_amount\n  &lt;chr&gt;        &lt;dbl&gt;\n1 21202       648069\n2 21601       400000\n3 20005       250000\n4 21401       190570\n5 20772       128041\n6 21701       116708\n7 21740        63070\n8 21014        52490\n\n\nWhy, it’s downtown Baltimore, including the Inner Harbor area.",
    "crumbs": [
      "Data Cleaning Part II: Janitor"
    ]
  },
  {
    "objectID": "filters.html",
    "href": "filters.html",
    "title": "Filters and selections",
    "section": "",
    "text": "More often than not, we have more data than we want. Sometimes we need to be rid of that data. In dplyr, there’s two ways to go about this: filtering and selecting.\nFiltering creates a subset of the data based on criteria. All records where the amount is greater than 150,000. All records that match “College Park”. Something like that. Filtering works with rows – when we filter, we get fewer rows back than we start with.\nSelecting simply returns only the fields named. So if you only want to see city and amount, you select those fields. When you look at your data again, you’ll have two columns. If you try to use one of your columns that you had before you used select, you’ll get an error. Selecting works with columns. You will have the same number of records when you are done, but fewer columns of data to work with.\nLet’s continue to work with the UMD course data we used in the previous chapter. First, we need to load the tidyverse:\n\nlibrary(tidyverse)\n\n\numd_courses &lt;- read_rds(\"data/umd_courses.rds\")\n\nIf we want to see only those courses offered a particular department, we can use the filter function to isolate just those records. Filter works with something called a comparison operator. We need to filter all records equal to “Journalism”. The comparison operators in R, like most programming languages, are == for equal to, != for not equal to, &gt; for greater than, &gt;= for greater than or equal to and so on.\nBe careful: = is not == and = is not “equal to”. = is an assignment operator in most languages – how things get named.\n\njournalism_courses &lt;- umd_courses |&gt; filter(department == \"Journalism\")\n\nhead(journalism_courses)\n\n# A tibble: 6 × 9\n  id       title        description   term department sections instructors seats\n  &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n1 JOUR282  Beyond Face… \"Credit on… 202112 Journalism        1 Denitsa Yo…    35\n2 JOUR698  Special Pro…  &lt;NA&gt;       202112 Journalism        0 &lt;NA&gt;            0\n3 JOUR175  Media Liter… \"Additiona… 202112 Journalism        1 Susan Moel…    20\n4 JOUR199  Survey Appr…  &lt;NA&gt;       202112 Journalism        1 Karen Denny     5\n5 JOUR130  Self-Presen… \"Credit on… 202112 Journalism        1 Amber Moore    25\n6 JOUR459W Special Top… \"This cour… 202112 Journalism        1 Shannon Sc…    23\n# ℹ 1 more variable: syllabus_count &lt;dbl&gt;\n\n\nAnd just like that, we have just Journalism results, which we can verify looking at the head, the first six rows.\nWe also have more data than we might want. For example, we may only want to work with the course id and title.\nTo simplify our dataset, we can use select.\n\nselected_journalism_courses &lt;- journalism_courses |&gt; select(id, title)\n\nhead(selected_journalism_courses)\n\n# A tibble: 6 × 2\n  id       title                                                                \n  &lt;chr&gt;    &lt;chr&gt;                                                                \n1 JOUR282  Beyond Facebook: How Social Media are Transforming Society, Culture,…\n2 JOUR698  Special Problems in Communication                                    \n3 JOUR175  Media Literacy                                                       \n4 JOUR199  Survey Apprenticeship                                                \n5 JOUR130  Self-Presentation in the Age of YouTube                              \n6 JOUR459W Special Topics in Journalism; Sports Media & Athlete Branding        \n\n\nAnd now we only have two columns of data for whatever analysis we might want to do.\n\n\nSo let’s say we wanted to see all the courses in the Theatre department with at least 15 seats. We can do this a number of ways. The first is we can chain together a whole lot of filters.\n\ntheatre_seats_15 &lt;- umd_courses |&gt; filter(department == \"Theatre\") |&gt; filter(seats &gt;= 15)\n\nnrow(theatre_seats_15)\n\n[1] 308\n\n\nThat gives us 308 records But that’s repetitive, no? We can do better using a single filter and boolean operators – AND and OR. In this case, AND is & and OR is |.\nThe difference? With AND, all conditions must be true to be included. With OR, any of those conditions things can be true and it will be included.\nHere’s the difference.\n\nand_theatre_seats_15 &lt;- umd_courses |&gt; filter(department == \"Theatre\" & seats &gt;= 15)\n\nnrow(and_theatre_seats_15)\n\n[1] 308\n\n\nSo AND gives us the same answer we got before. What does OR give us?\n\nand_theatre_seats_15 &lt;- umd_courses |&gt; filter(department == \"Theatre\" | seats &gt;= 15)\n\nnrow(and_theatre_seats_15)\n\n[1] 54000\n\n\nSo there’s 54,000 rows that are EITHER Theatre classes OR have at least 15 seats. OR is additive; AND is restrictive.\nA general tip about using filter: it’s easier to work your way towards the filter syntax you need rather than try and write it once and trust the result. Each time you modify your filter, check the results to see if they make sense. This adds a little time to your process but you’ll thank yourself for doing it because it helps avoid mistakes.",
    "crumbs": [
      "Filters and selections"
    ]
  },
  {
    "objectID": "filters.html#combining-filters",
    "href": "filters.html#combining-filters",
    "title": "Filters and selections",
    "section": "",
    "text": "So let’s say we wanted to see all the courses in the Theatre department with at least 15 seats. We can do this a number of ways. The first is we can chain together a whole lot of filters.\n\ntheatre_seats_15 &lt;- umd_courses |&gt; filter(department == \"Theatre\") |&gt; filter(seats &gt;= 15)\n\nnrow(theatre_seats_15)\n\n[1] 308\n\n\nThat gives us 308 records But that’s repetitive, no? We can do better using a single filter and boolean operators – AND and OR. In this case, AND is & and OR is |.\nThe difference? With AND, all conditions must be true to be included. With OR, any of those conditions things can be true and it will be included.\nHere’s the difference.\n\nand_theatre_seats_15 &lt;- umd_courses |&gt; filter(department == \"Theatre\" & seats &gt;= 15)\n\nnrow(and_theatre_seats_15)\n\n[1] 308\n\n\nSo AND gives us the same answer we got before. What does OR give us?\n\nand_theatre_seats_15 &lt;- umd_courses |&gt; filter(department == \"Theatre\" | seats &gt;= 15)\n\nnrow(and_theatre_seats_15)\n\n[1] 54000\n\n\nSo there’s 54,000 rows that are EITHER Theatre classes OR have at least 15 seats. OR is additive; AND is restrictive.\nA general tip about using filter: it’s easier to work your way towards the filter syntax you need rather than try and write it once and trust the result. Each time you modify your filter, check the results to see if they make sense. This adds a little time to your process but you’ll thank yourself for doing it because it helps avoid mistakes.",
    "crumbs": [
      "Filters and selections"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "If you were at all paying attention in pre-college science classes, you have probably seen this equation:\nd = rt or distance = rate*time\nIn English, that says we can know how far something has traveled if we know how fast it’s going and for how long. If we multiply the rate by the time, we’ll get the distance.\nIf you remember just a bit about algebra, you know we can move these things around. If we know two of them, we can figure out the third. So, for instance, if we know the distance and we know the time, we can use algebra to divide the distance by the time to get the rate.\nd/t = r or distance/time = rate\nIn 2012, the South Florida Sun Sentinel found a story in this formula.\nPeople were dying on South Florida tollways in terrible car accidents. What made these different from other car fatal car accidents that happen every day in the US? Police officers driving way too fast were causing them.\nBut do police regularly speed on tollways or were there just a few random and fatal exceptions?\nThanks to Florida’s public records laws, the Sun Sentinel got records from the toll transponders in police cars in south Florida. The transponders recorded when a car went through a given place. And then it would do it again. And again.\nGiven that those places are fixed – they’re toll plazas – and they had the time it took to go from one toll plaza to another, they had the distance and the time.\nIt took high school algebra to find how fast police officers were driving. And the results were shocking.\nTwenty percent of police officers had exceeded 90 miles per hour on toll roads. In a 13-month period, officers drove between 90 and 110 mph more than 5,000 times. And these were just instances found on toll roads. Not all roads have tolls.\nThe story was a stunning find, and the newspaper documented case after case of police officers violating the law and escaping punishment. And, in 2013, they won the Pulitzer Prize for Public Service.\nAll with simple high school algebra.\n\n\nIt’s a single word in a single job description, but a Buzzfeed job posting in 2017 is another indicator in what could be a profound shift in how data journalism is both practiced and taught.\n“We’re looking for someone with a passion for news and a commitment to using data to find amazing, important stories — both quick hits and deeper analyses that drive conversations,” the posting seeking a data journalist says. It goes on to list five things BuzzFeed is looking for: Excellent collaborator, clear writer, deep statistical understanding, knowledge of obtaining and restructuring data.\nAnd then there’s this:\n“You should have a strong command of at least one toolset that (a) allows for filtering, joining, pivoting, and aggregating tabular data, and (b) enables reproducible workflows.”\nThis is not the data journalism of 20 years ago. When it started, it was a small group of people in newsrooms using spreadsheets and databases. Data journalism now encompases programming for all kinds of purposes, product development, user interface design, data visualization and graphics on top of more traditional skills like analyzing data and writing stories.\nIn this book, you’ll get a taste of modern data journalism through programming in R, a statistics language. You’ll be challenged to think programmatically while thinking about a story you can tell to readers in a way that they’ll want to read. They might seem like two different sides of the brain – mutually exclusive skills. They aren’t. I’m confident you’ll see programming is a creative endeavor and storytelling can be analytical.\nCombining them together has the power to change policy, expose injustice and deeply inform.\n\n\n\nThis book is all in the R statistical language. To follow along, you’ll do the following:\n\nInstall the R language on your computer. Go to the R Project website, click download R and select a mirror closest to your location. Then download the version for your computer.\nInstall R Studio Desktop. The free version is great.\n\nGoing forward, you’ll see passages like this:\n\ninstall.packages(\"tidyverse\")\n\nThat is code that you’ll need to run in your R Studio. When you see that, you’ll know what to do.\n\n\n\nThis book is the collection of class materials originally written for Matt Waite’s Data Journalism class at the University of Nebraska-Lincoln’s College of Journalism and Mass Communications. It has been substantially updated by Derek Willis and Sean Mussenden for data journalism classes at the University of Maryland Philip Merrill College of Journalism, with contributions from Sarah Cohen of Arizona State University.\nThere’s some things you should know about it:\n\nIt is free for students.\nThe topics will remain the same but the text is going to be constantly tinkered with.\nWhat is the work of the authors is copyright Matt Waite 2020, Sarah Cohen 2022 and Derek Willis and Sean Mussenden 2023.\nThe text is Attribution-NonCommercial-ShareAlike 4.0 International Creative Commons licensed. That means you can share it and change it, but only if you share your changes with the same license and it cannot be used for commercial purposes. I’m not making money on this so you can’t either.\n\nAs such, the whole book – authored in Quarto – in its original form is open sourced on Github. Pull requests welcomed!\n\n\n\n\n\nGoogle Sheets\nPublic records and open data\nR Basics\nReplication\nData basics and structures\nAggregates\nMutating\nWorking with dates\nFilters\nCleaning I: Data smells\nCleaning II: Janitor\nCleaning III: Open Refine\nCleaning IV: Pulling Data from PDFs\nJoins\nBasic data scraping\nGetting data from APIs: Census\nVisualizing for reporting: Basics\nVisualizing for reporting: Publishing\nGeographic data basics\nGeographic queries\nGeographic visualization\nText analysis basics\nBasic statistics\nWriting with and about data\nData journalism ethics",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#modern-data-journalism",
    "href": "index.html#modern-data-journalism",
    "title": "Introduction",
    "section": "",
    "text": "It’s a single word in a single job description, but a Buzzfeed job posting in 2017 is another indicator in what could be a profound shift in how data journalism is both practiced and taught.\n“We’re looking for someone with a passion for news and a commitment to using data to find amazing, important stories — both quick hits and deeper analyses that drive conversations,” the posting seeking a data journalist says. It goes on to list five things BuzzFeed is looking for: Excellent collaborator, clear writer, deep statistical understanding, knowledge of obtaining and restructuring data.\nAnd then there’s this:\n“You should have a strong command of at least one toolset that (a) allows for filtering, joining, pivoting, and aggregating tabular data, and (b) enables reproducible workflows.”\nThis is not the data journalism of 20 years ago. When it started, it was a small group of people in newsrooms using spreadsheets and databases. Data journalism now encompases programming for all kinds of purposes, product development, user interface design, data visualization and graphics on top of more traditional skills like analyzing data and writing stories.\nIn this book, you’ll get a taste of modern data journalism through programming in R, a statistics language. You’ll be challenged to think programmatically while thinking about a story you can tell to readers in a way that they’ll want to read. They might seem like two different sides of the brain – mutually exclusive skills. They aren’t. I’m confident you’ll see programming is a creative endeavor and storytelling can be analytical.\nCombining them together has the power to change policy, expose injustice and deeply inform.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#installations",
    "href": "index.html#installations",
    "title": "Introduction",
    "section": "",
    "text": "This book is all in the R statistical language. To follow along, you’ll do the following:\n\nInstall the R language on your computer. Go to the R Project website, click download R and select a mirror closest to your location. Then download the version for your computer.\nInstall R Studio Desktop. The free version is great.\n\nGoing forward, you’ll see passages like this:\n\ninstall.packages(\"tidyverse\")\n\nThat is code that you’ll need to run in your R Studio. When you see that, you’ll know what to do.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "Introduction",
    "section": "",
    "text": "This book is the collection of class materials originally written for Matt Waite’s Data Journalism class at the University of Nebraska-Lincoln’s College of Journalism and Mass Communications. It has been substantially updated by Derek Willis and Sean Mussenden for data journalism classes at the University of Maryland Philip Merrill College of Journalism, with contributions from Sarah Cohen of Arizona State University.\nThere’s some things you should know about it:\n\nIt is free for students.\nThe topics will remain the same but the text is going to be constantly tinkered with.\nWhat is the work of the authors is copyright Matt Waite 2020, Sarah Cohen 2022 and Derek Willis and Sean Mussenden 2023.\nThe text is Attribution-NonCommercial-ShareAlike 4.0 International Creative Commons licensed. That means you can share it and change it, but only if you share your changes with the same license and it cannot be used for commercial purposes. I’m not making money on this so you can’t either.\n\nAs such, the whole book – authored in Quarto – in its original form is open sourced on Github. Pull requests welcomed!",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#what-well-cover",
    "href": "index.html#what-well-cover",
    "title": "Introduction",
    "section": "",
    "text": "Google Sheets\nPublic records and open data\nR Basics\nReplication\nData basics and structures\nAggregates\nMutating\nWorking with dates\nFilters\nCleaning I: Data smells\nCleaning II: Janitor\nCleaning III: Open Refine\nCleaning IV: Pulling Data from PDFs\nJoins\nBasic data scraping\nGetting data from APIs: Census\nVisualizing for reporting: Basics\nVisualizing for reporting: Publishing\nGeographic data basics\nGeographic queries\nGeographic visualization\nText analysis basics\nBasic statistics\nWriting with and about data\nData journalism ethics",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "mutating.html",
    "href": "mutating.html",
    "title": "Mutating data",
    "section": "",
    "text": "Often the data you have will prompt questions that it doesn’t immediately answer. Election results, for example, have raw vote totals but we often don’t use those to make comparisons between candidates unless the numbers are small. We need percentages!\nTo do that in R, we can use dplyr and mutate to calculate new metrics in a new field using existing fields of data. That’s the essence of mutate - using the data you have to answer a new question.\nSo first we’ll import the tidyverse so we can read in our data and begin to work with it.\n\nlibrary(tidyverse)\n\nNow we’ll import a dataset of county-level gubernatorial results from Maryland’s 2022 general election that is in the data folder in this chapter’s pre-lab directory. We’ll use this to explore ways to create new information from existing data.\n\ngeneral_22 &lt;- read_csv('data/md_gov_county.csv')\n\nRows: 24 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): county\ndbl (7): fips_code, cox, moore, lashar, wallace, harding, write_ins\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet’s add a column called percent_moore for the percentage of votes that went to Wes Moore, the Democratic candidate who won the election, in each county. The code to calculate a percentage is pretty simple. Remember, with summarize, we used n() to count things. With mutate, we use very similar syntax to calculate a new value – a new column of data – using other values in our dataset.\nTo calculate a percentage, we need both the number of votes for Moore but also the total number of votes. We’ll use mutate to create both columns. The first will be total votes. The key here is to save the dataframe to itself so that our changes stick.\n\ngeneral_22 &lt;- general_22 |&gt;\n  mutate(\n    total_votes = cox + moore + lashar + wallace + write_ins,\n    pct_moore = moore/total_votes\n  )\n\nBut what do you see right away? Do those numbers look like we expect them to? No. They’re a decimal expressed as a percentage. So let’s fix that by multiplying by 100. Since we’re replacing the contents of our new pct_moore column, we can just update our previous code and run it again:\n\ngeneral_22 &lt;- general_22 |&gt;\n  mutate(\n    pct_moore = (moore/total_votes)*100\n  )\n\nNow, does this ordering do anything for us? No. Let’s fix that with arrange.\n\ngeneral_22 &lt;- general_22 |&gt;\n  mutate(\n    pct_moore = (moore/total_votes)*100\n  ) |&gt; \n  arrange(desc(pct_moore))\n\nSo now we have results ordered by pct_moore with the highest percentage first. To see the lowest percentage first, we can reverse that arrange function - we don’t need to recalculate the column:\n\ngeneral_22 |&gt;\n  arrange(pct_moore)\n\n# A tibble: 24 × 10\n   fips_code county       cox moore lashar wallace harding write_ins total_votes\n       &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n 1     24023 Garrett     8195  2402    194      52     138         0       10843\n 2     24001 Allegany   13833  6390    279     143     267         0       20645\n 3     24015 Cecil      18159  8691    472     155     311         0       27477\n 4     24011 Caroline    6727  3276    176      56     140         0       10235\n 5     24039 Somerset    3974  2254     67      46      81         0        6341\n 6     24043 Washington 26943 15723    614     252     472         0       43532\n 7     24013 Carroll    38969 25155   1515     436     561         0       66075\n 8     24047 Worcester  13433  8550    273     121     161         0       22377\n 9     24037 Saint Mar… 20279 13291    661     253     346         0       34484\n10     24035 Queen Ann… 12840  8577    416     120     212         0       21953\n# ℹ 14 more rows\n# ℹ 1 more variable: pct_moore &lt;dbl&gt;\n\n\nMoore had his weakest performance in Garrett County, at the far western edge of the state.\n\n\nMutate is also useful for standardizing data - for example, making different spellings of, say, campaign spending recipients.\nLet’s load some Maryland state campaign expenditures into a maryland_expenses dataframe, and focus in particular on the payee_name column.\n\nmaryland_expenses &lt;- read_csv(\"data/maryland_expenses.csv\")\n\nRows: 97912 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (12): expenditure_date, payee_name, address, payee_type, committee_name,...\ndbl  (1): amount\nlgl  (1): expense_toward\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmaryland_expenses\n\n# A tibble: 97,912 × 14\n   expenditure_date payee_name          address payee_type amount committee_name\n   &lt;chr&gt;            &lt;chr&gt;               &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;         \n 1 3/12/2021        &lt;NA&gt;                &lt;NA&gt;    Reimburse   350   Salling   Joh…\n 2 3/29/2021        Dundalk Eagle News… PO Box… Business/…  329   Salling   Joh…\n 3 4/29/2021        Dundalk Eagle News… PO Box… Business/…  400   Salling   Joh…\n 4 5/18/2021        Dundalk Eagle News… PO Box… Business/…  350   Salling   Joh…\n 5 6/9/2021         Dundalk Heritage F… Dundal… Business/…  200   Salling   Joh…\n 6 6/9/2021         Dundalk Heritage F… Dundal… Business/…  250   Salling   Joh…\n 7 6/1/2021         Neighborhood Signs  6655 a… Business/…   77.4 Salling   Joh…\n 8 4/16/2021        &lt;NA&gt;                &lt;NA&gt;    Reimburse   150   Salling   Joh…\n 9 7/1/2021         MSP CUSTOM SOL      1000 P… Business/…  238.  Salling   Joh…\n10 7/2/2021         Squire's Restaurant 6723 H… Business/…  260   Salling   Joh…\n# ℹ 97,902 more rows\n# ℹ 8 more variables: expense_category &lt;chr&gt;, expense_purpose &lt;chr&gt;,\n#   expense_toward &lt;lgl&gt;, expense_method &lt;chr&gt;, vendor &lt;chr&gt;, fundtype &lt;chr&gt;,\n#   comments &lt;chr&gt;, x14 &lt;chr&gt;\n\n\nYou’ll notice that there’s a mix of styles: lower-case and upper-case names like “Anedot” and “ANEDOT”, for example. R will think those are two different payees, and that will mean that any aggregates we create based on payee_name won’t be accurate.\nSo how can we fix that? Mutate - it’s not just for math! And a function called str_to_upper that will convert a character column into all uppercase.\n\nstandardized_maryland_expenses &lt;- maryland_expenses |&gt;\n  mutate(\n    payee_upper = str_to_upper(payee_name)\n)\n\nThere are lots of potential uses for standardization - addresses, zip codes, anything that can be misspelled or abbreviated.\n\n\n\nMutate is even more useful when combined with some additional functions. Let’s keep rolling with our expenditure data. Take a look at the address column: it contains a full address, including the state, spelled out. It would be useful to have a separate state column with an abbreviation. We can check to see if a state name is contained in that column and then populate a new column with the value we want, using the functions str_detect and case_when. We can identify the state by the following pattern: a space, followed by the full name, followed by another space. So, ” Maryland “. The case_when function handles multiple variations, such as if the state is Maryland or the state is Texas, etc. Crucially, we can tell R to populate the new column with NA if it doesn’t find a match.\n\nmaryland_expenses_with_state &lt;- maryland_expenses |&gt;\n  mutate(\n    state = case_when(\n        str_detect(address, \" Maryland \") ~ \"MD\",\n        str_detect(address, \" California \") ~ \"CA\",\n        str_detect(address, \" Washington \") ~ \"WA\",\n        str_detect(address, \" Louisiana \") ~ \"LA\",\n        str_detect(address, \" Florida \") ~ \"FL\",\n        str_detect(address, \" North Carolina \") ~ \"NC\",\n        str_detect(address, \" Massachusetts \") ~ \"MA\",\n        str_detect(address, \" West Virginia \") ~ \"WV\",\n        str_detect(address, \" Virginia \") ~ \"VA\",\n        .default = NA\n      )\n  )\n\nThere’s a lot going on here, so let’s unpack it. It starts out as a typical mutate statement, but case_when introduces some new things. Each line checks to see if the pattern is contained in the address column, followed by ~ and then a value for the new column for records that match that check. You can read it like this: “If we find ’ Maryland ’ in the address column, then put ‘MD’ in the state column” for Maryland and then a handful of states, and if we don’t match any state we’re looking for, make state NA.\nWe can then use our new state column in group_by statements to make summarizing easier.\n\nmaryland_expenses_with_state |&gt;\n  group_by(state) |&gt;\n  summarize(total = sum(amount)) |&gt;\n  arrange(desc(total))\n\n# A tibble: 10 × 2\n   state     total\n   &lt;chr&gt;     &lt;dbl&gt;\n 1 MD    77723146.\n 2 WA    15552127.\n 3 VA    10519646.\n 4 CA     3370284.\n 5 FL     1470592.\n 6 MA     1264728.\n 7 NC      691006.\n 8 LA      255522.\n 9 WV       41088.\n10 &lt;NA&gt;        NA \n\n\nMost expenditures seem to have occurred in Maryland, which makes sense, although we haven’t assigned a state for every transaction.\nMutate is there to make your data more useful and to make it easier for you to ask more and better questions of it.",
    "crumbs": [
      "Mutating data"
    ]
  },
  {
    "objectID": "mutating.html#another-use-of-mutate",
    "href": "mutating.html#another-use-of-mutate",
    "title": "Mutating data",
    "section": "",
    "text": "Mutate is also useful for standardizing data - for example, making different spellings of, say, campaign spending recipients.\nLet’s load some Maryland state campaign expenditures into a maryland_expenses dataframe, and focus in particular on the payee_name column.\n\nmaryland_expenses &lt;- read_csv(\"data/maryland_expenses.csv\")\n\nRows: 97912 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (12): expenditure_date, payee_name, address, payee_type, committee_name,...\ndbl  (1): amount\nlgl  (1): expense_toward\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmaryland_expenses\n\n# A tibble: 97,912 × 14\n   expenditure_date payee_name          address payee_type amount committee_name\n   &lt;chr&gt;            &lt;chr&gt;               &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;         \n 1 3/12/2021        &lt;NA&gt;                &lt;NA&gt;    Reimburse   350   Salling   Joh…\n 2 3/29/2021        Dundalk Eagle News… PO Box… Business/…  329   Salling   Joh…\n 3 4/29/2021        Dundalk Eagle News… PO Box… Business/…  400   Salling   Joh…\n 4 5/18/2021        Dundalk Eagle News… PO Box… Business/…  350   Salling   Joh…\n 5 6/9/2021         Dundalk Heritage F… Dundal… Business/…  200   Salling   Joh…\n 6 6/9/2021         Dundalk Heritage F… Dundal… Business/…  250   Salling   Joh…\n 7 6/1/2021         Neighborhood Signs  6655 a… Business/…   77.4 Salling   Joh…\n 8 4/16/2021        &lt;NA&gt;                &lt;NA&gt;    Reimburse   150   Salling   Joh…\n 9 7/1/2021         MSP CUSTOM SOL      1000 P… Business/…  238.  Salling   Joh…\n10 7/2/2021         Squire's Restaurant 6723 H… Business/…  260   Salling   Joh…\n# ℹ 97,902 more rows\n# ℹ 8 more variables: expense_category &lt;chr&gt;, expense_purpose &lt;chr&gt;,\n#   expense_toward &lt;lgl&gt;, expense_method &lt;chr&gt;, vendor &lt;chr&gt;, fundtype &lt;chr&gt;,\n#   comments &lt;chr&gt;, x14 &lt;chr&gt;\n\n\nYou’ll notice that there’s a mix of styles: lower-case and upper-case names like “Anedot” and “ANEDOT”, for example. R will think those are two different payees, and that will mean that any aggregates we create based on payee_name won’t be accurate.\nSo how can we fix that? Mutate - it’s not just for math! And a function called str_to_upper that will convert a character column into all uppercase.\n\nstandardized_maryland_expenses &lt;- maryland_expenses |&gt;\n  mutate(\n    payee_upper = str_to_upper(payee_name)\n)\n\nThere are lots of potential uses for standardization - addresses, zip codes, anything that can be misspelled or abbreviated.",
    "crumbs": [
      "Mutating data"
    ]
  },
  {
    "objectID": "mutating.html#a-more-powerful-use",
    "href": "mutating.html#a-more-powerful-use",
    "title": "Mutating data",
    "section": "",
    "text": "Mutate is even more useful when combined with some additional functions. Let’s keep rolling with our expenditure data. Take a look at the address column: it contains a full address, including the state, spelled out. It would be useful to have a separate state column with an abbreviation. We can check to see if a state name is contained in that column and then populate a new column with the value we want, using the functions str_detect and case_when. We can identify the state by the following pattern: a space, followed by the full name, followed by another space. So, ” Maryland “. The case_when function handles multiple variations, such as if the state is Maryland or the state is Texas, etc. Crucially, we can tell R to populate the new column with NA if it doesn’t find a match.\n\nmaryland_expenses_with_state &lt;- maryland_expenses |&gt;\n  mutate(\n    state = case_when(\n        str_detect(address, \" Maryland \") ~ \"MD\",\n        str_detect(address, \" California \") ~ \"CA\",\n        str_detect(address, \" Washington \") ~ \"WA\",\n        str_detect(address, \" Louisiana \") ~ \"LA\",\n        str_detect(address, \" Florida \") ~ \"FL\",\n        str_detect(address, \" North Carolina \") ~ \"NC\",\n        str_detect(address, \" Massachusetts \") ~ \"MA\",\n        str_detect(address, \" West Virginia \") ~ \"WV\",\n        str_detect(address, \" Virginia \") ~ \"VA\",\n        .default = NA\n      )\n  )\n\nThere’s a lot going on here, so let’s unpack it. It starts out as a typical mutate statement, but case_when introduces some new things. Each line checks to see if the pattern is contained in the address column, followed by ~ and then a value for the new column for records that match that check. You can read it like this: “If we find ’ Maryland ’ in the address column, then put ‘MD’ in the state column” for Maryland and then a handful of states, and if we don’t match any state we’re looking for, make state NA.\nWe can then use our new state column in group_by statements to make summarizing easier.\n\nmaryland_expenses_with_state |&gt;\n  group_by(state) |&gt;\n  summarize(total = sum(amount)) |&gt;\n  arrange(desc(total))\n\n# A tibble: 10 × 2\n   state     total\n   &lt;chr&gt;     &lt;dbl&gt;\n 1 MD    77723146.\n 2 WA    15552127.\n 3 VA    10519646.\n 4 CA     3370284.\n 5 FL     1470592.\n 6 MA     1264728.\n 7 NC      691006.\n 8 LA      255522.\n 9 WV       41088.\n10 &lt;NA&gt;        NA \n\n\nMost expenditures seem to have occurred in Maryland, which makes sense, although we haven’t assigned a state for every transaction.\nMutate is there to make your data more useful and to make it easier for you to ask more and better questions of it.",
    "crumbs": [
      "Mutating data"
    ]
  },
  {
    "objectID": "pdfs.html",
    "href": "pdfs.html",
    "title": "Cleaning Data Part IV: PDFs",
    "section": "",
    "text": "The next circle of Hell on the Dante’s Inferno of Data Journalism is the PDF. Governments everywhere love the PDF and publish all kinds of records in a PDF. The problem is a PDF isn’t a data format – it’s a middle finger, saying I’ve Got Your Accountability Right Here, Pal.\nIt’s so ridiculous that there’s a constellation of tools that do nothing more than try to harvest tables out of PDFs. There are online services like CometDocs where you can upload your PDF and point and click your way into an Excel file. There are mobile device apps that take a picture of a table and convert it into a spreadsheet. But one of the best is a tool called Tabula. It was build by journalists for journalists.\nThere is a version of Tabula that will run inside of R – a library called Tabulizer – but the truth is I’m having the hardest time installing it on my machine, which leads me to believe that trying to install it across a classroom of various machines would be disastrous. The standalone version works just fine, and it provides a useful way for you to see what’s actually going on.\nUnfortunately, harvesting tables from PDFs with Tabula is an exercise in getting your hopes up, only to have them dashed. We’ll start with an example. First, let’s load the tidyverse and janitor.\n\nlibrary(tidyverse)\nlibrary(janitor)\n\n\n\nTabula works best when tables in PDFs are clearly defined and have nicely-formatted information. Here’s a perfect example: active voters by county in Maryland.\nDownload and install Tabula. Tabula works much the same way as Open Refine does – it works in the browser by spinning up a small webserver in your computer.\nWhen Tabula opens, you click browse to find the PDF on your computer somewhere, and then click import. After it imports, click autodetect tables. You’ll see red boxes appear around what Tabula believes are the tables. You’ll see it does a pretty good job at this.\n\n\n\n\n\n\n\n\n\nNow you can hit the green “Preview & Export Extracted Data” button on the top right. You should see something very like this:\n\n\n\n\n\n\n\n\n\nYou can now export that extracted table to a CSV file using the “Export” button. And then we can read it into R:\n\nvoters_by_county &lt;- read_csv(\"data/tabula-Eligible Active Voters by County - GG22.csv\")\n\nRows: 25 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): County\ndbl (1): BAR\nnum (8): DEM, REP, GRN, LIB, WCP, OTH, UNA, TOTAL\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nvoters_by_county\n\n# A tibble: 25 × 10\n   County              DEM    REP   BAR   GRN   LIB   WCP   OTH    UNA  TOTAL\n   &lt;chr&gt;             &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Allegany          11793  22732     0    76   223    73   382   8337  43616\n 2 Anne Arundel     173922 129893     0   632  2257   434  2951  96403 406492\n 3 Baltimore City   303620  28211     0   908  1080   666  3984  56665 395134\n 4 Baltimore County 309297 137378     0   898  2493   687  5921 106789 563463\n 5 Calvert           23779  27912     0    94   410    69   548  15169  67981\n 6 Caroline           6250  10539     0    34   108    40   160   4454  21585\n 7 Carroll           33572  63771     0   191   797   102  1033  28139 127605\n 8 Cecil             20666  31961     0   111   447   104   678  16360  70327\n 9 Charles           74373  23334     0   129   425   143   864  21819 121087\n10 Dorchester         9608   8965     0    22   110    33   191   3745  22674\n# ℹ 15 more rows\n\n\nBoom - we’re good to go.\n\n\n\nHere’s a slightly more involved PDF, from Maryland’s 2020 annual report on unintentional drug and alcohol-related intoxication deaths. Specifically, we’re looking at Table 7 on page 67 of the report which lists the number of fentanyl-related deaths by jurisdiction:\n\n\n\n\n\n\n\n\n\nLooks like a spreadsheet, right? Save that PDF file to your computer in a place where you’ll remember it (like a Downloads folder).\nNow let’s repeat the steps we did to import the PDF into Tabula, go to page 67. It should look like this:\n\n\n\n\n\n\n\n\n\nLet’s draw a box around what we want, but there’s a catch: the headers aren’t on a single line. If you draw your box around the whole table and preview, you’ll see that there’s a problem. To fix that, we’ll need to limit our box to just the data. Using your cursor, click and drag a box across the table so it looks like this:\n\n\n\n\n\n\n\n\n\nNow you can hit the green “Preview & Export Extracted Data” button on the top right. Using the “Stream” method, you should see something very like this:\n\n\n\n\n\n\n\n\n\nYou can now export that extracted table to a CSV file using the “Export” button. And then we can read it into R and clean up the column names and some other things:\n\nfentanyl_deaths &lt;- read_csv(\"data/tabula-Annual_2020_Drug_Intox_Report.csv\") |&gt; clean_names()\n\nRows: 29 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): MARYLAND ......................................\ndbl (6): 26, 29, 58, 186, 340, 1,119\nnum (5): 1,594, 1,888, 1,927, 2,342, 9,509\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nfentanyl_deaths\n\n# A tibble: 29 × 12\n   maryland       x26   x29   x58  x186  x340 x1_119 x1_594 x1_888 x1_927 x2_342\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 NORTHWEST A…     6     3     7     8    32    109    119    166    146    200\n 2 GARRETT ...…     1     0     0     0     2      0      2      2      5      5\n 3 ALLEGANY ..…     1     1     1     1     5     29     29     29     19     44\n 4 WASHINGTON …     1     1     4     1    14     31     39     70     70     95\n 5 FREDERICK .…     3     1     2     6    11     49     49     65     52     56\n 6 BALTIMORE M…    10    16    35   142   248    792   1118   1415   1395   1605\n 7 BALTIMORE C…     2     4    12    72   120    419    573    758    810    920\n 8 BALTIMORE C…     4     5    11    36    65    182    244    308    285    328\n 9 ANNE ARUNDE…     2     3     6    23    29     98    152    184    164    209\n10 CARROLL ...…     0     1     2     4    11     20     40     55     47     37\n# ℹ 19 more rows\n# ℹ 1 more variable: x9_509 &lt;dbl&gt;\n\n\n\n\n\nThe good news is that we have data we don’t have to retype. The bad news is, we have a few things to fix, starting with the fact that the headers shouldn’t be headers. Let’s start by re-importing it and specifying that the first row doesn’t have column headers:\n\nfentanyl_deaths &lt;- read_csv(\"data/tabula-Annual_2020_Drug_Intox_Report.csv\", col_names = FALSE) |&gt; clean_names()\n\nRows: 30 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): X1\ndbl (5): X2, X3, X4, X5, X6\nnum (6): X7, X8, X9, X10, X11, X12\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nfentanyl_deaths\n\n# A tibble: 30 × 12\n   x1             x2    x3    x4    x5    x6    x7    x8    x9   x10   x11   x12\n   &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 MARYLAND .…    26    29    58   186   340  1119  1594  1888  1927  2342  9509\n 2 NORTHWEST …     6     3     7     8    32   109   119   166   146   200   796\n 3 GARRETT ..…     1     0     0     0     2     0     2     2     5     5    17\n 4 ALLEGANY .…     1     1     1     1     5    29    29    29    19    44   159\n 5 WASHINGTON…     1     1     4     1    14    31    39    70    70    95   326\n 6 FREDERICK …     3     1     2     6    11    49    49    65    52    56   294\n 7 BALTIMORE …    10    16    35   142   248   792  1118  1415  1395  1605  6776\n 8 BALTIMORE …     2     4    12    72   120   419   573   758   810   920  3690\n 9 BALTIMORE …     4     5    11    36    65   182   244   308   285   328  1468\n10 ANNE ARUND…     2     3     6    23    29    98   152   184   164   209   870\n# ℹ 20 more rows\n\n\nOk, now we have all the data. But we need actual headers. Let’s add those using rename(), keeping in mind that the new name comes first.\n\nfentanyl_deaths &lt;- read_csv(\"data/tabula-Annual_2020_Drug_Intox_Report.csv\", col_names = FALSE) |&gt; \n  clean_names() |&gt; \n  rename(jurisdiction = x1, deaths_2011 = x2, deaths_2012 = x3, deaths_2013 = x4, deaths_2014 = x5, deaths_2015 = x6, deaths_2016 = x7, deaths_2017 = x8, \n         deaths_2018 = x9, deaths_2019 = x10, deaths_2020 = x11, deaths_total = x12)\n\nRows: 30 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): X1\ndbl (5): X2, X3, X4, X5, X6\nnum (6): X7, X8, X9, X10, X11, X12\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nfentanyl_deaths\n\n# A tibble: 30 × 12\n   jurisdiction      deaths_2011 deaths_2012 deaths_2013 deaths_2014 deaths_2015\n   &lt;chr&gt;                   &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 MARYLAND .......…          26          29          58         186         340\n 2 NORTHWEST AREA .…           6           3           7           8          32\n 3 GARRETT ........…           1           0           0           0           2\n 4 ALLEGANY .......…           1           1           1           1           5\n 5 WASHINGTON .....…           1           1           4           1          14\n 6 FREDERICK ......…           3           1           2           6          11\n 7 BALTIMORE METRO …          10          16          35         142         248\n 8 BALTIMORE CITY .…           2           4          12          72         120\n 9 BALTIMORE COUNTY…           4           5          11          36          65\n10 ANNE ARUNDEL ...…           2           3           6          23          29\n# ℹ 20 more rows\n# ℹ 6 more variables: deaths_2016 &lt;dbl&gt;, deaths_2017 &lt;dbl&gt;, deaths_2018 &lt;dbl&gt;,\n#   deaths_2019 &lt;dbl&gt;, deaths_2020 &lt;dbl&gt;, deaths_total &lt;dbl&gt;\n\n\nWe could stop here, but there are a bunch of periods in the jurisdiction column and it’s better to remove them - it will make filtering easier. Let’s use str_replace_all() to do that:\n\nfentanyl_deaths &lt;- read_csv(\"data/tabula-Annual_2020_Drug_Intox_Report.csv\", col_names = FALSE) |&gt; \n  clean_names() |&gt; \n  rename(jurisdiction = x1, deaths_2011 = x2, deaths_2012 = x3, deaths_2013 = x4, deaths_2014 = x5, deaths_2015 = x6, deaths_2016 = x7, deaths_2017 = x8, \n         deaths_2018 = x9, deaths_2019 = x10, deaths_2020 = x11, deaths_total = x12) |&gt; \n  mutate(jurisdiction = str_squish(str_replace_all(jurisdiction,'\\\\.','')))\n\nRows: 30 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): X1\ndbl (5): X2, X3, X4, X5, X6\nnum (6): X7, X8, X9, X10, X11, X12\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nfentanyl_deaths\n\n# A tibble: 30 × 12\n   jurisdiction      deaths_2011 deaths_2012 deaths_2013 deaths_2014 deaths_2015\n   &lt;chr&gt;                   &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 MARYLAND                   26          29          58         186         340\n 2 NORTHWEST AREA              6           3           7           8          32\n 3 GARRETT                     1           0           0           0           2\n 4 ALLEGANY                    1           1           1           1           5\n 5 WASHINGTON                  1           1           4           1          14\n 6 FREDERICK                   3           1           2           6          11\n 7 BALTIMORE METRO …          10          16          35         142         248\n 8 BALTIMORE CITY              2           4          12          72         120\n 9 BALTIMORE COUNTY            4           5          11          36          65\n10 ANNE ARUNDEL                2           3           6          23          29\n# ℹ 20 more rows\n# ℹ 6 more variables: deaths_2016 &lt;dbl&gt;, deaths_2017 &lt;dbl&gt;, deaths_2018 &lt;dbl&gt;,\n#   deaths_2019 &lt;dbl&gt;, deaths_2020 &lt;dbl&gt;, deaths_total &lt;dbl&gt;\n\n\nThere are a few important things to explain here:\n\nBecause we’re replacing a literal period (.), we need to make sure that R knows that. Hence the ‘\\.’ Why? Because ‘.’ is a valid expression meaning “any character”, so if we didn’t have the backslashes the above code would make the entire column blank (try it!)\nThe str_squish function cleans up any excess spaces, at the beginning, middle or end of a character column. If we then use filter, we can do so with confidence.\nI put “deaths_” in front of each yearly column because R likes it when columns don’t begin with a number. You can have a column called 2011, but you literally have to use the backticks (2011) to refer to it in code.\n\nAll things considered, that was pretty easy. Many - most? - electronic PDFs aren’t so easy to parse. Sometimes you’ll need to open the exported CSV file and clean things up before importing into R. Other times you’ll be able to do that cleaning in R itself.\nHere’s the sad truth: THIS IS PRETTY GOOD. It sure beats typing it out. And since many government processes don’t change all that much, you can save the code to process subsequent versions of PDFs.",
    "crumbs": [
      "Cleaning Data Part IV: PDFs"
    ]
  },
  {
    "objectID": "pdfs.html#easy-does-it",
    "href": "pdfs.html#easy-does-it",
    "title": "Cleaning Data Part IV: PDFs",
    "section": "",
    "text": "Tabula works best when tables in PDFs are clearly defined and have nicely-formatted information. Here’s a perfect example: active voters by county in Maryland.\nDownload and install Tabula. Tabula works much the same way as Open Refine does – it works in the browser by spinning up a small webserver in your computer.\nWhen Tabula opens, you click browse to find the PDF on your computer somewhere, and then click import. After it imports, click autodetect tables. You’ll see red boxes appear around what Tabula believes are the tables. You’ll see it does a pretty good job at this.\n\n\n\n\n\n\n\n\n\nNow you can hit the green “Preview & Export Extracted Data” button on the top right. You should see something very like this:\n\n\n\n\n\n\n\n\n\nYou can now export that extracted table to a CSV file using the “Export” button. And then we can read it into R:\n\nvoters_by_county &lt;- read_csv(\"data/tabula-Eligible Active Voters by County - GG22.csv\")\n\nRows: 25 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): County\ndbl (1): BAR\nnum (8): DEM, REP, GRN, LIB, WCP, OTH, UNA, TOTAL\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nvoters_by_county\n\n# A tibble: 25 × 10\n   County              DEM    REP   BAR   GRN   LIB   WCP   OTH    UNA  TOTAL\n   &lt;chr&gt;             &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Allegany          11793  22732     0    76   223    73   382   8337  43616\n 2 Anne Arundel     173922 129893     0   632  2257   434  2951  96403 406492\n 3 Baltimore City   303620  28211     0   908  1080   666  3984  56665 395134\n 4 Baltimore County 309297 137378     0   898  2493   687  5921 106789 563463\n 5 Calvert           23779  27912     0    94   410    69   548  15169  67981\n 6 Caroline           6250  10539     0    34   108    40   160   4454  21585\n 7 Carroll           33572  63771     0   191   797   102  1033  28139 127605\n 8 Cecil             20666  31961     0   111   447   104   678  16360  70327\n 9 Charles           74373  23334     0   129   425   143   864  21819 121087\n10 Dorchester         9608   8965     0    22   110    33   191   3745  22674\n# ℹ 15 more rows\n\n\nBoom - we’re good to go.",
    "crumbs": [
      "Cleaning Data Part IV: PDFs"
    ]
  },
  {
    "objectID": "pdfs.html#when-it-looks-good-but-needs-a-little-fixing",
    "href": "pdfs.html#when-it-looks-good-but-needs-a-little-fixing",
    "title": "Cleaning Data Part IV: PDFs",
    "section": "",
    "text": "Here’s a slightly more involved PDF, from Maryland’s 2020 annual report on unintentional drug and alcohol-related intoxication deaths. Specifically, we’re looking at Table 7 on page 67 of the report which lists the number of fentanyl-related deaths by jurisdiction:\n\n\n\n\n\n\n\n\n\nLooks like a spreadsheet, right? Save that PDF file to your computer in a place where you’ll remember it (like a Downloads folder).\nNow let’s repeat the steps we did to import the PDF into Tabula, go to page 67. It should look like this:\n\n\n\n\n\n\n\n\n\nLet’s draw a box around what we want, but there’s a catch: the headers aren’t on a single line. If you draw your box around the whole table and preview, you’ll see that there’s a problem. To fix that, we’ll need to limit our box to just the data. Using your cursor, click and drag a box across the table so it looks like this:\n\n\n\n\n\n\n\n\n\nNow you can hit the green “Preview & Export Extracted Data” button on the top right. Using the “Stream” method, you should see something very like this:\n\n\n\n\n\n\n\n\n\nYou can now export that extracted table to a CSV file using the “Export” button. And then we can read it into R and clean up the column names and some other things:\n\nfentanyl_deaths &lt;- read_csv(\"data/tabula-Annual_2020_Drug_Intox_Report.csv\") |&gt; clean_names()\n\nRows: 29 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): MARYLAND ......................................\ndbl (6): 26, 29, 58, 186, 340, 1,119\nnum (5): 1,594, 1,888, 1,927, 2,342, 9,509\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nfentanyl_deaths\n\n# A tibble: 29 × 12\n   maryland       x26   x29   x58  x186  x340 x1_119 x1_594 x1_888 x1_927 x2_342\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 NORTHWEST A…     6     3     7     8    32    109    119    166    146    200\n 2 GARRETT ...…     1     0     0     0     2      0      2      2      5      5\n 3 ALLEGANY ..…     1     1     1     1     5     29     29     29     19     44\n 4 WASHINGTON …     1     1     4     1    14     31     39     70     70     95\n 5 FREDERICK .…     3     1     2     6    11     49     49     65     52     56\n 6 BALTIMORE M…    10    16    35   142   248    792   1118   1415   1395   1605\n 7 BALTIMORE C…     2     4    12    72   120    419    573    758    810    920\n 8 BALTIMORE C…     4     5    11    36    65    182    244    308    285    328\n 9 ANNE ARUNDE…     2     3     6    23    29     98    152    184    164    209\n10 CARROLL ...…     0     1     2     4    11     20     40     55     47     37\n# ℹ 19 more rows\n# ℹ 1 more variable: x9_509 &lt;dbl&gt;",
    "crumbs": [
      "Cleaning Data Part IV: PDFs"
    ]
  },
  {
    "objectID": "pdfs.html#cleaning-up-the-data-in-r",
    "href": "pdfs.html#cleaning-up-the-data-in-r",
    "title": "Cleaning Data Part IV: PDFs",
    "section": "",
    "text": "The good news is that we have data we don’t have to retype. The bad news is, we have a few things to fix, starting with the fact that the headers shouldn’t be headers. Let’s start by re-importing it and specifying that the first row doesn’t have column headers:\n\nfentanyl_deaths &lt;- read_csv(\"data/tabula-Annual_2020_Drug_Intox_Report.csv\", col_names = FALSE) |&gt; clean_names()\n\nRows: 30 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): X1\ndbl (5): X2, X3, X4, X5, X6\nnum (6): X7, X8, X9, X10, X11, X12\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nfentanyl_deaths\n\n# A tibble: 30 × 12\n   x1             x2    x3    x4    x5    x6    x7    x8    x9   x10   x11   x12\n   &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 MARYLAND .…    26    29    58   186   340  1119  1594  1888  1927  2342  9509\n 2 NORTHWEST …     6     3     7     8    32   109   119   166   146   200   796\n 3 GARRETT ..…     1     0     0     0     2     0     2     2     5     5    17\n 4 ALLEGANY .…     1     1     1     1     5    29    29    29    19    44   159\n 5 WASHINGTON…     1     1     4     1    14    31    39    70    70    95   326\n 6 FREDERICK …     3     1     2     6    11    49    49    65    52    56   294\n 7 BALTIMORE …    10    16    35   142   248   792  1118  1415  1395  1605  6776\n 8 BALTIMORE …     2     4    12    72   120   419   573   758   810   920  3690\n 9 BALTIMORE …     4     5    11    36    65   182   244   308   285   328  1468\n10 ANNE ARUND…     2     3     6    23    29    98   152   184   164   209   870\n# ℹ 20 more rows\n\n\nOk, now we have all the data. But we need actual headers. Let’s add those using rename(), keeping in mind that the new name comes first.\n\nfentanyl_deaths &lt;- read_csv(\"data/tabula-Annual_2020_Drug_Intox_Report.csv\", col_names = FALSE) |&gt; \n  clean_names() |&gt; \n  rename(jurisdiction = x1, deaths_2011 = x2, deaths_2012 = x3, deaths_2013 = x4, deaths_2014 = x5, deaths_2015 = x6, deaths_2016 = x7, deaths_2017 = x8, \n         deaths_2018 = x9, deaths_2019 = x10, deaths_2020 = x11, deaths_total = x12)\n\nRows: 30 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): X1\ndbl (5): X2, X3, X4, X5, X6\nnum (6): X7, X8, X9, X10, X11, X12\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nfentanyl_deaths\n\n# A tibble: 30 × 12\n   jurisdiction      deaths_2011 deaths_2012 deaths_2013 deaths_2014 deaths_2015\n   &lt;chr&gt;                   &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 MARYLAND .......…          26          29          58         186         340\n 2 NORTHWEST AREA .…           6           3           7           8          32\n 3 GARRETT ........…           1           0           0           0           2\n 4 ALLEGANY .......…           1           1           1           1           5\n 5 WASHINGTON .....…           1           1           4           1          14\n 6 FREDERICK ......…           3           1           2           6          11\n 7 BALTIMORE METRO …          10          16          35         142         248\n 8 BALTIMORE CITY .…           2           4          12          72         120\n 9 BALTIMORE COUNTY…           4           5          11          36          65\n10 ANNE ARUNDEL ...…           2           3           6          23          29\n# ℹ 20 more rows\n# ℹ 6 more variables: deaths_2016 &lt;dbl&gt;, deaths_2017 &lt;dbl&gt;, deaths_2018 &lt;dbl&gt;,\n#   deaths_2019 &lt;dbl&gt;, deaths_2020 &lt;dbl&gt;, deaths_total &lt;dbl&gt;\n\n\nWe could stop here, but there are a bunch of periods in the jurisdiction column and it’s better to remove them - it will make filtering easier. Let’s use str_replace_all() to do that:\n\nfentanyl_deaths &lt;- read_csv(\"data/tabula-Annual_2020_Drug_Intox_Report.csv\", col_names = FALSE) |&gt; \n  clean_names() |&gt; \n  rename(jurisdiction = x1, deaths_2011 = x2, deaths_2012 = x3, deaths_2013 = x4, deaths_2014 = x5, deaths_2015 = x6, deaths_2016 = x7, deaths_2017 = x8, \n         deaths_2018 = x9, deaths_2019 = x10, deaths_2020 = x11, deaths_total = x12) |&gt; \n  mutate(jurisdiction = str_squish(str_replace_all(jurisdiction,'\\\\.','')))\n\nRows: 30 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): X1\ndbl (5): X2, X3, X4, X5, X6\nnum (6): X7, X8, X9, X10, X11, X12\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nfentanyl_deaths\n\n# A tibble: 30 × 12\n   jurisdiction      deaths_2011 deaths_2012 deaths_2013 deaths_2014 deaths_2015\n   &lt;chr&gt;                   &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 MARYLAND                   26          29          58         186         340\n 2 NORTHWEST AREA              6           3           7           8          32\n 3 GARRETT                     1           0           0           0           2\n 4 ALLEGANY                    1           1           1           1           5\n 5 WASHINGTON                  1           1           4           1          14\n 6 FREDERICK                   3           1           2           6          11\n 7 BALTIMORE METRO …          10          16          35         142         248\n 8 BALTIMORE CITY              2           4          12          72         120\n 9 BALTIMORE COUNTY            4           5          11          36          65\n10 ANNE ARUNDEL                2           3           6          23          29\n# ℹ 20 more rows\n# ℹ 6 more variables: deaths_2016 &lt;dbl&gt;, deaths_2017 &lt;dbl&gt;, deaths_2018 &lt;dbl&gt;,\n#   deaths_2019 &lt;dbl&gt;, deaths_2020 &lt;dbl&gt;, deaths_total &lt;dbl&gt;\n\n\nThere are a few important things to explain here:\n\nBecause we’re replacing a literal period (.), we need to make sure that R knows that. Hence the ‘\\.’ Why? Because ‘.’ is a valid expression meaning “any character”, so if we didn’t have the backslashes the above code would make the entire column blank (try it!)\nThe str_squish function cleans up any excess spaces, at the beginning, middle or end of a character column. If we then use filter, we can do so with confidence.\nI put “deaths_” in front of each yearly column because R likes it when columns don’t begin with a number. You can have a column called 2011, but you literally have to use the backticks (2011) to refer to it in code.\n\nAll things considered, that was pretty easy. Many - most? - electronic PDFs aren’t so easy to parse. Sometimes you’ll need to open the exported CSV file and clean things up before importing into R. Other times you’ll be able to do that cleaning in R itself.\nHere’s the sad truth: THIS IS PRETTY GOOD. It sure beats typing it out. And since many government processes don’t change all that much, you can save the code to process subsequent versions of PDFs.",
    "crumbs": [
      "Cleaning Data Part IV: PDFs"
    ]
  },
  {
    "objectID": "r-basics.html",
    "href": "r-basics.html",
    "title": "R Basics",
    "section": "",
    "text": "R is a programming language, one specifically geared toward data analysis.\nLike all programming languages, it has certain built-in functions.\nThere are many ways you can write and execute R code. The first, and most basic, is the console, shown here as part of a software tool called RStudio (Desktop Open Source Edition) that we’ll be using all semester.\n\n\n\nready\n\n\nThink of the console like talking directly to the R language engine that’s busy working inside your computer. You use it send R commands, making sure to use the only language it understands, which is R. The R language engine processes those commands, and sends information back to you.\nUsing the console is direct, but it has some drawbacks and some quirks we’ll get into later. Let’s examine a basic example of how the console works.\nIf you load up R Studio, type 2+2 into the console and hit enter it will spit out the number 4, as displayed below.\n\n2+2\n\n[1] 4\n\n\nIt’s not very complex, and you knew the answer before hand, but you get the idea. With R, we can compute things.\nWe can also store things for later use under a specific name. In programming languages, these are called variables. We can assign things to variables using this left-facing arrow: &lt;-. The &lt;- is a called an assignment operator.\nIf you load up R studio and type this code in the console…\n\nnumber &lt;- 2\n\n…and then type this code, it will spit out the number 4, as show below.\n\nnumber * number\n\n[1] 4\n\n\nWe can have as many variables as we can name. We can even reuse them (but be careful you know you’re doing that or you’ll introduce errors).\nIf you load up R studio and type this code in the console…\n\nfirstnumber &lt;- 1\nsecondnumber &lt;- 2 \n\n…and then type this, it will split out the number 6, as shown below.\n\n(firstnumber + secondnumber) * secondnumber\n\n[1] 6\n\n\nWe can store anything in a variable. A whole table. A list of numbers. A single word. A whole book. All the books of the 18th century. Variables are really powerful. We’ll explore them at length.\nA quick note about the console: After this brief introduction, we won’t spend much time in R Studio actually writing code directly into the console. Instead, we’ll write code in fancied-up text files – interchangably called R Markdown or R Notebooks – as will be explained in the next chapter. But that code we write in those text files will still execute in the console, so it’s good to know how it works.\n\n\nThe real strength of any programming language is the external libraries (often called “packages”) that power it. The base language can do a lot, but it’s the external libraries that solve many specific problems – even making the base language easier to use.\nWith R, there are hundreds of free, useful libraries that make it easier to do data journalism, created by a community of thousands of R users in multiple fields who contribute to open-source coding projects.\nFor this class, we’ll make use of several external libraries.\nMost of them are part of a collection of libraries bundled into one “metapackage” called the Tidyverse that streamlines tasks like:\n\nLoading data into R. (We’ll use the readr Tidyverse library)\nCleaning and reshaping the data before analysis. (We’ll use the the tidyr and dplyr Tidyverse libraries)\nData analysis. (We’ll use the dplyr Tidyverse library)\nData visualization (We’ll use the ggplot2 Tidyverse library)\n\nTo install packages, we use the function install.packages().\nYou only need to install a library once, the first time you set up a new computer to do data journalism work. You never need to install it again, unless you want to update to a newer version of the package.\nTo install all of the Tidyverse libraries at once, the function is install.packages('tidyverse'). You can type it directly in the console.\nTo use the R Markdown files mentioned earlier, we also need to install a Tidyverse-related library that doesn’t load as part of the core Tidyverse package. The package is called, conveniently, rmarkdown. The code to install that is install.packages('rmarkdown')",
    "crumbs": [
      "R Basics"
    ]
  },
  {
    "objectID": "r-basics.html#about-libraries",
    "href": "r-basics.html#about-libraries",
    "title": "R Basics",
    "section": "",
    "text": "The real strength of any programming language is the external libraries (often called “packages”) that power it. The base language can do a lot, but it’s the external libraries that solve many specific problems – even making the base language easier to use.\nWith R, there are hundreds of free, useful libraries that make it easier to do data journalism, created by a community of thousands of R users in multiple fields who contribute to open-source coding projects.\nFor this class, we’ll make use of several external libraries.\nMost of them are part of a collection of libraries bundled into one “metapackage” called the Tidyverse that streamlines tasks like:\n\nLoading data into R. (We’ll use the readr Tidyverse library)\nCleaning and reshaping the data before analysis. (We’ll use the the tidyr and dplyr Tidyverse libraries)\nData analysis. (We’ll use the dplyr Tidyverse library)\nData visualization (We’ll use the ggplot2 Tidyverse library)\n\nTo install packages, we use the function install.packages().\nYou only need to install a library once, the first time you set up a new computer to do data journalism work. You never need to install it again, unless you want to update to a newer version of the package.\nTo install all of the Tidyverse libraries at once, the function is install.packages('tidyverse'). You can type it directly in the console.\nTo use the R Markdown files mentioned earlier, we also need to install a Tidyverse-related library that doesn’t load as part of the core Tidyverse package. The package is called, conveniently, rmarkdown. The code to install that is install.packages('rmarkdown')",
    "crumbs": [
      "R Basics"
    ]
  },
  {
    "objectID": "rvest.html",
    "href": "rvest.html",
    "title": "Scraping data with Rvest",
    "section": "",
    "text": "Scraping data with Rvest\nSometimes, governments put data online on a page or in a searchable database. And when you ask them for a copy of the data underneath the website, they say no.\nWhy? Because they have a website. That’s it. That’s their reason. They say they don’t have to give you the data because they’ve already given you the data, never mind that they haven’t given to you in a form you can actually load into R with ease.\nLucky for us, there’s a way for us to write code to get data even when an agency hasn’t made it easy: webscraping.\nOne of the most powerful tools you can learn as a data journalist is how to scrape data from the web. Scraping is the process of programming a computer to act like a human that opens a web browser, goes to a website, ingests the HTML from that website into R and turns it into data.\nThe degree of difficulty here goes from “Easy” to “So Hard You Want To Throw Your Laptop Out A Window.” And the curve between the two can be steep. You can learn how to scrape “Easy” in a day. The hard ones take a little more time, but it’s often well worth the effort because it lets you get stories you couldn’t get without it.\nIn this chapter, we’ll show you an easy one. And in the next chapter, we’ll so you a moderately harder one.\nLet’s start easy.\nWe’re going to use a library called rvest, which you can install it the same way we’ve done all installs: go to the console and install.packages(\"rvest\").\nLike so many R package names, rvest is a bad pun. You’re supposed to read it to sound like “harvest”, as in “harvesting” information from a website the same way you’d harvest crops in a field.\nWe’ll load these packages first:\n\nlibrary(rvest)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(janitor)\n\nFor this example, we’re going to work on loading a simple table of press releases from the Maryland State Courts.\nLet’s suppose we can’t find a table like that for download, but we do see a version on the website at this URL: https://www.courts.state.md.us/media/news.\n\n\n\n\n\n\n\n\n\nWe could get this table into R with the following manual steps: highlighting the text, copying it into Excel, saving it as a csv, and reading it into R. Or, we could write a few lines of webscraping code to have R do that for us!\nIn this simple example, it’s probably faster to do it manually than have R do it for us. But during the time when ballots are being counted, this table is likely to change, and we don’t want to keep doing manual repetitive tasks.\nWhy would we ever write code to grab a single table? There’s several reasons:\n\nOur methods are transparent. If a colleague wants to run our code from scratch to factcheck our work, they don’t need to repeat the manual steps, which are harder to document than writing code.\nLet’s suppose we wanted to grab the same table every day, to monitor for changes. Writing a script once, and pressing a single button every day is going to be much more efficient than doing this manually every day.\nIf we’re doing it manually, we’re more likely to make a mistake, like maybe failing to copy every row from the whole table.\nIt’s good practice to prepare us to do more complex scraping jobs. As we’ll see in the next chapter, if we ever want to grab the same table from hundreds of pages, writing code is much faster and easier than going to a hundred different pages ourselves and downloading data.\n\nSo, to scrape, the first thing we need to do is start with the URL. Let’s store it as an object called ag_url.\n\ncourts_url &lt;- \"https://www.courts.state.md.us/media/news\"\n\nWhen we go to the web page, we can see a nicely-designed page that contains our information.\nBut what we really care about, for our purposes, is the html code that creates that page.\nIn our web browser, if we right-click anywhere on the page and select “view source” from the popup menu, we can see the source code. Or you can just copy this into Google Chrome: view-source:https://www.courts.state.md.us/media/news.\nHere’s a picture of what some of the source code looks like.\n\n\n\n\n\n\n\n\n\nWe’ll use those HTML tags – things like &lt;table&gt; and &lt;tr&gt; – to grab the info we need.\nOkay, step 1.\nLet’s write a bit of code to tell R to go to the URL for the page and ingest all of that HTML code. In the code below, we’re starting with our URL and using the read_html() function from rvest to ingest all of the page html, storing it as an object called results.\n\n# read in the html\nresults &lt;- courts_url |&gt;\n  read_html()\n\n# display the html below\nresults\n\n{html_document}\n&lt;html lang=\"en\" dir=\"ltr\" prefix=\"content: http://purl.org/rss/1.0/modules/content/  dc: http://purl.org/dc/terms/  foaf: http://xmlns.com/foaf/0.1/  og: http://ogp.me/ns#  rdfs: http://www.w3.org/2000/01/rdf-schema#  schema: http://schema.org/  sioc: http://rdfs.org/sioc/ns#  sioct: http://rdfs.org/sioc/types#  skos: http://www.w3.org/2004/02/skos/core#  xsd: http://www.w3.org/2001/XMLSchema# \"&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] &lt;body class=\"path-media no-sidebars\"&gt;\\n        &lt;a href=\"#main-content\" cl ...\n\n\nIf you’re running this code in R Studio, in our environment window at right, you’ll see results as a “list of 2”.\nThis is not a dataframe, it’s a different type of data structure a “nested list.”\nIf we click on the name “results” in our environment window, we can see that it’s pulled in the html and shown us the general page structure. Nested within the &lt;html&gt; tag is the &lt;head&gt; and &lt;body&gt;, the two fundamental sections of most web pages. We’re going to pull information out of the &lt;body&gt; tag in a bit.\n\n\n\n\n\n\n\n\n\nNow, our task is to just pull out the section of the html that contains the information we need.\nBut which part do we need from that mess of html code? To figure that out, we can go back to the page in a web browser like chrome, and use built in developer tools to “inspect” the html code underlying the page.\nOn the page, find the data we want to grab and right click on the word “Date” in the column header of the table. That will bring up a dropdown menu. Select “Inspect”, which will pop up a window called the “element inspector” that shows us where different elements on the page are located, what html tags created those elements, and other info.\n\n\n\n\n\n\n\n\n\nThe table that we want is actually contained inside an html &lt;table&gt;. It has a &lt;tbody&gt; that contains one row &lt;tr&gt; per release.\nBecause it’s inside of a table, and not some other kind of element (like a &lt;div&gt;), rvest has a special function for easily extracting and converting html tables, called html_table(). This function extracts all the html tables on the page, but this page only has one so we’re good.\n\n# read in the html and extract all the tables\nresults &lt;- courts_url |&gt;\n  read_html() |&gt;\n  html_table()\n\n# show the dataframe\n\nresults\n\n[[1]]\n# A tibble: 22 × 2\n   Date               `News Release/News Item`                                  \n   &lt;chr&gt;              &lt;chr&gt;                                                     \n 1 October 3, 2025    Supreme Court of Maryland holds off-site oral arguments a…\n 2 October 1, 2025    Media Advisory: Supreme Court of Maryland to hold off-sit…\n 3 September 26, 2025 Media Advisory: Baltimore City District Court Mental Heal…\n 4 September 22, 2025 Media Advisory: Circuit Court for Baltimore City Adult Dr…\n 5 September 9, 2025  Baltimore City District courthouse, 501 E. Fayette St., c…\n 6 September 2, 2025  Maryland Judiciary celebrates the 20th Anniversary of the…\n 7 June 27, 2025      Judge Kenneth A. Talley named district administrative jud…\n 8 June 16, 2025      Media Advisory: Tara H. Jackson is the new Prince George’…\n 9 May 30, 2025       Ten Baltimore City Public Schools students awarded $20,50…\n10 May 12, 2025       Media Advisory: Circuit Court for Baltimore City’s Adult …\n# ℹ 12 more rows\n\n[[2]]\n# A tibble: 41 × 2\n   Date              `News Release/News Item`                                   \n   &lt;chr&gt;             &lt;chr&gt;                                                      \n 1 December 22, 2024 Maryland courts, court offices, and administrative offices…\n 2 December 20, 2024 Legionella Bacteria Detected in Baltimore City Circuit and…\n 3 December 11, 2024 Maryland Judiciary Honors Student Winners of 19th Annual C…\n 4 December 11, 2024 Media Advisory: District Court in Anne Arundel County offe…\n 5 December 10, 2024 Media Advisory: District Court in Prince George’s County o…\n 6 December 2, 2024  Media Advisory: Anne Arundel County District Court celebra…\n 7 December 2, 2024  Media Advisory: Maryland Judiciary announces winners of th…\n 8 November 20, 2024 Media Advisory: National Adoption Day celebrations hosted …\n 9 November 20, 2024 Judge Joseph M. Stanalonis named administrative judge for …\n10 November 20, 2024 Senior Judge Alan M. Wilner retires as chair of the  Stand…\n# ℹ 31 more rows\n\n[[3]]\n# A tibble: 37 × 2\n   Date              `News Release/News Item`                                   \n   &lt;chr&gt;             &lt;chr&gt;                                                      \n 1 December 28, 2023 Maryland Judiciary launches E-rent program statewide       \n 2 December 20, 2023 Student winners of the Maryland Judiciary's 18th Annual Co…\n 3 December 5, 2023  Media Advisory: Maryland Judiciary announces winners of th…\n 4 November 22, 2023 Retired Baltimore Ravens Wide Receiver Torrey Smith celebr…\n 5 November 15, 2023 Media Advisory: Circuit courts in Maryland host National A…\n 6 November 9, 2023  Media Advisory: Baltimore City District Court Veterans Tre…\n 7 November 7, 2023  Maryland Judiciary launches E-rent program in Prince Georg…\n 8 November 3, 2023  Supreme Court of Maryland holds oral arguments at Dr. Henr…\n 9 October 30, 2023  Media Advisory: Baltimore City District Court Re-Entry Pro…\n10 October 26, 2023  Maryland Judiciary issues statement after suspect in the m…\n# ℹ 27 more rows\n\n[[4]]\n# A tibble: 48 × 2\n   Date              `News Release/News Item`                                   \n   &lt;chr&gt;             &lt;chr&gt;                                                      \n 1 December 20, 2022 Media Advisory: Supreme Court of Maryland to hold open mee…\n 2 December 14, 2022 Voter-approved constitutional change renames high courts t…\n 3 December 7, 2022  Student winners of the Maryland Judiciary's 17th Annual Co…\n 4 December 7, 2022  Retired Baltimore Ravens Wide Receiver Jermaine Lewis cele…\n 5 December 6, 2022  Media Advisory: Judge DaNeeka Varner Cotton named administ…\n 6 December 2, 2022  Media Advisory: Winners to be announced for Maryland Judic…\n 7 November 22, 2022 Media Advisory: Baltimore City District Court Re-Entry Pro…\n 8 November 17, 2022 New La Plata Court Help Center opens in Charles County     \n 9 November 16, 2022 Media Advisory: Circuit courts in Maryland host in-person …\n10 November 10, 2022 Media Advisory: Baltimore City District Court Veterans Tre…\n# ℹ 38 more rows\n\n[[5]]\n# A tibble: 46 × 2\n   Date              `News Release/News Item`                                   \n   &lt;chr&gt;             &lt;chr&gt;                                                      \n 1 December 27, 2021 Maryland Judiciary reverts operations to Phase III in resp…\n 2 December 22, 2021 Small fire at the Circuit Court for Talbot County causes t…\n 3 December 15, 2021 Media Advisory: District Court in Anne Arundel County will…\n 4 December 10, 2021 New Rockville District Court Help Center is already making…\n 5 December 9, 2021  Chief Judge Joseph M. Getty congratulates student winners …\n 6 December 8, 2021  Media Advisory: Grand opening celebration of the District …\n 7 December 8, 2021  Media Advisory: Winners announced for Maryland Judiciary’s…\n 8 November 17, 2021 Media Advisory: For a second year, circuit courts in Maryl…\n 9 November 15, 2021 Media Advisory: Baltimore City Veterans Treatment Court ce…\n10 November 1, 2021  Maryland lawyers donate 1.03 million hours of legal servic…\n# ℹ 36 more rows\n\n[[6]]\n# A tibble: 67 × 2\n   Date              `News Release/News Item`                                   \n   &lt;chr&gt;             &lt;chr&gt;                                                      \n 1 December 22, 2020 The Maryland Judiciary extends Phase II operations through…\n 2 December 21, 2020 Maryland courts, court offices, and administrative offices…\n 3 November 24, 2020 Maryland Judiciary restricts operations to Phase II in res…\n 4 November 18, 2020 Media Advisory: Circuit courts in Maryland prepare for vir…\n 5 November 16, 2020 Media Advisory: Circuit Court for Montgomery County to hon…\n 6 November 13, 2020 Judge Ruth A. Jakubowski named circuit and county administ…\n 7 November 13, 2020 Judge Sherri D. Koch named District Administrative Judge f…\n 8 November 12, 2020 Maryland Judiciary restricts operations to Phase III in re…\n 9 November 5, 2020  Somerset County Circuit Court holds its first Adult Recove…\n10 November 5, 2020  Baltimore County Family Recovery Support Program celebrate…\n# ℹ 57 more rows\n\n[[7]]\n# A tibble: 53 × 2\n   Date              `News Release/News Item`                                   \n   &lt;chr&gt;             &lt;chr&gt;                                                      \n 1 December 20, 2019 Maryland courts, court offices, and administrative offices…\n 2 December 11, 2019 Circuit Court for Washington County reduces trial jury ser…\n 3 December 6, 2019  Maryland Judiciary hosts young winners of 14th Annual Conf…\n 4 December 5, 2019  Media Advisory: District Court to hold Schools in the Cour…\n 5 December 4, 2019  Media Advisory: Baltimore City District Court Adult Drug T…\n 6 December 2, 2019  Media Advisory: Anne Arundel County District Court to hono…\n 7 December 2, 2019  Media Advisory: Winners announced for Maryland Judiciary’s…\n 8 November 18, 2019 Towson District Courthouse scheduled to move to new Catons…\n 9 November 18, 2019 Media Advisory: Circuit courts in Maryland celebrate adopt…\n10 November 13, 2019 Media Advisory: Maryland’s highest court to welcome studen…\n# ℹ 43 more rows\n\n[[8]]\n# A tibble: 82 × 2\n   Date              `News Release/News Item`                                   \n   &lt;chr&gt;             &lt;chr&gt;                                                      \n 1 December 26, 2018 New court rules refine court processes regarding guardians…\n 2 December 20, 2018 Courts, court offices, and court administrative offices wi…\n 3 December 18, 2018 New Maryland Rules shift pro bono and IOLTA reporting for …\n 4 December 14, 2018 Scheduled website maintenance                              \n 5 December 12, 2018 Media Advisory: Anne Arundel County District Court to hono…\n 6 December 11, 2018 Maryland lawyers donate 1.16 million hours of legal servic…\n 7 December 10, 2018 Media Advisory: Maryland Judiciary hosts participants from…\n 8 December 4, 2018  Suzanne C. Johnson named clerk of the Maryland Court of Ap…\n 9 December 3, 2018  Media Advisory: Courts, court offices, and administrative …\n10 November 28, 2018 Media Advisory: District Court to hold Schools in the Cour…\n# ℹ 72 more rows\n\n[[9]]\n# A tibble: 77 × 2\n   Date              `News Release/News Item`                                   \n   &lt;chr&gt;             &lt;chr&gt;                                                      \n 1 December 21, 2017 New court rules regarding guardianships of vulnerable Mary…\n 2 December 21, 2017 Judiciary Honors Students for Promoting Peace Through Artw…\n 3 December 15, 2017 Baltimore City District Court Honors Re-Entry Project Grad…\n 4 December 14, 2017 Court of Appeals Hosts Youth Conference                    \n 5 December 8, 2017  Phase II of construction on Robert C. Murphy Courts of App…\n 6 December 5, 2017  Judicial Data Center Outage - Thursday, December 7, 2017   \n 7 December 5, 2017  Court of Appeals Hosts United States Naval Academy Midship…\n 8 November 30, 2017 Pretrial Pilot Program to Launch in Baltimore County       \n 9 November 29, 2017 Maryland Judiciary Hosts Oratorical Contest for Youths fro…\n10 November 28, 2017 Media Advisory: Maryland Judiciary Hosts Students Who Help…\n# ℹ 67 more rows\n\n[[10]]\n# A tibble: 59 × 2\n   Date              `News Release/News Item`                                   \n   &lt;chr&gt;             &lt;chr&gt;                                                      \n 1 December 16, 2016 Judges Mentor Students in Civics                           \n 2 December 15, 2016 Judiciary Honors Young Artists for Promoting Peace         \n 3 December 12, 2016 Maryland Judiciary Expands MDEC and Electronic Filing to L…\n 4 December 9, 2016  Media Advisory:  Maryland’s Chief Judge Congratulates Youn…\n 5 December 8, 2016  Judges to Mentor Washington County Students on Civics      \n 6 December 8, 2016  Judges Mentor Students on Civics                           \n 7 December 8, 2016  Marilyn Bentley Named Clerk of Baltimore City Circuit Court\n 8 December 1, 2016  New Email Scam Targets Attorneys                           \n 9 November 23, 2016 Media Advisory: Judges to Mentor Montgomery County Student…\n10 November 21, 2016 Judge Brett W. Wilson Named First Judicial Circuit Adminis…\n# ℹ 49 more rows\n\n[[11]]\n# A tibble: 64 × 2\n   Date              `News Release/News Item`                                   \n   &lt;chr&gt;             &lt;chr&gt;                                                      \n 1 December 28, 2015 Media Advisory:Pre-Trial Proceedings Begin for State of Ma…\n 2 December 22, 2015 Media Advisory: New Trial Date Scheduled for State of Mary…\n 3 December 21, 2015 Chief Judge Honors Young Artists for Peace                 \n 4 December 15, 2015 New Rules Concerning Briefs Filed in Maryland’s Appellate …\n 5 December 9, 2015  New Mobile App Improves Access to Justice                  \n 6 December 1, 2015  Media Advisory:Jury Selection Continues and Trial Begins f…\n 7 November 25, 2015 Media Advisory: Judges Bring Civics to Carroll County Stud…\n 8 November 23, 2015 Jury Selection for State of Maryland v. William Porter on …\n 9 November 20, 2015 Petition for Expungement of Records (CC-DC-CR-072) - Revis…\n10 November 19, 2015 Media Advisory:Pre-Trial Motions Hearing for State of Mary…\n# ℹ 54 more rows\n\n[[12]]\n# A tibble: 50 × 2\n   Date              `News Release/News Item`                                   \n   &lt;chr&gt;             &lt;chr&gt;                                                      \n 1 December 23, 2014 Judge Charles J. Peters to Head Baltimore Circuit Court’s …\n 2 December 9, 2014  Baltimore City District Court Celebrates Newest Drug Treat…\n 3 December 5, 2014  Media Advisory:District Court of  Maryland’s Towson Courth…\n 4 November 25, 2014 Judiciary Warns  Public to Beware of “Court” Scams         \n 5 November 24, 2014 Judges Help Bring Civics to Charles County High School Stu…\n 6 November 14, 2014 Young Artists Honored for Peace-making Efforts             \n 7 November 12, 2014 Media Advisory: Maryland Judiciary Hosts Youths from DJS F…\n 8 November 10, 2014 Media Advisory: Judiciary Holds Reception to Honor Young A…\n 9 November 7, 2014  Media Advisory: Anne Arundel County Circuit Court Address …\n10 November 7, 2014  Media Advisory: Circuit Courts in Maryland Celebrate “Happ…\n# ℹ 40 more rows\n\n[[13]]\n# A tibble: 26 × 2\n   Date              `News Release/News Item`                                   \n   &lt;chr&gt;             &lt;chr&gt;                                                      \n 1 December 30, 2013 Media Advisory: Judiciary Warns of False “Court Case” Emai…\n 2 December 27, 2013 Judge Althea M. Handy Heads Civil Docket for Baltimore Cit…\n 3 December 17, 2013 Media Advisory: Judiciary Helps Bring Civics to Baltimore …\n 4 December 12, 2013 Harford County Courts Visited by Maryland’s Chief Judge    \n 5 December 11, 2013 Cecil County Courts Welcome Maryland’s Chief  Judge        \n 6 December 4, 2013  Maryland’s New Chief Judge Visits Charles County Courts    \n 7 December 4, 2013  Maryland’s New Chief Judge Visits Prince George’s County D…\n 8 December 2, 2013  State Task Force Begins Study of Civil Right to Counsel fo…\n 9 November 18, 2013 Media Advisory: Maryland Judiciary Hosts Youths from DJS F…\n10 November 18, 2013 Media Advisory: Circuit Courts in Maryland Celebrate Natio…\n# ℹ 16 more rows\n\n[[14]]\n# A tibble: 1 × 2\n  Date             `News Release/News Item`                                     \n  &lt;chr&gt;            &lt;chr&gt;                                                        \n1 October 13, 0023 Media Advisory: Circuit Court for Baltimore City’s Adult Dru…\n\n\nIn the environment window at right, look at results Note that it’s now a “list of 11”.\nThis gets a little complicated, but what you’re seeing here is a nested list that contains one data frame – also called tibbles – one for each table that exists on the web page we scraped. There are tables for each year on the page, so we have 11 years’ worth of press releases.\nLet’s say we are interested in 2023, so all we need to do now is to store that first dataframe as an object. We can do that with this code, which says “keep only the first dataframe from our nested list.”\n\n# Read in all html from table, store all tables on page as nested list of dataframes.\nresults &lt;- courts_url |&gt;\n  read_html() |&gt;\n  html_table()\n\n# Just keep the first dataframe in our list\n\nresults &lt;- results[[1]]\n\n# show the dataframe\n\nresults\n\n# A tibble: 22 × 2\n   Date               `News Release/News Item`                                  \n   &lt;chr&gt;              &lt;chr&gt;                                                     \n 1 October 3, 2025    Supreme Court of Maryland holds off-site oral arguments a…\n 2 October 1, 2025    Media Advisory: Supreme Court of Maryland to hold off-sit…\n 3 September 26, 2025 Media Advisory: Baltimore City District Court Mental Heal…\n 4 September 22, 2025 Media Advisory: Circuit Court for Baltimore City Adult Dr…\n 5 September 9, 2025  Baltimore City District courthouse, 501 E. Fayette St., c…\n 6 September 2, 2025  Maryland Judiciary celebrates the 20th Anniversary of the…\n 7 June 27, 2025      Judge Kenneth A. Talley named district administrative jud…\n 8 June 16, 2025      Media Advisory: Tara H. Jackson is the new Prince George’…\n 9 May 30, 2025       Ten Baltimore City Public Schools students awarded $20,50…\n10 May 12, 2025       Media Advisory: Circuit Court for Baltimore City’s Adult …\n# ℹ 12 more rows\n\n\nWe now have a proper dataframe, albeit with some lengthy column headers.\nFrom here, we can do a little cleaning. First we’ll use clean_names() to lower the column names. Then use rename() to replace the title column with a simpler version and properly format the date.\n\n# Read in all html from table, get the HTML table.\nresults &lt;- courts_url |&gt;\n  read_html() |&gt;\n  html_table()\n\n# Standardize column headers, remove last row\n\nresults &lt;- results[[1]] |&gt;\n  clean_names() |&gt;\n  rename(title = news_release_news_item) |&gt;\n  mutate(date = mdy(date))\n\n# show the dataframe\nresults\n\n# A tibble: 22 × 2\n   date       title                                                             \n   &lt;date&gt;     &lt;chr&gt;                                                             \n 1 2025-10-03 Supreme Court of Maryland holds off-site oral arguments at Easton…\n 2 2025-10-01 Media Advisory: Supreme Court of Maryland to hold off-site oral a…\n 3 2025-09-26 Media Advisory: Baltimore City District Court Mental Health Court…\n 4 2025-09-22 Media Advisory: Circuit Court for Baltimore City Adult Drug Treat…\n 5 2025-09-09 Baltimore City District courthouse, 501 E. Fayette St., closes Oc…\n 6 2025-09-02 Maryland Judiciary celebrates the 20th Anniversary of the Annual …\n 7 2025-06-27 Judge Kenneth A. Talley named district administrative judge for t…\n 8 2025-06-16 Media Advisory: Tara H. Jackson is the new Prince George’s County…\n 9 2025-05-30 Ten Baltimore City Public Schools students awarded $20,500 in sch…\n10 2025-05-12 Media Advisory: Circuit Court for Baltimore City’s Adult Drug Tre…\n# ℹ 12 more rows\n\n\nAnd there we go. We now have a nice tidy dataframe of Maryland state court press releases. We could combine all 11 dataframes if we wanted to have the whole set.\nWhat about HTML that doesn’t have a table? Well, that’s where things get a bit more complicated, and rely on your ability to read HTML and identify particular elements of it.\nLet’s consider the list of press releases from the little-known but fascinating Office of the State Prosecutor: https://osp.maryland.gov/press-releases/. Let’s save that URL as a variable as we did above:\n\nosp_url &lt;- \"https://osp.maryland.gov/press-releases/\"\n\nThe next step is similar to before, too, except this time we’re not going to call html_table() because there are no tables here that we want. In order to find what we want, we’ll need to right-click on the first release date and choose “Inspect”.\nOne reason that we use HTML elements like ul and li is to help organize the code and make it easier to maintain. But it also helps with scraping, because we can zero in on the elements we want. Let’s start by reading it in:\n\nosp_results &lt;- osp_url |&gt;\n  read_html()\n\n# show the result\nosp_results\n\n{html_document}\n&lt;html lang=\"en-US\"&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] &lt;body&gt;&lt;div class=\"container\"&gt;\\n        &lt;div class=\"skipNav\"&gt;\\n            ...\n\n\nOnce again we get a list of two as our result. In this case, we don’t want to isolate the first item in our list. Instead, we want to try to locate all of the li tags inside this HTML. Since the li tag is an HTML element, we use the function html_elements() to get them. If we only wanted a single element, we’d use html_element():\n\nosp_results |&gt; html_elements('li')\n\n{xml_nodeset (91)}\n [1] &lt;li&gt;\\n&lt;a href=\"https://www.facebook.com/OSPMD/\" class=\"sm-facebook\" titl ...\n [2] &lt;li&gt;\\n&lt;a href=\"https://twitter.com/#!/@OSP_MD\" class=\"sm-twitter\" title= ...\n [3] &lt;li&gt;\\n&lt;a href=\"https://www.instagram.com/osp_md/\" class=\"sm-instagram2\"  ...\n [4] &lt;li&gt;&lt;a href=\"https://www.linkedin.com/company/office-of-the-maryland-sta ...\n [5] &lt;li id=\"menu-item-565\" class=\"menu-item menu-item-type-custom menu-item- ...\n [6] &lt;li id=\"menu-item-561\" class=\"menu-item menu-item-type-custom menu-item- ...\n [7] &lt;li id=\"menu-item-1954\" class=\"menu-item menu-item-type-post_type menu-i ...\n [8] &lt;li id=\"menu-item-150\" class=\"menu-item menu-item-type-custom menu-item- ...\n [9] &lt;li id=\"menu-item-1919\" class=\"menu-item menu-item-type-post_type menu-i ...\n[10] &lt;li id=\"menu-item-1382\" class=\"menu-item menu-item-type-post_type menu-i ...\n[11] &lt;li id=\"menu-item-564\" class=\"menu-item menu-item-type-post_type menu-it ...\n[12] &lt;li id=\"menu-item-1363\" class=\"menu-item menu-item-type-custom menu-item ...\n[13] &lt;li id=\"menu-item-566\" class=\"menu-item menu-item-type-custom menu-item- ...\n[14] &lt;li id=\"menu-item-1378\" class=\"menu-item menu-item-type-custom menu-item ...\n[15] &lt;li id=\"menu-item-1379\" class=\"menu-item menu-item-type-custom menu-item ...\n[16] &lt;li id=\"menu-item-1380\" class=\"menu-item menu-item-type-custom menu-item ...\n[17] &lt;li class=\"listing-item\"&gt;&lt;a class=\"title\" href=\"https://osp.maryland.gov ...\n[18] &lt;li class=\"listing-item\"&gt;&lt;a class=\"title\" href=\"https://osp.maryland.gov ...\n[19] &lt;li class=\"listing-item\"&gt;&lt;a class=\"title\" href=\"https://osp.maryland.gov ...\n[20] &lt;li class=\"listing-item\"&gt;&lt;a class=\"title\" href=\"https://osp.maryland.gov ...\n...\n\n\nGood news and bad news here: we have all of the\n\ntags, but we have lots of them, and it’s hard to see if we have the right ones. Let’s see what we’re dealing with and just look at the text inside the\n\ntags. To do that, we add html_text() to the end of the last code block:\n\nosp_results |&gt; html_elements('li') |&gt; html_text()\n\n [1] \"Facebook \"                                                                                                                                                                                                                                           \n [2] \"Twitter \"                                                                                                                                                                                                                                            \n [3] \"Instagram  \"                                                                                                                                                                                                                                         \n [4] \"LinkedIn\"                                                                                                                                                                                                                                            \n [5] \"Home\"                                                                                                                                                                                                                                                \n [6] \"Leadership\"                                                                                                                                                                                                                                          \n [7] \"FILE A COMPLAINT\"                                                                                                                                                                                                                                    \n [8] \"Press Releases\"                                                                                                                                                                                                                                      \n [9] \"Social Media\"                                                                                                                                                                                                                                        \n[10] \"Careers\"                                                                                                                                                                                                                                             \n[11] \"Contact Us\"                                                                                                                                                                                                                                          \n[12] \"OSP Annual Reports\"                                                                                                                                                                                                                                  \n[13] \"Maryland Judiciary Case Search\"                                                                                                                                                                                                                      \n[14] \"State Board of Elections\"                                                                                                                                                                                                                            \n[15] \"State Ethics Commission\"                                                                                                                                                                                                                             \n[16] \"Attorney Grievance Commission\"                                                                                                                                                                                                                       \n[17] \"September 15, 2025: Maryland Department of Health Police Captain Charged with Misconduct in Office and Theft\"                                                                                                                                        \n[18] \"July 28, 2025: Former Co-Owner of Mixed Martial Arts Studio Charged with Embezzlement, Identity Fraud, Theft & Other Crimes\"                                                                                                                         \n[19] \"August 20, 2025: Cecil County Sheriff’s Deputy Charged with Misconduct, Misuse of Law Enforcement Databases, and a Wiretap Violation\"                                                                                                                \n[20] \"July 16, 2025: Former Legislative Aide Sentenced for Stealing Legislative Scholarship Funds\"                                                                                                                                                         \n[21] \"March 25, 2025: Former Legislative Aide Pleads Guilty to Felony Theft for Stealing Legislative Scholarship Funds\"                                                                                                                                    \n[22] \"January 22, 2025: Former Legislative Aide Charged with Felony Theft and Other Charges for Stealing Legislative Scholarship Funds\"                                                                                                                    \n[23] \"December 13, 2024 – Orphan’s Court Judge Indicted for Illegal Recording and Misconduct in Office\"                                                                                                                                                    \n[24] \"November 13, 2024: Former Prince George’s County Council Member Jamel “Mel” Franklin Sentenced to Jail for Felony Theft and Perjury.\"                                                                                                                \n[25] \"November 1, 2024: Easton Police Department Officer Sentenced to Jail on Two Counts of Misconduct in Office\"                                                                                                                                          \n[26] \"September 11, 2024: Easton Police Department Officer Convicted of Two Charges of Misconduct in Office\"                                                                                                                                               \n[27] \"August 26, 2024: Former Prince George’s County Council Member Pleads Guilty to Felony Theft Scheme and Perjury\"                                                                                                                                      \n[28] \"July 18, 2024: Former Anne Arundel County Register of Wills Sentenced for Misconduct in Office\"                                                                                                                                                      \n[29] \"June 20, 2024: Prince George’s County Councilman Charged with Multiple Counts of Felony Theft Scheme, Embezzlement, and Perjury\"                                                                                                                     \n[30] \"June 4, 2024: Anne Arundel County Register of Wills Pleads Guilty to Misconduct in Office\"                                                                                                                                                           \n[31] \"April 4, 2024: Easton Police Department Officer Charged with Misconduct in Office\"                                                                                                                                                                   \n[32] \"April 3, 2024: John King for Governor Campaign Cited for Authority Line Violations\"                                                                                                                                                                  \n[33] \"January 26, 2024: Anne Arundel County Register of Wills Charged with Misconduct in Office, Misappropriation by a Fiduciary, and Theft\"                                                                                                               \n[34] \"January 10, 2024: Former Cecil County Sheriff’s Deputy Sentenced to Prison for Misconduct in Office and Visual Surveillance with Prurient Intent\"                                                                                                    \n[35] \"November 9, 2023: Baltimore County Man Pleads Guilty to Voting in a Maryland Election without U.S. Citizenship\"                                                                                                                                      \n[36] \"October 12, 2023: 1776 Project PAC Fined Over $20,000 for Failure to Include Authority Line in Carroll County School Board Race\"                                                                                                                     \n[37] \"October 11, 2023: Former Cecil County Sheriff’s Deputy Pleads Guilty to Misconduct in Office and Visual Surveillance with Prurient Intent\"                                                                                                           \n[38] \"October 05, 2023: Baltimore County Man Charged for Voting in a Maryland Election without U.S. Citizenship\"                                                                                                                                           \n[39] \"August 23, 2023: Cecil County Sheriff’s Office Deputy Charged with Misconduct in Office, Witness Retaliation, Revenge Porn, and Related Offenses\"                                                                                                    \n[40] \"July 31, 2023: Treasurer for Political Committees “Friends of Cathy Bevins” and the “Baltimore County Victory Slate” Sentenced to Six Months Incarceration\"                                                                                          \n[41] \"May 25, 2023: Treasurer for Political Committees “Friends of Cathy Bevins” and the “Baltimore County Victory Slate” Pleads Guilty to Felony Theft Scheme and Perjury\"                                                                                \n[42] \"April 18, 2023: Former Baltimore City State’s Attorney Sentenced to Two Years in Federal Prison for Unlawfully Obtaining Phone Records\"                                                                                                              \n[43] \"April 13, 2023: Calvert County Sheriff’s Deputy Pleads Guilty To Misconduct In Office\"                                                                                                                                                               \n[44] \"February 16, 2023: Treasurer to Political Committees “Friends of Cathy Bevins” and the “Baltimore County Victory Slate” Charged with Multiple Counts Of Felony Theft Scheme, Embezzlement, and Perjury\"                                              \n[45] \"January 13, 2023: Former Maryland State Delegate Richard K. Impallaria Pleads Guilty to Misconduct in Office for Misusing State Funds\"                                                                                                               \n[46] \"January 9, 2023: Calvert County Sheriff’s Deputy Charged with Misconduct in Office for Engaging in Sexual Relations with a Person Requesting Assistance\"                                                                                             \n[47] \"December 9, 2022: Former Baltimore City Assistant State’s Attorney Pleads Guilty to Federal Charges For Unlawfully Obtaining Phone Records\"                                                                                                          \n[48] \"September 30, 2022: Former Baltimore City Assistant State’s Attorney Facing Federal Charges for Unlawfully Obtaining Phone Records\"                                                                                                                  \n[49] \"September 29, 2022: Treasurer of Political Committee “Team 30 Slate” Pleads Guilty to Theft\"                                                                                                                                                         \n[50] \"July 27, 2022: Delegate Richard Impallaria charged with Felony Theft and related crimes for schemes related to misuse of state funds\"                                                                                                                \n[51] \"May 4, 2022: Treasurer of Political Committee “Team 30 Slate” Charged with Theft of Campaign Funds\"                                                                                                                                                  \n[52] \"April 18, 2022: Andrew Bradshaw, Former Mayor of Cambridge, MD, Pleads Guilty to Five Counts of Distributing Revenge Porn\"                                                                                                                           \n[53] \"November 30, 2021: Former Baltimore City Homicide Prosecutor Indicted on 88 Charges, including Stalking, Extortion, and Misconduct in Office\"                                                                                                        \n[54] \"November 23, 2021:  Fairmount Heights Police Officer Charged with Kidnapping, Perjury and Misconduct in Office\"                                                                                                                                      \n[55] \"November 15, 2021: Mayor of Cambridge Charged with violating Maryland’s Revenge Porn Statute\"                                                                                                                                                        \n[56] \"October 26, 2021: Non-Profit CEO Sentenced to Two Years in Federal Prison after Pleading Guilty to Wire Fraud in Connection with the Misuse of Federal Funds Intended For The Treatment of Survivors of Domestic Violence and Sexual Assault\"        \n[57] \"October 5, 2021: Former Executive Director Of Maryland Environmental Service Facing Federal And State Charges For Allegedly Fraudulently Obtaining More Than $276,731 From His Employer, And State Felony Violations Of The Maryland Wiretap Statute\"\n[58] \"September 23, 2021: Two Additional Baltimore Correctional Employees Plead Guilty, Making a Total of Eight Correctional Employees Convicted of Charges Related to Theft of over $400,000 Dollars of State Funds.\"                                     \n[59] \"August 20, 2021: Ivan Gonzalez, Former Baltimore Police Officer and Candidate for Mayor of Baltimore City, Pleads Guilty to Perjury\"                                                                                                                 \n[60] \"August 11, 2021: Baltimore Correctional Officer Pleads Guilty to Charges of Felony Theft and Misconduct in Office\"                                                                                                                                   \n[61] \"August 9, 2021: Edward M. Estes, former Mayor of the City of Glenarden, Pleads Guilty to Misconduct in Office\"                                                                                                                                       \n[62] \"July 30, 2021:  Lora Walters, former Deputy Director of the Cecil County Board of Elections, Pleads Guilty to Misconduct in Office\"                                                                                                                  \n[63] \"July 29, 2021: Baltimore Correctional Officers Plead Guilty to Charges of Felony Theft and Misconduct in Office\"                                                                                                                                     \n[64] \"July 26, 2021: Baltimore City Employee Pleads Guilty to Perjury\"                                                                                                                                                                                     \n[65] \"July 9, 2021: Correctional Officer at Metropolitan Transition Center in Baltimore Pleads Guilty to Charges of Felony Theft and Misconduct in Office\"                                                                                                 \n[66] \"June 10, 2021:  Five Correctional Officers charged with Felony Theft and Misconduct in Office\"                                                                                                                                                       \n[67] \"June 4, 2021:  Lora Walters, former Deputy Director of the Cecil County Board of Elections, charged with Misconduct in Office, Perjury, and other offenses.\"                                                                                         \n[68] \"March 29, 2021: Ivan Gonzalez, Baltimore Police Officer and Candidate for Mayor of Baltimore City, charged with Perjury and other crimes related to his 2020 Mayoral Campaign\"                                                                       \n[69] \"March 11, 2021: New Bank Fraud and Aggravated Identity Theft Charges Added to Non-Profit CEO’S Previous Federal Indictment Charging Her with Wire Fraud, Bank Fraud, and Aggravated Identity Theft\"                                                  \n[70] \"January 15, 2021: Correctional Officers and State Employee Charged with Theft and Bribery\"                                                                                                                                                           \n[71] \"January 13, 2021: Baltimore City Employee Charged with Perjury\"                                                                                                                                                                                      \n[72] \"November 17, 2020: Edward M. Estes, Mayor of Glen Arden, Maryland, Charged with Violating the Maryland Wiretap Statute and Misconduct in Office\"                                                                                                     \n[73] \"June 19, 2020: Former Baltimore Mayor Catherine Pugh Pleads Guilty To Perjury\"                                                                                                                                                                       \n[74] \"March 17, 2020: Level II – Flexible Operations of the Pandemic Flu and Other Infectious Diseases Attendance and Leave Policy\"                                                                                                                        \n[75] \"February 13, 2020: Devan Martin,  OSP Case #20-5785\"                                                                                                                                                                                                 \n[76] \"January 17, 2020: Greensboro Police Chief Pleads Guilty to Misconduct in Office\"                                                                                                                                                                     \n[77] \"January 6, 2020: Charles Blomquist and Sarah David Named Deputy State Prosecutors\"                                                                                                                                                                   \n[78] \"December 18, 2019: Ex-Baltimore Mayor Catherine Pugh Charged with Perjury\"                                                                                                                                                                           \n[79] \"December 10, 2019: City of District Heights Mayor Sentenced for Misconduct in Office\"                                                                                                                                                                \n[80] \"December 5, 2019: Former Baltimore City School Business Manager Pleads Guilty to Theft of School Funds\"                                                                                                                                              \n[81] \"November 25, 2019: Charlton T. Howard III Sworn in as State Prosecutor\"                                                                                                                                                                              \n[82] \"November 20, 2019: City of District Heights Mayor Convicted of Misconduct in Office\"                                                                                                                                                                 \n[83] \"November 19, 2019: Maryland State Police Sergeant Charged with Misconduct in Office\"                                                                                                                                                                 \n[84] \"November 14, 2019: Greensboro Police Chief Charged with Misconduct in Office\"                                                                                                                                                                        \n[85] \"November 7, 2019: Former Baltimore City School Business Manager Charged with Theft of School Funds\"                                                                                                                                                  \n[86] \"Contact Us\"                                                                                                                                                                                                                                          \n[87] \"Report Fraud\"                                                                                                                                                                                                                                        \n[88] \"Facebook \"                                                                                                                                                                                                                                           \n[89] \"Twitter \"                                                                                                                                                                                                                                            \n[90] \"Instagram  \"                                                                                                                                                                                                                                         \n[91] \"LinkedIn\"                                                                                                                                                                                                                                            \n\n\nOk, so now we can see the press releases. But they are surrounded by a bunch of things we don’t want, and this still isn’t a dataframe. Let’s solve the second problem first with the as_tibble() function:\n\nreleases &lt;- osp_results |&gt; html_elements('li') |&gt; html_text() |&gt; as_tibble()\n\nNow we’re getting closer. The press releases have a specific HTML class tied to the\n\ntag - in this case “listing-item”. We can isolate those:\n\nreleases &lt;- osp_results |&gt; html_elements('li.listing-item') |&gt; html_text() |&gt; as_tibble()\n\nOk, this is better, but it’s still not great. We have at least two issues:\n\nThe column is called value\nThe column has two kinds of information in it: a date and the title.\n\nLet’s solve both of those using the separate() function, which does what you think it does and gives us a chance to rename the columns:\n\nreleases &lt;- osp_results |&gt; html_elements('li.listing-item') |&gt; html_text() |&gt; as_tibble() |&gt; separate(value, c('date', 'title'), sep=\":\")\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 1 rows [7].\n\n\nThat worked, but it also exposed a couple of things, as that Warning message indicates: not all of the rows had a colon in them and the title column has a leading space. Let’s fix both of those - we’ll drop the rows where the title is NA and convert the dates into actual dates.\n\nreleases &lt;- osp_results |&gt; html_elements('li.listing-item') |&gt; html_text() |&gt; as_tibble() |&gt; separate(value, c('date', 'title'), sep=\":\") |&gt; drop_na(title) |&gt; mutate(title = str_squish(title), date = mdy(date))\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 1 rows [7].\n\n\nAnd now you have the data.",
    "crumbs": [
      "Scraping data with Rvest"
    ]
  },
  {
    "objectID": "start-math.html",
    "href": "start-math.html",
    "title": "Newsroom math",
    "section": "",
    "text": "Statistics are people with the tears washed off\n- Paul Brodeur\n\nJo Craven McGinty, then of The New York Times, used simple rates and ratios to discover that a 6-story brick New Jersey hospital was the most expensive in the nation. In 2012, Bayonne Medical Center “charged the highest amounts in the country for nearly one-quarter of the most common hospital treatments,” the Times story said.\nTo do this story, McGinty only needed to know the number of the procedures reported to the government and the total amount each hospital charged. Dividing those to find an average price, then ranking the most common procedures, led to this surprising result.\n\n\nUsing averages, percentages and percent change is the bread and butter of data journalism, leading to stories ranging from home price comparisons to school reports and crime trends. It may have been charming at one time for reporters to announce that they didn’t “do” math, but no longer. Instead, it is now an announcement that the reporter can only do some of the job. You will never be able to tackle complicated, in-depth stories without reviewing basic math.\nThe good news is that most of the math and statistics you need in a newsroom isn’t nearly as difficult as high school algebra. You learned it somewhere around the 4th grade. You then had a decade to forget it before deciding you didn’t like math. But mastering this most basic arithmetic again is a requirement in the modern age.\nIn working with typical newsroom math, you will need to learn how to:\n\nOvercome your fear of numbers\nIntegrate numbers into your reporting\nRoutinely compute averages, differences and rates\nSimplify and select the right numbers for your story\n\nWhile this chapter covers general tips, you can find specific instructions for typical newsroom math in this Appendix A\n\n\n\nWhen we learned to read, we got used to the idea that 26 letters in American English could be assembled into units that we understand without thinking – words, sentences, paragraphs and books. We never got the same comfort level with 10 digits, and neither did our audience.\nThink of your own reaction to seeing a page of words. Now imagine it as a page of numbers.\nInstead, picture the number “five”. It’s easy. It might be fingers or it might be a team on a basketball court. But it’s simple to understand.\nNow picture the number 275 million. It’s hard. Unfortunately, 275 billion isn’t much harder, even though it’s magnitudes larger. (A million seconds goes by in about 11 days but you may not have been alive for a billion seconds – about 36 years.)\nThe easiest way to get used to some numbers is to learn ways to cut them down to size by calculating rates, ratios or percentages. In your analysis, keep an eye out for the simplest accurate way to characterize the numbers you want to use. “Characterize” is the important word here – it’s not usually necessary to be overly precise so long as your story doesn’t hinge on a nuanced reading of small differences. (And is anything that depends on that news? It may not be.)\nHere’s one example of putting huge numbers in perspective. Pay attention to what you really can picture - it’s probably the $21 equivalent.\n\nThe Chicago hedge fund billionaire Kenneth C. Griffin, for example, earns about $68.5 million a month after taxes, according to court filings made by his wife in their divorce. He has given a total of $300,000 to groups backing Republican presidential candidates. That is a huge sum on its face, yet is the equivalent of only $21.17 for a typical American household, according to Congressional Budget Office data on after-tax income.  “Buying Power”, Nicholas Confessore, Sarah Cohen and Karen Yourish, The New York Times, October 2015\n\nOriginally the reporters had written it even more simply, but editors found the facts so unbelievable that they wanted give readers a chance to do the math themselves. That’s reasonable, but here’s an even simpler way to say it: “earned nearly $1 billion after taxes…He has given $300,000 to groups backing candidates, the equivalent of a dinner at Olive Garden for the typical American family , based on Congressional Budget Office income data.” (And yes, the reporter checked the price for an Olive Garden meal at the time for four people.)\n\n\n\nFor journalists, numbers – or facts – make up the third leg of a stool supported by human stories or anecdotes, and insightful comment from experts. They serve us in three ways:\n\nAs summaries. Almost by definition, a number counts something, averages something, or otherwise summarizes something. Sometimes, it does a good job, as in the average height of Americans. Sometimes it does a terrible job, as in the average income of Americans. Try to find summaries that accurately characterize the real world.\nAs opinions. Sometimes it’s an opinion derived after years of impartial study. Sometimes it’s an opinion tinged with partisan or selective choices of facts. Use them accordingly.\nAs guesses. Sometimes it’s a good guess, sometimes it’s an off-the-cuff guess. And sometimes it’s a hopeful guess. Even when everything is presumably counted many times, it’s still a (very nearly accurate) guess. Yes, the “audits” of presidential election results in several states in 2021 found a handful of errors – not a meaningful number, but a few just the same.\n\nOnce you find the humanity in your numbers, by cutting them down to size and relegating them to their proper role, you’ll find yourself less fearful. You’ll be able to characterize what you’ve learned rather than numb your readers with every number in your notebook. You may even find that finding facts on your own is fun.\n\n\n\n\n\n\nSteve Doig’s “Math Crib Sheet”\nAppendix A: Common newsroom math, adapted from drafts of the book Numbers in the Newsroom, by Sarah Cohen.\n\n\n\n\n\n“Avoiding Numeric Novocain: Writing Well with Numbers,” by Chip Scanlan, Poynter.com\nT. Christian Miller’s “Writing the data-driven story”\nA viral Twitter thread:\n\nWhat happens in your head when you do 27+48?--- Gene Belcher (@Wparks91) June 25, 2019\n\n\n\n\n\n\nImagine that someone gave you $1 million and you could spend it on anything you want. Write down a list of things that would add up to about that amount. That should be easy. Now, imagine someone gave you $1 billion and you could spend it on whatever you want, but anything left over after a year had to be returned. How would you spend it? (You can give away money, but it can’t be more than 50% of a charity’s annual revenues. So you can’t give 10 $100 million gifts!) See how far you get trying to spend it. A few homes, a few yachts, student loan repayments for all of your friends? You’ve hardly gotten started.",
    "crumbs": [
      "Newsroom math"
    ]
  },
  {
    "objectID": "start-math.html#why-numbers",
    "href": "start-math.html#why-numbers",
    "title": "Newsroom math",
    "section": "",
    "text": "Using averages, percentages and percent change is the bread and butter of data journalism, leading to stories ranging from home price comparisons to school reports and crime trends. It may have been charming at one time for reporters to announce that they didn’t “do” math, but no longer. Instead, it is now an announcement that the reporter can only do some of the job. You will never be able to tackle complicated, in-depth stories without reviewing basic math.\nThe good news is that most of the math and statistics you need in a newsroom isn’t nearly as difficult as high school algebra. You learned it somewhere around the 4th grade. You then had a decade to forget it before deciding you didn’t like math. But mastering this most basic arithmetic again is a requirement in the modern age.\nIn working with typical newsroom math, you will need to learn how to:\n\nOvercome your fear of numbers\nIntegrate numbers into your reporting\nRoutinely compute averages, differences and rates\nSimplify and select the right numbers for your story\n\nWhile this chapter covers general tips, you can find specific instructions for typical newsroom math in this Appendix A",
    "crumbs": [
      "Newsroom math"
    ]
  },
  {
    "objectID": "start-math.html#overcoming-your-fear-of-math",
    "href": "start-math.html#overcoming-your-fear-of-math",
    "title": "Newsroom math",
    "section": "",
    "text": "When we learned to read, we got used to the idea that 26 letters in American English could be assembled into units that we understand without thinking – words, sentences, paragraphs and books. We never got the same comfort level with 10 digits, and neither did our audience.\nThink of your own reaction to seeing a page of words. Now imagine it as a page of numbers.\nInstead, picture the number “five”. It’s easy. It might be fingers or it might be a team on a basketball court. But it’s simple to understand.\nNow picture the number 275 million. It’s hard. Unfortunately, 275 billion isn’t much harder, even though it’s magnitudes larger. (A million seconds goes by in about 11 days but you may not have been alive for a billion seconds – about 36 years.)\nThe easiest way to get used to some numbers is to learn ways to cut them down to size by calculating rates, ratios or percentages. In your analysis, keep an eye out for the simplest accurate way to characterize the numbers you want to use. “Characterize” is the important word here – it’s not usually necessary to be overly precise so long as your story doesn’t hinge on a nuanced reading of small differences. (And is anything that depends on that news? It may not be.)\nHere’s one example of putting huge numbers in perspective. Pay attention to what you really can picture - it’s probably the $21 equivalent.\n\nThe Chicago hedge fund billionaire Kenneth C. Griffin, for example, earns about $68.5 million a month after taxes, according to court filings made by his wife in their divorce. He has given a total of $300,000 to groups backing Republican presidential candidates. That is a huge sum on its face, yet is the equivalent of only $21.17 for a typical American household, according to Congressional Budget Office data on after-tax income.  “Buying Power”, Nicholas Confessore, Sarah Cohen and Karen Yourish, The New York Times, October 2015\n\nOriginally the reporters had written it even more simply, but editors found the facts so unbelievable that they wanted give readers a chance to do the math themselves. That’s reasonable, but here’s an even simpler way to say it: “earned nearly $1 billion after taxes…He has given $300,000 to groups backing candidates, the equivalent of a dinner at Olive Garden for the typical American family , based on Congressional Budget Office income data.” (And yes, the reporter checked the price for an Olive Garden meal at the time for four people.)",
    "crumbs": [
      "Newsroom math"
    ]
  },
  {
    "objectID": "start-math.html#put-math-in-its-place",
    "href": "start-math.html#put-math-in-its-place",
    "title": "Newsroom math",
    "section": "",
    "text": "For journalists, numbers – or facts – make up the third leg of a stool supported by human stories or anecdotes, and insightful comment from experts. They serve us in three ways:\n\nAs summaries. Almost by definition, a number counts something, averages something, or otherwise summarizes something. Sometimes, it does a good job, as in the average height of Americans. Sometimes it does a terrible job, as in the average income of Americans. Try to find summaries that accurately characterize the real world.\nAs opinions. Sometimes it’s an opinion derived after years of impartial study. Sometimes it’s an opinion tinged with partisan or selective choices of facts. Use them accordingly.\nAs guesses. Sometimes it’s a good guess, sometimes it’s an off-the-cuff guess. And sometimes it’s a hopeful guess. Even when everything is presumably counted many times, it’s still a (very nearly accurate) guess. Yes, the “audits” of presidential election results in several states in 2021 found a handful of errors – not a meaningful number, but a few just the same.\n\nOnce you find the humanity in your numbers, by cutting them down to size and relegating them to their proper role, you’ll find yourself less fearful. You’ll be able to characterize what you’ve learned rather than numb your readers with every number in your notebook. You may even find that finding facts on your own is fun.",
    "crumbs": [
      "Newsroom math"
    ]
  },
  {
    "objectID": "start-math.html#going-further",
    "href": "start-math.html#going-further",
    "title": "Newsroom math",
    "section": "",
    "text": "Steve Doig’s “Math Crib Sheet”\nAppendix A: Common newsroom math, adapted from drafts of the book Numbers in the Newsroom, by Sarah Cohen.\n\n\n\n\n\n“Avoiding Numeric Novocain: Writing Well with Numbers,” by Chip Scanlan, Poynter.com\nT. Christian Miller’s “Writing the data-driven story”\nA viral Twitter thread:\n\nWhat happens in your head when you do 27+48?--- Gene Belcher (@Wparks91) June 25, 2019",
    "crumbs": [
      "Newsroom math"
    ]
  },
  {
    "objectID": "start-math.html#exercises",
    "href": "start-math.html#exercises",
    "title": "Newsroom math",
    "section": "",
    "text": "Imagine that someone gave you $1 million and you could spend it on anything you want. Write down a list of things that would add up to about that amount. That should be easy. Now, imagine someone gave you $1 billion and you could spend it on whatever you want, but anything left over after a year had to be returned. How would you spend it? (You can give away money, but it can’t be more than 50% of a charity’s annual revenues. So you can’t give 10 $100 million gifts!) See how far you get trying to spend it. A few homes, a few yachts, student loan repayments for all of your friends? You’ve hardly gotten started.",
    "crumbs": [
      "Newsroom math"
    ]
  },
  {
    "objectID": "visualizing-for-publication.html",
    "href": "visualizing-for-publication.html",
    "title": "Visualizing your data for publication",
    "section": "",
    "text": "Doing data visualization well, and at professional level, takes time, skill and practice to perfect. Understanding it and doing it at a complex level is an entire class on it’s own. It uses some of the same skills here – grouping, filtering, calculating – but then takes that data and turns it into data pictures.\nBut simple stuff – and even some slightly complicated stuff – can be done with tools made for people who aren’t data viz pros.\nThe tool we’re going to use is called Datawrapper.\nFirst, let’s get some data and work with it. Let’s use a simple CSV of total votes cast for four Maryland Republicans who ran for statewide office in the 2022 general election. Let’s look at it.\n\nlibrary(tidyverse)\n\n\nmd_gop_cands &lt;- read_csv(\"data/md_gop_cands.csv\")\n\n\nhead(md_gop_cands)\n\n# A tibble: 4 × 5\n  Candidate        `Early Voting` `Election Day`  Mail  Votes\n  &lt;chr&gt;                     &lt;dbl&gt;          &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 Michael Peroutka         131551         451685 24669 606557\n2 Dan Cox                  123287         424837 22112 569450\n3 Barry Glassman           142726         484892 28656 655379\n4 Chris Chaffee            130458         447330 23561 600543\n\n\n\n\nMaking charts in Datawrapper is preposterously simple, which is the point. There are dozens of chart types, and dozens of options. To get from a csv to a chart to publication is very, very easy.\nFirst, go to datawrapper.de and sign up for an account. It’s free.\nOnce logged in, you’ll click on New Chart.\n\n\n\n\n\n\n\n\n\nThe first thing we’ll do is upload our CSV. Click on XLS/CSV and upload the file.\n\n\n\n\n\n\n\n\n\nNext up is to check and see what Datawrappper did with our data when we uploaded it. As you can see from the text on the left, if it’s blue, it’s a number. If it’s green, it’s a date. If it’s black, it’s text. Red means there’s a problem. This data is very clean, so it imports cleanly. Click on the “Proceed” button.\n\n\n\n\n\n\n\n\n\nNow we make a chart. Bar chart comes up by default, which is good, because with totals, that’s what we have.\nClick on Refine. The first option we want to change is column we’re using for the bars, which defaults to “Early Voting”. Let’s make it “Votes. Let’s also choose to sort the bars so that the largest value (and bar appears first). We do that by clicking on the”Sort bars” button.\n\n\n\n\n\n\n\n\n\nNow we need to annotate our charts. Every chart needs a title, a source line and a credit line. Most need chatter (called description here). Click on the “Annotate” tab to get started.\nReally think about the title and description: the title is like a headline and the description is provides some additional context. Another way to think about it: the title is the most important lesson from the graphic, and the description could be the next most important lesson or could provide more context to the title.\n\n\n\n\n\n\n\n\n\nTo publish, we click the “Publish & Embed” tab. Some publication systems allow for the embedding of HTML into a post or a story. Some don’t. The only way to know is to ask someone at your publication. Every publication system on the planet, though, can publish an image. So there’s always a way to export your chart as a PNG file, which you can upload like any photo.\n\n\n\n\n\n\n\n\n\n\n\nLet’s create a choropleth map - one that shows variations between the percentage of votes received by Wes Moore across Maryland counties. We’ll read that in from the data folder.\n\nmd_gov_county &lt;- read_csv(\"data/md_gov_county.csv\")\n\nIn order to make a map, we need to be able to tell Datawrapper that a certain column contains geographic information (besides the name of the county). The easiest way to do that for U.S. maps is to use something called a FIPS Code. You should read about them so you understand what they are, and think of them as a unique identifier for some geographical entity like a state or county. Our md_gov_county dataframe has a FIPS code for each county, but if you ever need one for a county, this is a solved problem thanks to the Tigris library that we used in pre_lab 9.\nWe’ll need to write code to add columns showing the total number of votes for each county and the percentage of votes received by Wes Moore in each county, then replace the CSV file in the data folder with it\n\nmd_gov_county &lt;- md_gov_county |&gt; \n  mutate(total = cox + moore + lashar + wallace + harding + write_ins) |&gt; \n  mutate(pct_moore = moore/total * 100)\n\nwrite_csv(md_gov_county, \"data/md_gov_county_with_percent.csv\")\n\nGo back to Datawrapper and click on “New Map”. Click on “Choropleth map” and then choose “USA &gt;&gt; Counties (2022)” for the map base and click the Proceed button.\nNow we can upload the md_gov_county_with_percent.csv file we just saved using the Upload File button. It should look like the following image:\n\n\n\n\n\n\n\n\n\nWe’ll need to make sure that Datawrapper understands what the data is and where the FIPS code is. Click on the “Match” tab and make sure that yours looks like the image below:\n\n\n\n\n\n\n\n\n\nClick the “Proceed” button (you should have to click it twice, since the first time it will tell you that there’s no data for 3,197 counties - the rest of the U.S.). That will take you to the Visualize tab.\nYou’ll see that the map currently is of the whole nation, and we only have Maryland data. Let’s fix that.\nLook for “Hide regions without data” under Appearance, and click the slider icon to enable that feature. You should see a map zoomed into Maryland with some counties in various colors.\nBut it’s a little rough visually, so let’s clean that up.\nLook for the “Show color legend” label and add a caption for the legend, which is the horizontal bar under the title. Then click on the “Annotate” tab to add a title, description, data source and byline. The title should represent the headline, while the description should be a longer phrase that tells people what they are looking at.\nThat’s better, but check out the tooltip by hovering over a county. It’s not super helpful. Let’s change the tooltip behavior to show the county name and a better-formatted number.\nClick the “Customize tooltips” button so it expands down. Change {{ fips_code }} to {{ county }} and {{ pctmoore }} to {{ FORMAT(pctmoore, “00.0%”)}}\nExperiment with the “Show labels” options to see if you can add county labels to your map.\nOk, that looks better. Let’s publish!\nClick the “Proceed” button until you get to the “Publish & Embed” tab, then click “Publish Now”.",
    "crumbs": [
      "Visualizing your data for publication"
    ]
  },
  {
    "objectID": "visualizing-for-publication.html#datawrapper",
    "href": "visualizing-for-publication.html#datawrapper",
    "title": "Visualizing your data for publication",
    "section": "",
    "text": "Making charts in Datawrapper is preposterously simple, which is the point. There are dozens of chart types, and dozens of options. To get from a csv to a chart to publication is very, very easy.\nFirst, go to datawrapper.de and sign up for an account. It’s free.\nOnce logged in, you’ll click on New Chart.\n\n\n\n\n\n\n\n\n\nThe first thing we’ll do is upload our CSV. Click on XLS/CSV and upload the file.\n\n\n\n\n\n\n\n\n\nNext up is to check and see what Datawrappper did with our data when we uploaded it. As you can see from the text on the left, if it’s blue, it’s a number. If it’s green, it’s a date. If it’s black, it’s text. Red means there’s a problem. This data is very clean, so it imports cleanly. Click on the “Proceed” button.\n\n\n\n\n\n\n\n\n\nNow we make a chart. Bar chart comes up by default, which is good, because with totals, that’s what we have.\nClick on Refine. The first option we want to change is column we’re using for the bars, which defaults to “Early Voting”. Let’s make it “Votes. Let’s also choose to sort the bars so that the largest value (and bar appears first). We do that by clicking on the”Sort bars” button.\n\n\n\n\n\n\n\n\n\nNow we need to annotate our charts. Every chart needs a title, a source line and a credit line. Most need chatter (called description here). Click on the “Annotate” tab to get started.\nReally think about the title and description: the title is like a headline and the description is provides some additional context. Another way to think about it: the title is the most important lesson from the graphic, and the description could be the next most important lesson or could provide more context to the title.\n\n\n\n\n\n\n\n\n\nTo publish, we click the “Publish & Embed” tab. Some publication systems allow for the embedding of HTML into a post or a story. Some don’t. The only way to know is to ask someone at your publication. Every publication system on the planet, though, can publish an image. So there’s always a way to export your chart as a PNG file, which you can upload like any photo.\n\n\n\n\n\n\n\n\n\n\n\nLet’s create a choropleth map - one that shows variations between the percentage of votes received by Wes Moore across Maryland counties. We’ll read that in from the data folder.\n\nmd_gov_county &lt;- read_csv(\"data/md_gov_county.csv\")\n\nIn order to make a map, we need to be able to tell Datawrapper that a certain column contains geographic information (besides the name of the county). The easiest way to do that for U.S. maps is to use something called a FIPS Code. You should read about them so you understand what they are, and think of them as a unique identifier for some geographical entity like a state or county. Our md_gov_county dataframe has a FIPS code for each county, but if you ever need one for a county, this is a solved problem thanks to the Tigris library that we used in pre_lab 9.\nWe’ll need to write code to add columns showing the total number of votes for each county and the percentage of votes received by Wes Moore in each county, then replace the CSV file in the data folder with it\n\nmd_gov_county &lt;- md_gov_county |&gt; \n  mutate(total = cox + moore + lashar + wallace + harding + write_ins) |&gt; \n  mutate(pct_moore = moore/total * 100)\n\nwrite_csv(md_gov_county, \"data/md_gov_county_with_percent.csv\")\n\nGo back to Datawrapper and click on “New Map”. Click on “Choropleth map” and then choose “USA &gt;&gt; Counties (2022)” for the map base and click the Proceed button.\nNow we can upload the md_gov_county_with_percent.csv file we just saved using the Upload File button. It should look like the following image:\n\n\n\n\n\n\n\n\n\nWe’ll need to make sure that Datawrapper understands what the data is and where the FIPS code is. Click on the “Match” tab and make sure that yours looks like the image below:\n\n\n\n\n\n\n\n\n\nClick the “Proceed” button (you should have to click it twice, since the first time it will tell you that there’s no data for 3,197 counties - the rest of the U.S.). That will take you to the Visualize tab.\nYou’ll see that the map currently is of the whole nation, and we only have Maryland data. Let’s fix that.\nLook for “Hide regions without data” under Appearance, and click the slider icon to enable that feature. You should see a map zoomed into Maryland with some counties in various colors.\nBut it’s a little rough visually, so let’s clean that up.\nLook for the “Show color legend” label and add a caption for the legend, which is the horizontal bar under the title. Then click on the “Annotate” tab to add a title, description, data source and byline. The title should represent the headline, while the description should be a longer phrase that tells people what they are looking at.\nThat’s better, but check out the tooltip by hovering over a county. It’s not super helpful. Let’s change the tooltip behavior to show the county name and a better-formatted number.\nClick the “Customize tooltips” button so it expands down. Change {{ fips_code }} to {{ county }} and {{ pctmoore }} to {{ FORMAT(pctmoore, “00.0%”)}}\nExperiment with the “Show labels” options to see if you can add county labels to your map.\nOk, that looks better. Let’s publish!\nClick the “Proceed” button until you get to the “Publish & Embed” tab, then click “Publish Now”.",
    "crumbs": [
      "Visualizing your data for publication"
    ]
  },
  {
    "objectID": "workingwithdates.html",
    "href": "workingwithdates.html",
    "title": "Working with dates",
    "section": "",
    "text": "One of the most frustrating things in data is working with dates. Everyone has a different opinion on how to record them, and every software package on the planet has to sort it out. Dealing with it can be a little … confusing. And every dataset has something new to throw at you. So consider this an introduction.\nFirst, there’s the right way to display dates in data. Most of the rest of the world knows how to do this, but Americans aren’t taught it. The correct way to display dates is the following format: YYYY-MM-DD, or 2022-09-15. Any date that looks different should be converted into that format when you’re using R.\nLuckily, this problem is so common that the Tidyverse has an entire library for dealing with it: lubridate.\nWe’re going to do this two ways. First I’m going to show you how to use base R to solve a tricky problem. And then we’ll use a library called lubridate to solve a more common and less tricky problem. And then we’ll use a new library to solve most of the common problems before they start. If it’s not already installed, just run install.packages('lubridate')\n\n\nFirst, we’ll import tidyverse like we always do and our newly-installed lubridate.\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\nLet’s start with a dataset of campaign expenses from Maryland political committees:\n\nmaryland_expenses &lt;- read_csv(\"data/maryland_expenses.csv\")\n\nRows: 97912 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (12): expenditure_date, payee_name, address, payee_type, committee_name,...\ndbl  (1): amount\nlgl  (1): expense_toward\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(maryland_expenses)\n\n# A tibble: 6 × 14\n  expenditure_date payee_name           address payee_type amount committee_name\n  &lt;chr&gt;            &lt;chr&gt;                &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;         \n1 3/12/2021        &lt;NA&gt;                 &lt;NA&gt;    Reimburse     350 Salling   Joh…\n2 3/29/2021        Dundalk Eagle Newsp… PO Box… Business/…    329 Salling   Joh…\n3 4/29/2021        Dundalk Eagle Newsp… PO Box… Business/…    400 Salling   Joh…\n4 5/18/2021        Dundalk Eagle Newsp… PO Box… Business/…    350 Salling   Joh…\n5 6/9/2021         Dundalk Heritage Fa… Dundal… Business/…    200 Salling   Joh…\n6 6/9/2021         Dundalk Heritage Fa… Dundal… Business/…    250 Salling   Joh…\n# ℹ 8 more variables: expense_category &lt;chr&gt;, expense_purpose &lt;chr&gt;,\n#   expense_toward &lt;lgl&gt;, expense_method &lt;chr&gt;, vendor &lt;chr&gt;, fundtype &lt;chr&gt;,\n#   comments &lt;chr&gt;, x14 &lt;chr&gt;\n\n\nTake a look at that first column, expenditure_date. It looks like a date, but see the &lt;chr right below the column name? That means R thinks it’s actually a character column. What we need to do is make it into an actual date column, which lubridate is very good at doing. It has a variety of functions that match the format of the data you have. In this case, the current format is m/d/y, and the lubridate function is called mdy that we can use with mutate:\n\nmaryland_expenses &lt;- maryland_expenses |&gt; mutate(expenditure_date=mdy(expenditure_date))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `expenditure_date = mdy(expenditure_date)`.\nCaused by warning:\n!  18 failed to parse.\n\nhead(maryland_expenses)\n\n# A tibble: 6 × 14\n  expenditure_date payee_name           address payee_type amount committee_name\n  &lt;date&gt;           &lt;chr&gt;                &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;         \n1 2021-03-12       &lt;NA&gt;                 &lt;NA&gt;    Reimburse     350 Salling   Joh…\n2 2021-03-29       Dundalk Eagle Newsp… PO Box… Business/…    329 Salling   Joh…\n3 2021-04-29       Dundalk Eagle Newsp… PO Box… Business/…    400 Salling   Joh…\n4 2021-05-18       Dundalk Eagle Newsp… PO Box… Business/…    350 Salling   Joh…\n5 2021-06-09       Dundalk Heritage Fa… Dundal… Business/…    200 Salling   Joh…\n6 2021-06-09       Dundalk Heritage Fa… Dundal… Business/…    250 Salling   Joh…\n# ℹ 8 more variables: expense_category &lt;chr&gt;, expense_purpose &lt;chr&gt;,\n#   expense_toward &lt;lgl&gt;, expense_method &lt;chr&gt;, vendor &lt;chr&gt;, fundtype &lt;chr&gt;,\n#   comments &lt;chr&gt;, x14 &lt;chr&gt;\n\n\nNow look at the expenditure_date column: R says it’s a date column and it looks like we want it to: YYYY-MM-DD. Accept no substitutes.\nLubridate has functions for basically any type of character date format: mdy, ymd, even datetimes like ymd_hms.\nThat’s less code and less weirdness, so that’s good.\nBut to get clean data, I’ve installed a library and created a new field so I can now start to work with my dates. That seems like a lot, but don’t think your data will always be perfect and you won’t have to do these things.\nStill, there’s got to be a better way. And there is.\nFortunately, readr anticipates some date formatting and can automatically handle many of these issues (indeed it uses lubridate under the hood). When you are importing a CSV file, be sure to use read_csv, not read.csv.\nBut you’re not done with lubridate yet. It has some interesting pieces parts we’ll use elsewhere.\nFor example, in spreadsheets you can extract portions of dates - a month, day or year - with formulas. You can do the same in R with lubridate. Let’s say we wanted to add up the total amount spent in each month in our Maryland expenses data.\nWe could use formatting to create a Month field but that would group all the Aprils ever together. We could create a year and a month together, but that would give us an invalid date object and that would create problems later. Lubridate has something called a floor date that we can use.\nSo to follow along here, we’re going to use mutate to create a month field, group by to lump them together, summarize to count them up and arrange to order them. We’re just chaining things together.\n\nmaryland_expenses |&gt;\n  mutate(month = floor_date(expenditure_date, \"month\")) |&gt;\n  group_by(month) |&gt;\n  summarise(total_amount = sum(amount)) |&gt;\n  arrange(desc(total_amount))\n\n# A tibble: 25 × 2\n   month      total_amount\n   &lt;date&gt;            &lt;dbl&gt;\n 1 2022-10-01    15827467.\n 2 2022-09-01     6603431.\n 3 2022-08-01     5892055.\n 4 2022-11-01     4715694.\n 5 2021-07-01     2242692.\n 6 2021-09-01     2212083.\n 7 2021-08-01     2086313.\n 8 2021-06-01     1827400.\n 9 2021-05-01     1341210.\n10 2021-01-01      772923.\n# ℹ 15 more rows\n\n\nSo the month of June 2022 had the most expenditures by far in this data.",
    "crumbs": [
      "Working with dates"
    ]
  },
  {
    "objectID": "workingwithdates.html#making-dates-dates-again",
    "href": "workingwithdates.html#making-dates-dates-again",
    "title": "Working with dates",
    "section": "",
    "text": "First, we’ll import tidyverse like we always do and our newly-installed lubridate.\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\nLet’s start with a dataset of campaign expenses from Maryland political committees:\n\nmaryland_expenses &lt;- read_csv(\"data/maryland_expenses.csv\")\n\nRows: 97912 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (12): expenditure_date, payee_name, address, payee_type, committee_name,...\ndbl  (1): amount\nlgl  (1): expense_toward\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(maryland_expenses)\n\n# A tibble: 6 × 14\n  expenditure_date payee_name           address payee_type amount committee_name\n  &lt;chr&gt;            &lt;chr&gt;                &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;         \n1 3/12/2021        &lt;NA&gt;                 &lt;NA&gt;    Reimburse     350 Salling   Joh…\n2 3/29/2021        Dundalk Eagle Newsp… PO Box… Business/…    329 Salling   Joh…\n3 4/29/2021        Dundalk Eagle Newsp… PO Box… Business/…    400 Salling   Joh…\n4 5/18/2021        Dundalk Eagle Newsp… PO Box… Business/…    350 Salling   Joh…\n5 6/9/2021         Dundalk Heritage Fa… Dundal… Business/…    200 Salling   Joh…\n6 6/9/2021         Dundalk Heritage Fa… Dundal… Business/…    250 Salling   Joh…\n# ℹ 8 more variables: expense_category &lt;chr&gt;, expense_purpose &lt;chr&gt;,\n#   expense_toward &lt;lgl&gt;, expense_method &lt;chr&gt;, vendor &lt;chr&gt;, fundtype &lt;chr&gt;,\n#   comments &lt;chr&gt;, x14 &lt;chr&gt;\n\n\nTake a look at that first column, expenditure_date. It looks like a date, but see the &lt;chr right below the column name? That means R thinks it’s actually a character column. What we need to do is make it into an actual date column, which lubridate is very good at doing. It has a variety of functions that match the format of the data you have. In this case, the current format is m/d/y, and the lubridate function is called mdy that we can use with mutate:\n\nmaryland_expenses &lt;- maryland_expenses |&gt; mutate(expenditure_date=mdy(expenditure_date))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `expenditure_date = mdy(expenditure_date)`.\nCaused by warning:\n!  18 failed to parse.\n\nhead(maryland_expenses)\n\n# A tibble: 6 × 14\n  expenditure_date payee_name           address payee_type amount committee_name\n  &lt;date&gt;           &lt;chr&gt;                &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;         \n1 2021-03-12       &lt;NA&gt;                 &lt;NA&gt;    Reimburse     350 Salling   Joh…\n2 2021-03-29       Dundalk Eagle Newsp… PO Box… Business/…    329 Salling   Joh…\n3 2021-04-29       Dundalk Eagle Newsp… PO Box… Business/…    400 Salling   Joh…\n4 2021-05-18       Dundalk Eagle Newsp… PO Box… Business/…    350 Salling   Joh…\n5 2021-06-09       Dundalk Heritage Fa… Dundal… Business/…    200 Salling   Joh…\n6 2021-06-09       Dundalk Heritage Fa… Dundal… Business/…    250 Salling   Joh…\n# ℹ 8 more variables: expense_category &lt;chr&gt;, expense_purpose &lt;chr&gt;,\n#   expense_toward &lt;lgl&gt;, expense_method &lt;chr&gt;, vendor &lt;chr&gt;, fundtype &lt;chr&gt;,\n#   comments &lt;chr&gt;, x14 &lt;chr&gt;\n\n\nNow look at the expenditure_date column: R says it’s a date column and it looks like we want it to: YYYY-MM-DD. Accept no substitutes.\nLubridate has functions for basically any type of character date format: mdy, ymd, even datetimes like ymd_hms.\nThat’s less code and less weirdness, so that’s good.\nBut to get clean data, I’ve installed a library and created a new field so I can now start to work with my dates. That seems like a lot, but don’t think your data will always be perfect and you won’t have to do these things.\nStill, there’s got to be a better way. And there is.\nFortunately, readr anticipates some date formatting and can automatically handle many of these issues (indeed it uses lubridate under the hood). When you are importing a CSV file, be sure to use read_csv, not read.csv.\nBut you’re not done with lubridate yet. It has some interesting pieces parts we’ll use elsewhere.\nFor example, in spreadsheets you can extract portions of dates - a month, day or year - with formulas. You can do the same in R with lubridate. Let’s say we wanted to add up the total amount spent in each month in our Maryland expenses data.\nWe could use formatting to create a Month field but that would group all the Aprils ever together. We could create a year and a month together, but that would give us an invalid date object and that would create problems later. Lubridate has something called a floor date that we can use.\nSo to follow along here, we’re going to use mutate to create a month field, group by to lump them together, summarize to count them up and arrange to order them. We’re just chaining things together.\n\nmaryland_expenses |&gt;\n  mutate(month = floor_date(expenditure_date, \"month\")) |&gt;\n  group_by(month) |&gt;\n  summarise(total_amount = sum(amount)) |&gt;\n  arrange(desc(total_amount))\n\n# A tibble: 25 × 2\n   month      total_amount\n   &lt;date&gt;            &lt;dbl&gt;\n 1 2022-10-01    15827467.\n 2 2022-09-01     6603431.\n 3 2022-08-01     5892055.\n 4 2022-11-01     4715694.\n 5 2021-07-01     2242692.\n 6 2021-09-01     2212083.\n 7 2021-08-01     2086313.\n 8 2021-06-01     1827400.\n 9 2021-05-01     1341210.\n10 2021-01-01      772923.\n# ℹ 15 more rows\n\n\nSo the month of June 2022 had the most expenditures by far in this data.",
    "crumbs": [
      "Working with dates"
    ]
  }
]