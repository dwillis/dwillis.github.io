[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Journalism with R and the Tidyverse",
    "section": "",
    "text": "1 Introduction\nIf you were at all paying attention in pre-college science classes, you have probably seen this equation:\nIn English, that says we can know how far something has traveled if we know how fast it’s going and for how long. If we multiply the rate by the time, we’ll get the distance.\nIf you remember just a bit about algebra, you know we can move these things around. If we know two of them, we can figure out the third. So, for instance, if we know the distance and we know the time, we can use algebra to divide the distance by the time to get the rate.\nIn 2012, the South Florida Sun Sentinel found a story in this formula.\nPeople were dying on South Florida tollways in terrible car accidents. What made these different from other car fatal car accidents that happen every day in the US? Police officers driving way too fast were causing them.\nBut do police regularly speed on tollways or were there just a few random and fatal exceptions?\nThanks to Florida’s public records laws, the Sun Sentinel got records from the toll transponders in police cars in south Florida. The transponders recorded when a car went through a given place. And then it would do it again. And again.\nGiven that those places are fixed – they’re toll plazas – and they had the time it took to go from one toll plaza to another, they had the distance and the time.\nIt took high school algebra to find how fast police officers were driving. And the results were shocking.\nTwenty percent of police officers had exceeded 90 miles per hour on toll roads. In a 13-month period, officers drove between 90 and 110 mph more than 5,000 times. And these were just instances found on toll roads. Not all roads have tolls.\nThe story was a stunning find, and the newspaper documented case after case of police officers violating the law and escaping punishment. And, in 2013, they won the Pulitzer Prize for Public Service.\nAll with simple high school algebra.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#modern-data-journalism",
    "href": "index.html#modern-data-journalism",
    "title": "Data Journalism with R and the Tidyverse",
    "section": "1.1 Modern data journalism",
    "text": "1.1 Modern data journalism\nIt’s a single word in a single job description, but a Buzzfeed job posting in 2017 is another indicator in what could be a profound shift in how data journalism is both practiced and taught.\n“We’re looking for someone with a passion for news and a commitment to using data to find amazing, important stories — both quick hits and deeper analyses that drive conversations,” the posting seeking a data journalist says. It goes on to list five things BuzzFeed is looking for: Excellent collaborator, clear writer, deep statistical understanding, knowledge of obtaining and restructuring data.\nAnd then there’s this:\n“You should have a strong command of at least one toolset that (a) allows for filtering, joining, pivoting, and aggregating tabular data, and (b) enables reproducible workflows.”\nThis is not the data journalism of 20 years ago. When it started, it was a small group of people in newsrooms using spreadsheets and databases. Data journalism now encompases programming for all kinds of purposes, product development, user interface design, data visualization and graphics on top of more traditional skills like analyzing data and writing stories.\nIn this book, you’ll get a taste of modern data journalism through programming in R, a statistics language. You’ll be challenged to think programmatically while thinking about a story you can tell to readers in a way that they’ll want to read. They might seem like two different sides of the brain – mutually exclusive skills. They aren’t. I’m confident you’ll see programming is a creative endeavor and storytelling can be analytical.\nCombining them together has the power to change policy, expose injustice and deeply inform.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#installations",
    "href": "index.html#installations",
    "title": "Data Journalism with R and the Tidyverse",
    "section": "1.2 Installations",
    "text": "1.2 Installations\nThis book is all in the R statistical language. To follow along, you’ll do the following:\n\nInstall the R language on your computer. Go to the R Project website, click download R and select a mirror closest to your location. Then download the version for your computer.\nInstall R Studio Desktop. The free version is great.\n\nGoing forward, you’ll see passages like this:\n\ninstall.packages(\"tidyverse\")\n\nThat is code that you’ll need to run in your R Studio. When you see that, you’ll know what to do.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "Data Journalism with R and the Tidyverse",
    "section": "1.3 About this book",
    "text": "1.3 About this book\nThis book is the collection of class materials originally written for Matt Waite’s Data Journalism class at the University of Nebraska-Lincoln’s College of Journalism and Mass Communications. It has been substantially updated by Derek Willis and Sean Mussenden for data journalism classes at the University of Maryland Philip Merrill College of Journalism, with contributions from Sarah Cohen of Arizona State University.\nThere’s some things you should know about it:\n\nIt is free for students.\nThe topics will remain the same but the text is going to be constantly tinkered with.\nWhat is the work of the authors is copyright Matt Waite 2020, Sarah Cohen 2022 and Derek Willis and Sean Mussenden 2023.\nThe text is Attribution-NonCommercial-ShareAlike 4.0 International Creative Commons licensed. That means you can share it and change it, but only if you share your changes with the same license and it cannot be used for commercial purposes. I’m not making money on this so you can’t either.\n\nAs such, the whole book – authored in Quarto – in its original form is open sourced on Github. Pull requests welcomed!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#what-well-cover",
    "href": "index.html#what-well-cover",
    "title": "Data Journalism with R and the Tidyverse",
    "section": "1.4 What we’ll cover",
    "text": "1.4 What we’ll cover\n\nGoogle Sheets\nPublic records and open data\nR Basics\nReplication\nData basics and structures\nAggregates\nMutating\nWorking with dates\nFilters\nCleaning I: Data smells\nCleaning II: Janitor\nCleaning III: Open Refine\nCleaning IV: Pulling Data from PDFs\nJoins\nBasic data scraping\nGetting data from APIs\nVisualizing for reporting: Basics\nVisualizing for reporting: Publishing\nGeographic data basics\nGeographic analysis\nText analysis basics\nBasic statistics\nAI and data journalism\nWriting with and about data\nData journalism ethics",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "start-story.html",
    "href": "start-story.html",
    "title": "2  Learn a new way to read",
    "section": "",
    "text": "2.1 Read like a reporter\nGetting started in data journalism often feels as if you’ve left the newsroom and entered the land of statistics, computer programming and data science. This chapter will help you start seeing data reporting in a new way, by learning how to study great works of the craft as a writer rather than a reader.\nJelani Cobb tweeted, “an engineer doesn’t look at a bridge the same way pedestrians or drivers do.” They see it as a “language of angles and load bearing structures.” We just see a bridge. While he was referring to long-form writing, reporting with data can also be learned by example – if you spend enough time with the examples.\nAlmost all good writers and reporters try to learn from exemplary work. I know more than one reporter who studies prize-winning journalism to hone their craft. This site will have plenty of examples, but you should stay on the lookout for others.\nTry to approach data or empirical reporting as a reporter first, and a consumer second. The goal is to triangulate how the story was discovered, reported and constructed. You’ll want to think about why this story, told this way, at this time, was considered newsworthy enough to publish when another approach on the same topic might not have been.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Learn a new way to read</span>"
    ]
  },
  {
    "objectID": "start-story.html#read-like-a-reporter",
    "href": "start-story.html#read-like-a-reporter",
    "title": "2  Learn a new way to read",
    "section": "",
    "text": "What were the questions?\nIn data journalism, we often start with a tip, or a hypothesis. Sometimes it’s a simple question. Walt Bogdanich of The New York Times is renowned for seeing stories around every corner. Bogdanich has said that the prize-winning story “A Disability Epidemic Among a Railroad’s Retirees” came from a simple question he had when railway workers went on strike over pension benefits – how much were they worth? The story led to an FBI investigation and arrests, along with pension reform at the largest commuter rail in the country.\nThe hypothesis for some stories might be more directed. In 2021, the Howard Center for Investigative Journalism at ASU published “Little victims everywhere”, a set of stories on the lack of justice for survivors of child sexual assault on Native American reservations. That story came after previous reporters for the center analyzed data from the Justice Department showing that the FBI dropped most of the cases it investigated, and the Justice Department then only prosecuted about half of the matters referred to it by investigators. The hypothesis was that they were rarely pursued because federal prosecutors – usually focused on immigration, white collar crime and drugs – weren’t as prepared to pursue violent crime in Indian Country.\nWhen studying a data-driven investigation, try to imagine what the reporters were trying to prove or disprove, and what they used to do it. In journalism, we rely on a mixture of quantitative and qualitative methods. It’s not enough to prove the “numbers” or have the statistical evidence. That is just the beginning of the story. We are supposed to ground-truth them with the stories of actual people and places.\n\n\nGo beyond the numbers\nIt’s easy to focus on the numbers or statistics that make up the key findings, or the reason for the story. Some reporters make the mistake of thinking all of the numbers came from the same place – a rarity in most long-form investigations. Instead, the sources have been woven together and are a mix of original research and research done by others. Try to pay attention to any sourcing done in the piece. Sometimes, it will tell you that the analysis was original. Other times it’s more subtle.\nBut don’t just look at the statistics being reported in the story. In many (most?) investigations, some of the key people, places or time elements come directly from a database.\nWhen I was analyzing Paycheck Protection Program loan data for ProPublica, one fact hit me as I was looking at a handful of sketchy-looking records: a lot of them were from a single county in coastal New Jersey. It turned out to be a pretty good story.\nOften, the place that a reporter visits is determined by examples found in data. In this story on rural development funds, all of the examples came from an analysis of the database. Once the data gave us a good lead, the reporters examined press releases and other easy-to-get sources before calling and visiting the recipients or towns.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Learn a new way to read</span>"
    ]
  },
  {
    "objectID": "start-story.html#reading-tips",
    "href": "start-story.html#reading-tips",
    "title": "2  Learn a new way to read",
    "section": "2.2 Reading tips",
    "text": "2.2 Reading tips\nYou’ll get better at reading investigations and data-driven work over time, but for now, remember to go beyond the obvious:\n\nWhere might the reporters have found their key examples, and what made them good characters or illustrations of the larger issue? Could they have come from the data?\nWhat do you think came first – a narrative single example that was broadened by data (naively, qualitative method), or a big idea that was illustrated with characters (quantitative method)?\nWhat records were used? Were they public records, leaks, or proprietary data?\nWhat methods did they use? Did they do their own testing, use statistical analysis, or geographic methods? You won’t always know, but look for a methodology section or a description alongside each story.\nHow might you localize or adapt these methods to find your own stories?\nPick out the key findings (usually in the nut graf or in a series of bullets after the opening chapter): are they controversial? How might they have been derived? What might have been the investigative hypothesis? Have they given critics their due and tried to falsify their own work?\nHow effective is the writing and presentation of the story? What makes it compelling journalism rather than a dry study? How might you have done it differently? Is a video story better told in text, or would a text story have made a good documentary? Are the visual elements well integrated? Does the writing draw you in and keep you reading? Think about structure, story length, entry points and graphics all working together.\nAre you convinced? Are there holes or questions that didn’t get addressed?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Learn a new way to read</span>"
    ]
  },
  {
    "objectID": "start-story.html#analyze-data-for-story-not-study",
    "href": "start-story.html#analyze-data-for-story-not-study",
    "title": "2  Learn a new way to read",
    "section": "2.3 Analyze data for story, not study",
    "text": "2.3 Analyze data for story, not study\nAs journalists we’ll often be using data, social science methods and even interviewing differently than true experts. We’re seeking stories, not studies. Recognizing news in data is one of the hardest skills for less experienced reporters new to data journalism. This list of potential newsworthy data points is adapted from Paul Bradshaw’s “Data Journalism Heist”.\n\n\n\n\nCompare the claims of powerful people and institutions against facts – the classic investigative approach.\nReport on unexpected highs and lows (of change, or of some other characteristic)\nLook for outliers – individual values that buck a trend seen in the rest\nVerify or bust some myths\nFind signs of distress, happiness or dishonesty or any other emotion.\nUncover new or under-reported long-term trends.\nFind data suggesting your area is the same or different than most others of its kind.\n\nBradshaw also did a recent study of data journalism pieces: “Here are the angles journalists use most often to tell the stories in data”, in Online Journalism Blog. I’m not sure I agree, only because he’s looking mainly at visualizations rather than stories, but they’re worth considering.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Learn a new way to read</span>"
    ]
  },
  {
    "objectID": "start-story.html#exercises",
    "href": "start-story.html#exercises",
    "title": "2  Learn a new way to read",
    "section": "2.4 Exercises",
    "text": "2.4 Exercises\n\nIf you’re a member of Investigative Reporters and Editors, go to the site and find a recent prize-winning entry (usually text rather than broadcast). Get a copy of the IRE contest entry from the Resources page. Try to match up what the reporters said they did and how they did it with key portions of the story.\nThe next time you find a good data source, try to find a story that references it. If your data is local, you might look for a story that used similar data elsewhere, such as 911 response times or overdose deaths. But many stories use federal datasets that can easily be localized. Look at a description of the dataset and then the story to see how the data might have been used.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Learn a new way to read</span>"
    ]
  },
  {
    "objectID": "start-math.html",
    "href": "start-math.html",
    "title": "3  Newsroom math",
    "section": "",
    "text": "3.1 Why numbers?\nJo Craven McGinty, then of The New York Times, used simple rates and ratios to discover that a 6-story brick New Jersey hospital was the most expensive in the nation. In 2012, Bayonne Medical Center “charged the highest amounts in the country for nearly one-quarter of the most common hospital treatments,” the Times story said.\nTo do this story, McGinty only needed to know the number of the procedures reported to the government and the total amount each hospital charged. Dividing those to find an average price, then ranking the most common procedures, led to this surprising result.\nUsing averages, percentages and percent change is the bread and butter of data journalism, leading to stories ranging from home price comparisons to school reports and crime trends. It may have been charming at one time for reporters to announce that they didn’t “do” math, but no longer. Instead, it is now an announcement that the reporter can only do some of the job. You will never be able to tackle complicated, in-depth stories without reviewing basic math.\nThe good news is that most of the math and statistics you need in a newsroom isn’t nearly as difficult as high school algebra. You learned it somewhere around the 4th grade. You then had a decade to forget it before deciding you didn’t like math. But mastering this most basic arithmetic again is a requirement in the modern age.\nIn working with typical newsroom math, you will need to learn how to:\nWhile this chapter covers general tips, you can find specific instructions for typical newsroom math in this Appendix A",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Newsroom math</span>"
    ]
  },
  {
    "objectID": "start-math.html#why-numbers",
    "href": "start-math.html#why-numbers",
    "title": "3  Newsroom math",
    "section": "",
    "text": "Overcome your fear of numbers\nIntegrate numbers into your reporting\nRoutinely compute averages, differences and rates\nSimplify and select the right numbers for your story",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Newsroom math</span>"
    ]
  },
  {
    "objectID": "start-math.html#overcoming-your-fear-of-math",
    "href": "start-math.html#overcoming-your-fear-of-math",
    "title": "3  Newsroom math",
    "section": "3.2 Overcoming your fear of math",
    "text": "3.2 Overcoming your fear of math\nWhen we learned to read, we got used to the idea that 26 letters in American English could be assembled into units that we understand without thinking – words, sentences, paragraphs and books. We never got the same comfort level with 10 digits, and neither did our audience.\nThink of your own reaction to seeing a page of words. Now imagine it as a page of numbers.\nInstead, picture the number “five”. It’s easy. It might be fingers or it might be a team on a basketball court. But it’s simple to understand.\nNow picture the number 275 million. It’s hard. Unfortunately, 275 billion isn’t much harder, even though it’s magnitudes larger. (A million seconds goes by in about 11 days but you may not have been alive for a billion seconds – about 36 years.)\nThe easiest way to get used to some numbers is to learn ways to cut them down to size by calculating rates, ratios or percentages. In your analysis, keep an eye out for the simplest accurate way to characterize the numbers you want to use. “Characterize” is the important word here – it’s not usually necessary to be overly precise so long as your story doesn’t hinge on a nuanced reading of small differences. (And is anything that depends on that news? It may not be.)\nHere’s one example of putting huge numbers in perspective. Pay attention to what you really can picture - it’s probably the $21 equivalent.\n\nThe Chicago hedge fund billionaire Kenneth C. Griffin, for example, earns about $68.5 million a month after taxes, according to court filings made by his wife in their divorce. He has given a total of $300,000 to groups backing Republican presidential candidates. That is a huge sum on its face, yet is the equivalent of only $21.17 for a typical American household, according to Congressional Budget Office data on after-tax income.  “Buying Power”, Nicholas Confessore, Sarah Cohen and Karen Yourish, The New York Times, October 2015\n\nOriginally the reporters had written it even more simply, but editors found the facts so unbelievable that they wanted give readers a chance to do the math themselves. That’s reasonable, but here’s an even simpler way to say it: “earned nearly $1 billion after taxes…He has given $300,000 to groups backing candidates, the equivalent of a dinner at Olive Garden for the typical American family , based on Congressional Budget Office income data.” (And yes, the reporter checked the price for an Olive Garden meal at the time for four people.)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Newsroom math</span>"
    ]
  },
  {
    "objectID": "start-math.html#put-math-in-its-place",
    "href": "start-math.html#put-math-in-its-place",
    "title": "3  Newsroom math",
    "section": "3.3 Put math in its place",
    "text": "3.3 Put math in its place\nFor journalists, numbers – or facts – make up the third leg of a stool supported by human stories or anecdotes, and insightful comment from experts. They serve us in three ways:\n\nAs summaries. Almost by definition, a number counts something, averages something, or otherwise summarizes something. Sometimes, it does a good job, as in the average height of Americans. Sometimes it does a terrible job, as in the average income of Americans. Try to find summaries that accurately characterize the real world.\nAs opinions. Sometimes it’s an opinion derived after years of impartial study. Sometimes it’s an opinion tinged with partisan or selective choices of facts. Use them accordingly.\nAs guesses. Sometimes it’s a good guess, sometimes it’s an off-the-cuff guess. And sometimes it’s a hopeful guess. Even when everything is presumably counted many times, it’s still a (very nearly accurate) guess. Yes, the “audits” of presidential election results in several states in 2021 found a handful of errors – not a meaningful number, but a few just the same.\n\nOnce you find the humanity in your numbers, by cutting them down to size and relegating them to their proper role, you’ll find yourself less fearful. You’ll be able to characterize what you’ve learned rather than numb your readers with every number in your notebook. You may even find that finding facts on your own is fun.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Newsroom math</span>"
    ]
  },
  {
    "objectID": "start-math.html#going-further",
    "href": "start-math.html#going-further",
    "title": "3  Newsroom math",
    "section": "3.4 Going further",
    "text": "3.4 Going further\n\nTipsheets\n\nSteve Doig’s “Math Crib Sheet”\nAppendix A: Common newsroom math, adapted from drafts of the book Numbers in the Newsroom, by Sarah Cohen.\n\n\n\nReading and viewing\n\n“Avoiding Numeric Novocain: Writing Well with Numbers,” by Chip Scanlan, Poynter.com\nT. Christian Miller’s “Writing the data-driven story”\nA viral Twitter thread:\n\nWhat happens in your head when you do 27+48?--- Gene Belcher (@Wparks91) June 25, 2019",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Newsroom math</span>"
    ]
  },
  {
    "objectID": "start-math.html#exercises",
    "href": "start-math.html#exercises",
    "title": "3  Newsroom math",
    "section": "3.5 Exercises",
    "text": "3.5 Exercises\n\nImagine that someone gave you $1 million and you could spend it on anything you want. Write down a list of things that would add up to about that amount. That should be easy. Now, imagine someone gave you $1 billion and you could spend it on whatever you want, but anything left over after a year had to be returned. How would you spend it? (You can give away money, but it can’t be more than 50% of a charity’s annual revenues. So you can’t give 10 $100 million gifts!) See how far you get trying to spend it. A few homes, a few yachts, student loan repayments for all of your friends? You’ve hardly gotten started.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Newsroom math</span>"
    ]
  },
  {
    "objectID": "start-data-def.html",
    "href": "start-data-def.html",
    "title": "4  Defining “Data”",
    "section": "",
    "text": "4.1 The birth of a dataset\nMost journalism uses data collected for one purpose for something entirely different. Understanding its original uses – what matters to the people who collected it, and what doesn’t – will profoundly affect its accuracy or usefulness.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Defining \"Data\"</span>"
    ]
  },
  {
    "objectID": "start-data-def.html#the-birth-of-a-dataset",
    "href": "start-data-def.html#the-birth-of-a-dataset",
    "title": "4  Defining “Data”",
    "section": "",
    "text": "Trace data and administrative records\nIn “The Art of Access”, David Cullier and Charles N. Davis describe a process of tracking down the life and times of a dataset. Their purpose is to make sure they know how to request it from a government agency. The same idea applies to using data that we acquire elsewhere.\nUnderstanding how and why data exists is crucial to understanding what you, as a reporter, might do with it.\nAnything you can systematically search or analyze could be considered one piece of of data. As reporters, we usually deal with data that was created in the process of doing something else – conducting an inspection, delivering a tweet, or scoring a musical. In the sciences, this flotsam and jetsom that is left behind is called “digital trace data” if it was born digitally.\nIn journalism and in the social sciences, many of our data sources were born during some government process – a safety inspection, a traffic ticket, or the filing of a death certificate. These administrative records form the basis of much investigative reporting and they are often the subject of public records and FOIA requests. They were born as part of the government doing its job, without any thought given to how it might be used in another way. In the sciences, those are often called “administrative records”.\nThis trace data might be considered the first part of the definition above – information that can be stored and used.\nHere’s how Chris Bail from Duke University describes it.\n\n\nData collected and curated for analysis\nAnother kind of data is that which is compiled or collected specifically for the purpose of studying something. It might collected in the form of a survey or a poll, or it might be a system of sampling to measure pollution or weather. But it’s there because the information has intrinsic value AS information.\nThe video suggests a hard line between trace data and custom data. In practice, it’s not that clear. Many newsrooms may curate data published in other sources or in administrative records, such as the Washington Post’s police shooting dataset. In other cases, the agencies we are covering get already-compiled data from state and local governments.\nThis type of data might be considered the second type in the definition – tabular information that is used for decision-making.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Defining \"Data\"</span>"
    ]
  },
  {
    "objectID": "start-data-def.html#granular-and-aggregated-data",
    "href": "start-data-def.html#granular-and-aggregated-data",
    "title": "4  Defining “Data”",
    "section": "4.2 Granular and aggregated data",
    "text": "4.2 Granular and aggregated data\nOne of the hardest concepts for a lot of new data journalists is the idea of granularity of your data source. There are a lot of ways to think about this: individual items in a list vs. figures in a table; original records vs. compilations; granular data vs. statistics.\nGenerally, an investigative reporter is interested in getting data that is as close as possible to the most granular information that exists, at least on computer files. Here’s an example, which might give you a little intuition about why it’s so important to think this way:\nWhen someone dies in the US, a standard death certificate is filled out by a series of officials - the attending physician, the institution where they died and even the funeral direcor.\nClick on this link to see a blank version of the standard US death certificate form – notice the detail and the detailed instructions on how it is supposed to be filled out. 2\nA good reporter could imagine many stories coming out of these little boxes. Limiting yourself to just to COVID-19-related stories: You could profile the local doctor who signed the most COVID-19-related death certificates in their city, or examine the number of deaths that had COVID as a contributing, but not underlying or immediate, cause of death. You could compare smoking rates in the city with the number of decedents whose tobacco use likely contributed to their death. Maybe you’d want to know how long patients suffered with the disease before they died. And you could map the deaths to find the block in your town most devastated by the virus.\nEarly in the pandemic, Coulter Jones and Jon Kamp examined the records from one of the few states that makes them public, and concluded that “Coronavirus Deaths were Likely Missed in Michigan, Death Certificates Suggest”\nBut you probably can’t do that. The reason is that, in most states, death certificates are not public records and are treated as secrets. 3. Instead, state and local governments provide limited statistics related to the deaths, usually by county, with no detail. That’s the difference between granular data and aggregate data. Here are some of the typical (not universal) characteristics of each:\n\n\n\n\n\n\n\nGranular\nAggregate\n\n\n\n\nIntended for some purpose other than your work\nIntended to be presented as is to the public\n\n\nMany rows (records), few columns (variables)\nMany columns (variables), few rows (records)\n\n\nRequires a good understanding of the source\nExplanatory notes usually come with the data\n\n\nEasy to cross-reference and compile\nOften impossible to repurpose\n\n\nHas few numeric columns\nMay be almost entirely numerical\n\n\nIs intended for use in a database\nIs intended for use in a spreadsheet\n\n\n\nWe often have to consider the trade-offs. Granular data with the detail we need - especially when it involves personally identifiable information like names and addresses - can take months or years of negotiation over public records requests, even when the law allows it. It’s often much easier to convince an agency to provide summarized or incomplete data. Don’t balk at using it if it works for you. But understand that in the vast majority of cases, it’s been summarized in a way that’s lost information that could be important to your story.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Defining \"Data\"</span>"
    ]
  },
  {
    "objectID": "start-data-def.html#nouns",
    "href": "start-data-def.html#nouns",
    "title": "4  Defining “Data”",
    "section": "4.3 Nouns",
    "text": "4.3 Nouns\nThat brings us to one of the most important things you must find out about any data you begin to analyze: What “noun” does each row in a tabular dataset represent? In statistics, they might be called observations or cases. In data science, they’re usually called records. Either way, every row must represent the same thing – a person, a place, a year, a water sample or a school. And you can’t really do anything with it until you figure out what that is.\nIn 2015, Sarah Cohen did a story at The New York Times called “More Deportation Follow Minor Crimes, Records Show”. The government had claimed it was only removing hardened criminals from the country, but our analysis of the data suggested that many of them were for minor infractions.\nIn writing the piece, they had to work around a problem in our data: the agency refused to provide them anything that would help us distinguish individuals from one another. All the reporters knew was that each row represented one deportation – not one person! Without a column, or field or a variable or an attribute for an individual – say, name and date of birth, or some scrambled version of an their DHS number – they had no way to even estimate how often people were deported multiple times. If you read the story, you’ll see the very careful wording, except when they had reported out and spoken to people on the ground.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Defining \"Data\"</span>"
    ]
  },
  {
    "objectID": "start-data-def.html#further-reading",
    "href": "start-data-def.html#further-reading",
    "title": "4  Defining “Data”",
    "section": "4.4 Further reading",
    "text": "4.4 Further reading\n\n“Basic steps in working with data”, the Data Journalism Handbook, Steve Doig, ASU Professor. He describes in this piece the problem of not knowing exactly how the data was compiled.\n“Counting the Infected” , Rob Gebellof on The Daily, July 8, 2020.\n“Spreadsheet thinking vs. Database thinking”, by Robert Kosara, gets at the idea that looking at individual items is often a “database”, and statistical compilations are often “spreadsheets”.\n“Tidy Data”, in the Journal of Statistical Software (linked here in a pre-print) by Hadley Wickham , is the quintessential article on describing what we think of as “clean” data. For our purposes, much of what he describes as “tidy” comes when we have individual, granular records – not statistical compilations. It’s an academic article, but it has the underlying concepts that we’ll be working with all year.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Defining \"Data\"</span>"
    ]
  },
  {
    "objectID": "start-data-def.html#exercises",
    "href": "start-data-def.html#exercises",
    "title": "4  Defining “Data”",
    "section": "4.5 Exercises",
    "text": "4.5 Exercises\n\nThe next time you get a government statistical report, scour all of the footnotes to find some explanation of where the data came from. You’ll be surprised how often they are compilations of administrative records - the government version of trace data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Defining \"Data\"</span>"
    ]
  },
  {
    "objectID": "start-data-def.html#footnotes",
    "href": "start-data-def.html#footnotes",
    "title": "4  Defining “Data”",
    "section": "",
    "text": "I flipped the order of these two definitions!↩︎\nYou should do this whenever you get a dataset created from administrative records. That is, track down its origin and examine the pieces you were given and the pieces that were left out; look at what is written in free-form vs what is presented as a check box. You may need a copy of the template that an agency uses to collect the information, but many governments make these available on their websites or are willing to provide them without a fuss.↩︎\nSee “Secrecy in Death Records: A call to action”, by Megain Craig and Madeleine Davison, Journal of Civic Information, December 2020↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Defining \"Data\"</span>"
    ]
  },
  {
    "objectID": "gs-intro.html",
    "href": "gs-intro.html",
    "title": "5  Introduction",
    "section": "",
    "text": "5.1 Tutorials\nSome people consider using spreadsheets the table stakes for getting into data journalism. It’s relatively easy to see what you’re doing and you can easily share your work with your colleagues. In fact, pieces of the Pulitzer-Prize winning COVID-19 coverage from The New York Times was compiled using an elaborate and highly tuned set of Google spreadsheets with dozens of contributors.\nThis guide uses Google Sheets, although you should be able to do these exercises with Excel on the Mac or Windows if you prefer. Google Sheets has the advantage of being accessible anywhere with an internet connection and makes collaboration straightforward. You can easily share sheets with colleagues and work on them simultaneously.\nSpreadsheets in the form of Google Sheets or Excel are used in almost every workplace in America. This section covers most of what you need in the newsroom, which is a different set of skills than in other businesses.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "gs-intro.html#tutorials",
    "href": "gs-intro.html#tutorials",
    "title": "5  Introduction",
    "section": "",
    "text": "A Google Sheets Refresher: Start over with good habits\nSorting and filtering to find stories: The first step of interviewing data\nGrouping with pivot tables: Aggregating, and the super power of spreadsheets\nFormulas in Google Sheets: Percents, sums, and other basic computations used in newsrooms.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "gs-intro.html#practice-exercises",
    "href": "gs-intro.html#practice-exercises",
    "title": "5  Introduction",
    "section": "5.2 Practice exercises",
    "text": "5.2 Practice exercises\n\nPractice with “notice of claims” from Phoenix: Filtering and pivot table practice using claims made against the city of Phoenix 2010-2020.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "gs-refresher.html",
    "href": "gs-refresher.html",
    "title": "6  A Google Sheets Refresher",
    "section": "",
    "text": "6.1 Re-learning Sheets from the ground up\nSpreadsheets are everywhere, so it’s worth re-learning how to use them well. Reporters usually use spreadsheets in three ways:\n(This guide uses Google Sheets, which works on any operating system with a web browser. The keyboard shortcuts listed work on both Mac and PC.)\nSome reporters flinch at typing in 30 or 100 entries into a spreadsheet. You shouldn’t. If you learn to take notes in a structured way, you’ll always be able to find and verify your work. If you try to calculate a sum of 30 numbers on a calculator, you’ll have to type them all in at least twice anyway. Also, getting used to these easy tasks on a spreadsheet keeps your muscles trained for when you need to do more.\nThe areas of the spreadsheet have different visual clues, and learning to read them will make your life much easier.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>A Google Sheets Refresher</span>"
    ]
  },
  {
    "objectID": "gs-refresher.html#re-learning-sheets-from-the-ground-up",
    "href": "gs-refresher.html#re-learning-sheets-from-the-ground-up",
    "title": "6  A Google Sheets Refresher",
    "section": "",
    "text": "6.1.1 The spreadsheet grid\n\n\n\nWhen you start up a spreadsheet (try entering sheet.new into your browser!), you’ll see letters across the top and numbers down the side. If you ever played Battleship, you’ll recognize the idea – every little square, or cell, is referenced by the intersection of its column letter and row number:\nB2 is the cell that is currently active. You can tell because it’s outlined in the sheet and it’s shown on the upper left corner.\n\n\n6.1.2 Mouse shapes\n\n\n\n\n\n\n\nThe Copy Tool, or the thin black cross. When you see this, you’ll copy anything that’s selected. This can be good or bad.\n\n\n\nThe Evil Hand. If you use this symbol, you will MOVE the selection to a new location. This is very rarely a good idea or something you intend.\n\n\n\n\n\n\n6.1.3 Selecting cells and ranges\nSpreadsheets act only on the cells or regions you have selected. If you begin typing, you’ll start entering information into the currently selected cell.\nTo select: Hold the cursor over the cell and click ONCE – not twice. Check the formula bar to make sure you’ve selected what you think you’ve got. You can also look at the bottom right of your spreadsheet for more information.\nYou’ll often work with ranges of cells in formulas. These are defined by the corners of the area you want to work on – often a column of information. In the example below, the range is A1:B6, with the “:” referring to the word “through”.\nTo select a group of cells and act on them all at once: Hover the cursor over one corner, click ONCE and drag to the diagonal corner. Make sure the Evil Hand is nowhere to be seen. The entire area will be shaded in except for the currently selected cell. Look at the upper right corner to see how many rows and columns you selected.\n\n\n\n\n\n\n\n\n\n\n\n\nTo select a column or row : Hover the cursor over the letter at the top of the column. For a row, hover it over the row number in the margin\n\n\n6.1.4 Reading the screen\n\n\n\n6.1.5 Entering data\nSelect the cell and start typing. The information you type won’t be locked into the cell until you hit the Return / Enter key, or move your selection to another cell. Hit “Escape” to cancel the entry.\nYou can’t do a lot of things while you’re editing, so if you have a lot of greyed out menu items, look at your formula bar to see if you are still editing a cell.\nIf you’re having trouble getting to a menu item or seeing the result of your work, try hitting “Escape” and try again. You may not have actually entered the information into the sheet.\n\n\n6.1.6 Locking in headings\nAs your spreadsheet grows vertically with more rows, you’ll want to be able to see the top all the time. When it grows horizontally with more columns, you’ll probably want to see columns in the left, such as names. This is called “Freezing Panes” – you freeze part of the page so it stays in place when you move around.\nIn Google Sheets, this is done via the View -&gt; Freeze menu:\n\n\n\nfreeze panes\n\n\n\n\n6.1.7 Formatting tricks\n\nUse the toolbar buttons or the Format menu to make numbers easier to read.\nIf a column is filled with a lot of text, select the column and look in the toolbar for the “Wrap text” button (it looks like text wrapping around an image). This means that when you double-click to widen a column, it will get taller, not wider. This is good when you need to save valuable real estate on the screen.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>A Google Sheets Refresher</span>"
    ]
  },
  {
    "objectID": "gs-refresher.html#getting-started-with-a-dataset",
    "href": "gs-refresher.html#getting-started-with-a-dataset",
    "title": "6  A Google Sheets Refresher",
    "section": "6.2 Getting started with a dataset",
    "text": "6.2 Getting started with a dataset\nSLOW DOWN! Don’t do anything until you understand what you have in front of you and can predict what your next mouse click will do to it.\nMost data we encounter was created by someone else for some purpose other than ours. This means that you can’t assume anything. It may not be complete. It may be inaccurate. It may mean something completely different than it appears at first blush.\n\n6.2.1 First steps\n\nDocument where you got the spreadsheet and how you can get back to the original.\nRead anything you can about what it contains. Look for documentation that comes with the data.\nSave the original into a safe place with its original name and metadata. Work on a copy.\nIf the spreadsheet shows #### instead of words or numbers, widen your columns. If it shows 7E-14 or something like that, format them as numbers, not “Automatic”.\nCheck your corners – look at the top left and bottom right. Is the data all in one area? Are there footnotes or other non-data sections mixed in? We’re going to want to fix that later.\n\n\n\n6.2.2 Interview your data\n\n6.2.2.1 Headings\nThe most fraught part of data reporting is understanding what each column actually means. These often have cryptic, bureaucratic names. You may need to go back to the source of the data to be sure you actually understand them.\nIf your data doesn’t have any headings, that’s going to be your first priority. In effect, you’ll need to build what we call a data dictionary or record layout if one hasn’t been provided. Many reporters create these as a page in a dataset.\n\n\n6.2.2.2 Unit of analysis\nA unit of analysis refers to the items that are listed in the rows of your dataset. Ideally, every row should be at the same unit of analysis – a person, an inspection, or a city, for example. Summaries should be separated by a blank row, or moved to a different sheet. Think of this as the noun you’d use to describe every row.\n\n\n6.2.2.3 Row numbers\nThe data was probably given to you in some sort of natural sort order. Different computer systems sort differently – some are case-sensitive, others are not. It may depend on when and where the data was created! The order of the data may even depend on a column you don’t have. If you don’t do something now, you’ll never be able to get back to the original order, which could have meaning for both the agency and for fact-checking.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>A Google Sheets Refresher</span>"
    ]
  },
  {
    "objectID": "gs-filter-sort.html",
    "href": "gs-filter-sort.html",
    "title": "7  Sorting and filtering to find stories",
    "section": "",
    "text": "7.1 A sorting miracle\nWhen Stephen Neukam - who was sitting in this class a few years ago - wanted to find out who was funding candidates for Maryland’s open governor’s seat this year, he downloaded data from the State Board of Elections that listed contributions to the wide array of hopefuls seeking to replace Larry Hogan in Annapolis.\nHe wasn’t sure at first what he was looking for, so he started the way that many reporters do with data: by sorting and filtering. Were there outliers in the list of contributions, and which candidates were getting their money from unusual (non-Maryland) sources?\nNeukam quickly found his story: in the race to be governor, Maryland candidates, and in particular Wes Moore, a first-time Democratic candidate, were raising millions of dollars from out of state donors.\nThe story, “Millions in out-of-state donations help fuel high-profile Maryland Democratic governor candidates” helped explain where candidates were going to fund one of the most contested primaries in recent history (Moore ended up winning).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Sorting and filtering to find stories</span>"
    ]
  },
  {
    "objectID": "gs-filter-sort.html#sorting-and-filtering-as-a-reporting-tool",
    "href": "gs-filter-sort.html#sorting-and-filtering-as-a-reporting-tool",
    "title": "7  Sorting and filtering to find stories",
    "section": "7.2 Sorting and filtering as a reporting tool",
    "text": "7.2 Sorting and filtering as a reporting tool\nSorting and filtering can:\n\nNarrow your focus to specific items that you want to examine in your story.\nShow you rows containing the highest and lowest values of any column. That can be news or it can be errors or other problems with the data.\nLet you answer quick “how many?” questions, with a count of the rows that match your criteria. (In the next lesson, you’ll see that pivot tables, or group-by queries, are much more powerful for this in most cases.)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Sorting and filtering to find stories</span>"
    ]
  },
  {
    "objectID": "gs-filter-sort.html#example-data",
    "href": "gs-filter-sort.html#example-data",
    "title": "7  Sorting and filtering to find stories",
    "section": "7.3 Example data",
    "text": "7.3 Example data\n::: {.alert .alert-info } - Data from the State Board of Elections for use in this tutorial - Documentation from the SBOE’s site :::\n\nThe data for this is from the Maryland State Board of Elections’s Campaign Finance Database. There are a couple of caveats:\nIt includes money raised as of Jan. 12, 2022, which covers all of 2021.\nThese are self-reported by campaigns, and subject to amendment in case of errors or omissions.\n\nThe original data download link for Wes Moore’s contributions is https://github.com/stephenneukam/CNS_Annapolis/raw/main/Campaign_finance/Moore_ContributionsList.csv. Download it to your computer and then, in a browser, type sheet.new to create a new Google Sheet. From there, use File -&gt; Import and choose “Upload” and select the file on your computer. Click the “Import Data” button when it appears. Then give your sheet a name, like “Wes Moore Contributions”.\nIt’s a good example set for us because it’s been used as the basis of Neukam’s story and it has at least one of each data type that we plan to deal with in Google Sheets. And, critically, the first row contains headers, not data. Always have headers, even if you have to add them.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Sorting and filtering to find stories</span>"
    ]
  },
  {
    "objectID": "gs-filter-sort.html#understanding-data-types",
    "href": "gs-filter-sort.html#understanding-data-types",
    "title": "7  Sorting and filtering to find stories",
    "section": "7.4 Understanding data types",
    "text": "7.4 Understanding data types\nWhen you open the spreadsheet, the first thing to notice is its granularity. Unlike Census or budget spreadsheets, this is a list capturing specific characteristics of each contribution. Each column has the same type of data from top to bottom. Those types are:\n\nText. Text or “character” columns can come in long or short form. When they are standardized (the values can contain only one of a small list of values), they’re called “categorical”. If they’re more free-form, they might be called “free text”. The computer doesn’t know the difference, but you should. The Post data has examples of both. In spreadsheets, text is left-justified (they move toward the left of the cell and will line up vertically at the beginning)\nNumbers. These are pure numbers with no commas, dollar signs or other embellishments. In Google Sheets these can be formatted to look like numbers in different ways, but underneath they’re just numbers. Adding up a column of numbers that has a word in it or has missing values will just be ignored. It will trip up most other languages. These are right-justified, so the last digit is always lined up vertically.\nLogical: This is a subset of text. It can take one of only two values – yes or no, true or false. There is no “maybe”.\nDate and times: These are actual dates on the calendar, which have magical properties. Underneath, they are a number. In Google Sheets, that number is the number of days since December 30, 1899.1 They can also have time attached to them, which is a fraction of a day. What this means is that the number 44,536.5 is really Dec. 6, 2021 at noon. In Sheets, you use a format to tell the spreadsheet how you want to see the date or time, just the way you look at dollar values with commas and symbols. (If you get a spreadsheet with a lot of dates of 12/30/1899, it means there is a 0 in that column, which is sometimes a fill-in for “I don’t know.”)\n\nHere’s a picture of a date that is shown in a variety of formats.\n\n\n\ndate formats\n\n\nAll of these are the same, underlying value – the number at the left. Notice that all of these are right-justified.\nThis means that when you see “Friday, December 10”, the computer sees 44540.87431. When you put the dates in order, they won’t be alphabetized with all of the Fridays shown together. Instead, they’ll be arranged by the actual date and time.\nIt also means that you can compute 911 response times even when it crosses midnight, or compute someone’s age today given a date of birth. Keeping actual calendar dates in your data will give it much more power than just having the words. (Sheets uses the 1st of the month as a stand-in for an actual date when all you know is the month and year.)\n\n7.4.1 Sorting rows\nSorting means rearranging the rows of a data table into a different order. Some reporters take a conceptual shortcut and call this “sorting columns”. That thinking will only get you into trouble – it lets you forget that you want to keep the rows intact while changing the order in which you see them. In fact, in other languages it’s called “order by” or “arrange” by one or more columns – a much clearer way to think of it.\nTo sort in Google Sheets, first highlight the entire sheet by clicking on the button above the first row and to the left of the first column. So, between the 1 and the A. Then, look for the sort options under the Data drop-down menu at the top of your screen, and choose “Sort Range” and then “Advanced range sorting options”. Trust me, this is how you want to do it. Check the box that says “Data has header row” and Sort by Contribution Date. In this case, sorting from A to Z gives you a list of the contributions in chronological order.\n\n\nAdding fields to the sort\nAdding more columns to the sort box tells Sheets what to do when the first one is the same or tied. For example, sorting first by date then by amount gives you a list that shows all of the contributions by date in sequence:\n\n\n\n\n7.4.2 Filtering\nFiltering means picking out only some of the rows you want to see based on a criteria you select in a column. Think of it as casting a fishing net – the more filters you add, the fewer fish will be caught.\nTo turn on filters in Google Sheets, go to Data -&gt; Create a filter. It will add small down arrows to each column in the first row, another reason why headers are crucial. You can filter by multiple columns, and each filter you select adds more conditions, narrowing your net.\nTo find contributions of $5,000, use the drop-down menu under Contribution Amount to select it and hit “OK”.\nWhen you do this, notice that the drop-down arrow has turned into a solid green funnel and that any rows that don’t match your filter are hidden.\n\n\nThis method works for small-ish and simple-ish columns. If your column has more than 10,000 different entries, such as names or addresses, only the first 10,000 will be considered. We only caught these for stories when someone did a fact-check using a different method of filtering. If your column has a lot of distinct entries, use the option that says “Filter by condition”, and then use the “Text contains” option. Better yet, don’t use filtering for counting things at all.\n\nAdd more filters to narrow down your list of cases even more. For example, if you wanted to see $5,000 contributions from individuals, you would choose “Individual” under Contributor Type:\n\n\nDifferent kinds of filters\nThere are several options under the filter drop-down menu under “Filter by condition”, and you also can type values into a search box to try and filter that way (the latter option is best for text columns). There also is a “Filter by color” option. My opinion: don’t do this.\n ## FAQ\n\n\n\nHow do I turn off all of my filters\nIn the Data menu, choose “Remove filter” to remove all of the filters.\n\n\nWhere is the button to filter columns?\nSometimes you don’t want to see all of your columns – there are too many and they’re getting confusing. There is no column filter in Sheets (You’ll see how to filter, or “Select”, columns from a dataset in R later.)\nInstead, you can hide the columns you don’t want to see. When columns and rows are hidden, they generally won’t copy to a new sheet.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Sorting and filtering to find stories</span>"
    ]
  },
  {
    "objectID": "gs-filter-sort.html#footnotes",
    "href": "gs-filter-sort.html#footnotes",
    "title": "7  Sorting and filtering to find stories",
    "section": "",
    "text": "Each language deals with dates and times a little differently. We’ll see how R does it later on. But just know that dates can be tricky because of these differences and time is even more tricky↩︎",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Sorting and filtering to find stories</span>"
    ]
  },
  {
    "objectID": "gs-pivot.html",
    "href": "gs-pivot.html",
    "title": "8  Grouping with pivot tables",
    "section": "",
    "text": "Confusing grouping with sorting or arranging\nTo get totals for out of state contributions to Maryland’s 2022 gubernatorial primary candidates, Stephen Neukam downloaded data from the State Board of Elections for Wes Moore and other candidates. Make a copy of this Google Sheet so we can work with that data.\nNeukam’s story is about out-of-state contributions:\nSummarizing a list of items in a spreadsheet is done using pivot tables. In other languages, it’s considered “aggregating” or “grouping and summarizing”. Think of pivot tables and grouping as answering the questions, “How many?” and “How much?”. They are particularly powerful when your question also has the words “the most” or “the least” or “of each”. Some examples:\nMany reporters confuse this summarization with “sorting”. One reason is that this is how we express the concept in plain language: “I want to sort Skittles by color”.\nBut in data analysis, sorting and grouping are very different things. Sorting involves shuffling a table’s rows into some order based on the values in a column. In other languages, this is called arranging or ordering, much clearer concepts. Grouping, which is what pivot tables do, is a path to aggregating and computing summary statistics such as a count (the number of items), sum (how much they add up to), or average for each category. It means “make piles and compute statistics for each one.”",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Grouping with pivot tables</span>"
    ]
  },
  {
    "objectID": "gs-pivot.html#tutorial",
    "href": "gs-pivot.html#tutorial",
    "title": "8  Grouping with pivot tables",
    "section": "8.1 Tutorial",
    "text": "8.1 Tutorial\n\nSetting up the pivot table\nStart with your cursor somewhere in your data, and choose Insert, then Pivot table. Then hit the Create button.\n\n\n\ninsert menu\n\n\nIf all goes well, it will look like your data disappeared. It didn’t – you’re just on a new page. Here’s what it looks like:\n\n\n\npivot menu\n\n\n\n\nCounting, or “how many”?\nThe section on the right gives you an outline of what to do. The section on the left will get filled in as you make your pivot table. If you want to see the number of contributions by state, drag the state column into the “Rows” area, then drag something that’s always filled out into the Values area (candidate is a safe one in this data).\n ### More variables {.unnumbered}\nSuppose you’d like to see the number of contributions by candidate, with the candidates across the top and the state down the sides. Drag the candidate variable into the column area. Sorting can get tricky on pivot tables, but in this case it will work to put the largest number on top. This won’t work with percentages – it still sorts by the underlying number.\n\n\n\nEven more variables\nSay you wanted to see each candidate’s total number of contributions by month and state.\nThis isn’t always easy to organize. It means you’d need to have a pivot table with TWO columns down the side, and one across the top. Here’s my attempt at getting there:\n\n\n\nbadsort\n\n\nThis is after some fiddling with the formats, and I still can’t sort properly. We can’t sort by the combination of month and state.\nYour choices here are limited: Copy and paste the values of the pivot table into a new sheet and sort there, or create a new variable by concatenating the name of the month and state into one column.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Grouping with pivot tables</span>"
    ]
  },
  {
    "objectID": "gs-pivot.html#faq",
    "href": "gs-pivot.html#faq",
    "title": "8  Grouping with pivot tables",
    "section": "8.2 FAQ",
    "text": "8.2 FAQ\n\nEverything disappeared!\nIf you select something outside of that pivot table on the left, the menu on the right disappears. Select something in the pivot table area and it will likely come back.\n\n\nI have too many columns\nIf you want two sets of statistics – say, number of contributions and percent of contributions – across the top, it can get very wide and confusing very quickly. One alternative is to change it into more of a vertical rectangle by dragging the “Values” element from the columns to the rows on the right. (This only shows up when you have two calculations being made.)\n\n\nThings aren’t adding up\nYou have to be super careful about which column you use to Count things – it has to always be filled out (there can’t be any blanks). Go through the filters and find one that doesn’t have (Blanks) at the bottom to be sure.\n\n\nIt’s a crazy number!\nYou might have dragged a numeric column into the “Values” area. Check to see if it says “COUNTA” or “SUM”. Change it to “COUNTA” if it has something else on it, unless you wanted to add up that column.\n\n\nThis is so frustrating - I can’t get what I want\nRight? It’s time to go to a programming language!",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Grouping with pivot tables</span>"
    ]
  },
  {
    "objectID": "ethics.html",
    "href": "ethics.html",
    "title": "9  Ethics in data journalism",
    "section": "",
    "text": "9.1 Problems\nThis originally appeared on Open News in March 2013.\nIn 2009, a senior web editor asked me and another developer a question: could our development group build a new news application for Tampabay.com that displayed a gallery of mug shots? Stories about goofy crimes with strange mug shots were popular with readers. The vision, on the part of management, was a website that would display the mugshots collected every day from publicly available websites by two editors—well paid, professional editors with other responsibilities.\nNewsrooms are many things. Alive. Filled with energy. Fueled by stress, coffee and profanity. But they are also idea factories. Day after day, ideas come from everywhere. From reporters on the beat. From editors reading random things. From who knows where. Some of them are brilliant. Some would never work. Most need more people and time than are available. And some are dumber than anyone cares to admit.\nWe thought this idea was nuts. Why would we pay someone, let alone an editor, to fetch mug shots from the Internet? Couldn’t we do that with a scraper?\nIf only this were the most complex question we would face.\nBecause given enough time and enough creativity, scraping a mug shot website is easy. You need to recognize a pattern, parse some HTML and gather the pieces you need. At least that’s how it should work. Police agencies that put mugs online usually buy software from a vendor. Apparently, those vendors enjoy making horrific, non-standard, broken-in-interesting-and-unique-ways HTML. You’ll swear. A lot. But you’ll grind it out. And that’s part of the fun. Scraping isn’t any fun with clean, semantic, valid HTML. And scraping mug shot websites, by that definition, is tons of fun.\nThe complexity comes when you realize the data you are dealing with represent real people’s lives.\nThe first problem we faced, long before we actually had data, was that data has a life of its own. Because we were going to put this information in front of a big audience, Google was going to find it. That meant if we used our normal open door policy for the Googlebot, someone’s mug shot was going to be the first record in Google for their name, most likely. It would show up first because most people dont actively cultivate their name on the web for visibility in Google. It would show up first because we know how SEO works and they dont. It would show up first because our site would have more traffic than their site, and so Google would rank us higher.\nAnd that record in Google would exist as long as the URL did. Longer when you consider the cached versions Google keeps.\nThat was a problem because here are the things we could not know:",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Ethics in data journalism</span>"
    ]
  },
  {
    "objectID": "ethics.html#problems",
    "href": "ethics.html#problems",
    "title": "9  Ethics in data journalism",
    "section": "",
    "text": "Was this person wrongly arrested?\nWas this person innocent?\nWere the charges dropped against this person?\nDid this person lie about any of their information?",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Ethics in data journalism</span>"
    ]
  },
  {
    "objectID": "ethics.html#the-googlebot",
    "href": "ethics.html#the-googlebot",
    "title": "9  Ethics in data journalism",
    "section": "9.2 The Googlebot",
    "text": "9.2 The Googlebot\nSo it turned out to be very important to know the Googlebot. It’s your friend … until it isn’t. We went to our bosses and said words that no one had said to them before: we did not want Google to index these pages. In a news organization, the page view is the coin of the realm. It is — unfortunately — how many things are evaluated when the bosses ask if it was successful or not. So, with that in mind, Google is your friend. Google brings you traffic. Indeed, Google is your single largest referrer of traffic at a news organization, so you want to throw the doors open and make friends with the Googlebot.\nBut here we were, saying Google wasn’t our friend and that we needed to keep the Googlebot out. And, thankfully, our bosses listened to our argument. They too didn’t want to be the first result in Google for someone.\nSo, to make sure we were telling the Googlebot no, we used three lines of defense. We told it no in robots.txt and on individual pages as a meta tag, and we put the most interesting bits of data into a simple JavaScript wrapper that made it hard on the bot if the first two things failed.\nThe second solution had ramifications beyond the Googlebot. We decided that we were not trying to make a complete copy of the public record. That existed already. If you wanted to look at the actual public records, the sheriff’s offices in the area had websites and they were the official keeper of the record. We were making browsing those images easy, but we were not the public record.\nThat freedom had two consequences: it meant our scrapers could, at a certain point and given a number of failures, just give up on getting a mug. Data entered by humans will be flawed. There will be mistakes. Because of that, our code would have to try and deal with that. Well, there’s an infinite number of ways people can mess things up, so we decided that since we were not going to be an exact copy of the public record, we could deal with the most common failures and dump the rest. During testing, we were getting well over 98% of mugs without having to spend our lives coding for every possible variation of typo.\nThe second consequence of the decision actually came from the newspapers lawyers. They asked a question that dumbfounded us: How long are you keeping mugs? We never thought about it. Storage was cheap. We just assumed we’d keep them all. But, why should we do that? If we’re not a copy of the public record, we dont have to keep them. And, since we didnt know the result of each case, keeping them was really kind of pointless.\nSo, we asked around: How long does a misdemeanor case take to reach a judgement? The answer we got from various sources was about 60 days. From arrest to adjudication, it took about two months. So, at the 60 day mark, we deleted the data. We had no way of knowing if someone was guilty or innocent, so all of them had to go. We even called the script The Reaper.\nWe’d later learn that the practical impacts of this were nil. People looked at the day’s mugs and moved on. The amount of traffic a mug got after the day of arrest was nearly zero.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Ethics in data journalism</span>"
    ]
  },
  {
    "objectID": "ethics.html#data-lifetimes",
    "href": "ethics.html#data-lifetimes",
    "title": "9  Ethics in data journalism",
    "section": "9.3 Data Lifetimes",
    "text": "9.3 Data Lifetimes\nThe life of your data matters. You have to ask yourself, Is it useful forever? Does it become harmful after a set time? We had to confront the real impact of deleting mugs after 60 days. People share them, potentially lengthening their lifetime long after they’ve fallen off the homepage. Delete them and that URL goes away.\nWe couldn’t stop people from sharing links on social media—and indeed probably didn’t want to stop them from doing it. Heck, we did it while we were building it. We kept IMing URLs to each other. And that’s how we realized we had a problem. All our work to minimize the impact on someone wrongly accused of a crime could be damaged by someone sharing a link on Facebook or Twitter.\nThere’s a difference between frictionless and unobstructed sharing and some reasonable constraints.\nWe couldn’t stop people from posting a mug on Facebook, but we didn’t have to make it easy and we didn’t have to put that mug front and center. So we blocked Facebook from using the mug as the thumbnail image on a shared link. And, after 60 days, the URL to the mug will throw a 404 page not found error. Because it’s gone.\nWe couldn’t block Google from memorializing someone’s arrest, only to let it live on forever on Facebook.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Ethics in data journalism</span>"
    ]
  },
  {
    "objectID": "ethics.html#you-are-a-data-provider",
    "href": "ethics.html#you-are-a-data-provider",
    "title": "9  Ethics in data journalism",
    "section": "9.4 You Are a Data Provider",
    "text": "9.4 You Are a Data Provider\nThe last problem didn’t come until months later. And it came in the middle of the night. Two months after we launched, my phone rang at 1 a.m. This is never a good thing. It was my fellow developer, Jeremy Bowers, now with NPR, calling me from a hotel in Washington DC where he was supposed to appear in a wedding the next day. Amazon, which we were using for image hosting, was alerting him that our bandwidth bills had tripled on that day. And our traffic hadn’t changed.\nWhat was going on?\nAfter some digging, we found out that another developer had scraped our site—because we were so much easier to scrape than the Sheriff’s office sites—and had built a game out of our data called Pick the Perp. There were two problems with this: 1. The game was going viral on Digg (when it was still a thing) and Reddit. It was getting huge traffic. 2. That developer had hotlinked our images. He/she was serving them from our S3 account, which meant we were bearing the costs. And they were going up exponentially by the minute.\nWhat we didn’t realize when we launched, and what we figured out after Pick the Perp, was that we had become data provider, in a sense. We had done the hard work of getting the data out of a website and we put it into neat, semantic, easily digestible HTML. If you were after a stream of mugshots, why go through all the hassle of scraping four different sheriff’s office’s horrible HTML when you could just come get ours easily?\nWhoever built Pick the Perp, at least at the time, chose to use our site. But, in doing so, they also chose to hotlink images—use the URL of our S3 bucket, which cost us money—instead of hosting the images themselves.\nThat was a problem we hadn’t considered. People hotlink images all the time. And, until those images are deleted from our system, they’ll stay hotlinked somewhere.\nAmazon’s S3 has a system where you can attach a key to a file that expires after X period of time. In other words, the URL to your image only lasts 15 minutes, or an hour, or however long you decide, before it breaks. It gives you fine grained control over how long someone can use your image URL.\nSo at 3 a.m., after two hours of pulling our hair out, we figured out how to sync our image keys with our cache refreshes. So every 15 minutes, a url to an image expired and Pick the Perp came crashing down.\nWhile the Pick the Perp example is an easy one—it’s never cool to hotlink an image—it does raise an issue to consider. Because you are thinking carefully about how to build your app the right way doesn’t mean someone else will. And it doesn’t mean they won’t just go take your data from your site. So how could you deal with that? Make the data available as a download? Create an API that uses your same ethical constructs? Terms of service? All have pros and cons and are worth talking about before going forward.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Ethics in data journalism</span>"
    ]
  },
  {
    "objectID": "ethics.html#ethical-data",
    "href": "ethics.html#ethical-data",
    "title": "9  Ethics in data journalism",
    "section": "9.5 Ethical Data",
    "text": "9.5 Ethical Data\nWe live in marvelous times. The web offers you no end of tools to make things on the web, to put data from here on there, to make information freely available. But, we’re an optimistic lot. Developers want to believe that their software is being used only for good. And most people will use it for good. But, there are times where the data you’re working with makes people uncomfortable. Indeed, much of journalism is about making people uncomfortable, publishing things that make people angry, or expose people who don’t want to be exposed.\nWhat I want you to think about, before you write a line of code, is what does it mean to put your data on the internet? What could happen, good and bad? What should you do to be responsible about it?\nBecause it can have consequences.\nOn Dec. 23, the Journal News in New York published a map of every legal gun permit holder in their home circulation county. It was a public record. They put it into Google Fusion Tables and Google dutifully geocoded the addresses. It was a short distance to publication from there.\nWithin days, angry gun owners had besieged the newspaper with complaints, saying the paper had given criminals directions to people’s houses where they’d find valuable guns to steal. They said the paper had violated their privacy. One outraged gun owner assembled a list of the paper’s staff, including their home addresses, telephone numbers, email addresses and other details. The paper hired armed security to stand watch at the paper.\nBy February, the New York state legislature removed handgun permits from the public record, citing the Journal News as the reason.\nThere’s no end of arguments to be had about this, but the simple fact is this: The reason people were angry was because you could click on a dot on the map and see a name and an address. In Fusion Tables, removing that info window would take two clicks.\nBecause you can put data on the web does not mean you should put data on the web. And there’s a difference between a record being “public” and “in front of a large audience.”\nSo before you write the first line of code, ask these questions:\n\nThis data is public, but is it widely available? And does making it widely available and easy to use change anything?\nShould this data be searchable in a search engine?\nDoes this data expose information someone has a reasonable expectation that it would remain at least semi-private?\nDoes this data change over time?\nDoes this data expire?\nWhat is my strategy to update or delete data?\nHow easy should it be to share this data on social media?\nHow should I deal with other people who want this data? API? Bulk download?\n\nYour answers to these questions will guide how you build your app. And hopefully, it’ll guide you to better decisions about how to build an app with ethics in mind.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Ethics in data journalism</span>"
    ]
  },
  {
    "objectID": "publicrecords.html",
    "href": "publicrecords.html",
    "title": "10  Public records",
    "section": "",
    "text": "10.1 Federal law\nPublic records are the lifeblood of investigative reporting. They carry their own philosophical framework, in a manner of speaking.\nKeeping those things in mind as you navigate public records is helpful.\nYour access to public records and public meetings is a matter of the law. As a journalist, it is your job to know this law better than most lawyers. Which law applies depends on which branch of government you are asking. In addition to documents and other kinds of information, FOIA also provides access to structured datasets of the kind we’ll use in this class.\nThe Federal Government is covered by the Freedom of Information Act, or FOIA. FOIA is not a universal term. Do not use it if you are not talking to a federal agency. FOIA is a beacon of openness to the world. FOIA is deeply flawed and frustrating.\nWhy?\nThe law was enacted in 1966, but it’s still poorly understood by most federal employees, if not outright flouted by political appointees. Lawsuits are common.\nPost 9/11, the Bush administration rolled back many agency rules. Obama ordered a “presumption of openness” but followed it with some of the most restrictive policies ever seen. The Trump Administration, similar to the Obama administration, claims to be the most transparent administration, but has steadily removed records from open access and broadly denied access to records.\nResult? FOIA is in trouble.\nSPJ is a good resource.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Public records</span>"
    ]
  },
  {
    "objectID": "publicrecords.html#federal-law",
    "href": "publicrecords.html#federal-law",
    "title": "10  Public records",
    "section": "",
    "text": "There is no real timetable with FOIA. Requests can take months, even years.\nAs a journalist, you can ask that your request be expedited.\nGuess what? That requires review. More delays.\nExemptions are broad. National security, personal privacy, often overused.\nDenied? You can appeal. More delays.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Public records</span>"
    ]
  },
  {
    "objectID": "publicrecords.html#state-law",
    "href": "publicrecords.html#state-law",
    "title": "10  Public records",
    "section": "10.2 State law",
    "text": "10.2 State law\nStates are – generally – more open than the federal government. The distance between the government and the governed is smaller. Some states, like Florida and Texas, are very open. Others, like Virginia and Pennsylvania, are not. Maryland is somewhere in the middle.\nThese laws generally give you license to view – and obtain a copy of – a record held by a state or local government agency.\nWhat is a public record? Generally speaking, public records are information stored on paper or in an electronic format held by a state or local government agency, but each state has its own list of types of records – called “exemptions” – that are not subject to disclosure.\nIf a record has both exempt and non-exempt information mixed in, most states require an agency to disclose it after removing the exempt information, a process called “redaction.” Agencies aren’t required to create a record in order to fill your request.\nIn some states but not all – the public information law (or related case law) explicitly dictates that extracting a slice of a database doesn’t constitute creation of a record. Most states can charge you a reasonable fee for time spent retrieving or copying records, though many have provisions to waive those fees for journalists. Every state law operates on a different timeline. Some only require agencies respond in a “reasonable” time, but others spell out exactly how fast an agency must respond to you, and how fast they must turn over the record.\nThe Reporters Committee For Freedom of the Press has a good resource for learning the law in your state.\nPlease and thank you will get you more records than any lawyer or well-written request. Be nice. Be polite. And be persistent. Following up regularly to check on status of a request lets an agency know they can’t ignore you (and some will try). Hunting for records is like any other kind of reporting – you have to do research. You have to ask questions. Ask them: What records do you keep? For how long?\nWhen requesting data, you are going to scare the press office and you are going to confuse the agency lawyer. Request to have their data person on the phone.\nA good source of info? Records retention schedules, often required by law or administrative rule at an agency. Here’s an example from Maryland’s Circuit Courts.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Public records</span>"
    ]
  },
  {
    "objectID": "github.html",
    "href": "github.html",
    "title": "11  Using GitHub",
    "section": "",
    "text": "11.1 How It Works\nGitHub is a platform for managing and storing files, data and code built atop Git, a popular open source version control software. GitHub accounts are free and it’s easy to get started. The one prerequisite is that you have Git installed on your local computer. There are installers for Mac, Windows and Linux.\nVersion control is based on the ideas that you want to keep track of changes you make to a collection of files and that multiple people can work together without getting in each other’s way or having to do things in a set order. For individual users, it’s great for making sure that you always have your work.\nGitHub users work in what are known as repositories on their local computers and also push changes to a remote repository located on GitHub. That remote repository is key: if you lose your computer, you can fetch a version of your files from GitHub. If you want to work with someone else on the same files, you can each have a local copy, push changes to GitHub and then pull each others’ changes back to your local computers.\nSo, like Microsoft Word’s track changes but with a remote backup and multiple editors.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Using GitHub</span>"
    ]
  },
  {
    "objectID": "github.html#getting-started",
    "href": "github.html#getting-started",
    "title": "11  Using GitHub",
    "section": "11.2 Getting Started",
    "text": "11.2 Getting Started\nAfter installing Git and signing up for a GitHub account, download and install GitHub Desktop. It will have you sign into your GitHub account and then you’ll have access to any existing repositories. If you don’t have any, that’s fine! You can make one locally.\nGitHub has good documentation for working in the Desktop app, and while the emphasis in this book will be on using GitHub for version control, it also supports recording issues (read: problems or questions) with your files, contributing to projects that aren’t yours and more.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Using GitHub</span>"
    ]
  },
  {
    "objectID": "github.html#advanced-use",
    "href": "github.html#advanced-use",
    "title": "11  Using GitHub",
    "section": "11.3 Advanced Use",
    "text": "11.3 Advanced Use\nAlthough our focus is on the GitHub Desktop app, you can use Git and GitHub from your computer’s command line interface, and GitHub has a purpose-built command line client, too. GitHub can also serve as a publishing platform for many types of files, and entire websites are hosted on GitHub Pages.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Using GitHub</span>"
    ]
  },
  {
    "objectID": "r-basics.html",
    "href": "r-basics.html",
    "title": "12  R Basics",
    "section": "",
    "text": "12.1 About libraries\nR is a programming language, one specifically geared toward data analysis.\nLike all programming languages, it has certain built-in functions.\nThere are many ways you can write and execute R code. The first, and most basic, is the console, shown here as part of a software tool called RStudio (Desktop Open Source Edition) that we’ll be using all semester.\nThink of the console like talking directly to the R language engine that’s busy working inside your computer. You use it send R commands, making sure to use the only language it understands, which is R. The R language engine processes those commands, and sends information back to you.\nUsing the console is direct, but it has some drawbacks and some quirks we’ll get into later. Let’s examine a basic example of how the console works.\nIf you load up R Studio, type 2+2 into the console and hit enter it will spit out the number 4, as displayed below.\nIt’s not very complex, and you knew the answer before hand, but you get the idea. With R, we can compute things.\nWe can also store things for later use under a specific name. In programming languages, these are called variables. We can assign things to variables using this left-facing arrow: &lt;-. The &lt;- is a called an assignment operator.\nIf you load up R studio and type this code in the console…\n…and then type this code, it will spit out the number 4, as show below.\nWe can have as many variables as we can name. We can even reuse them (but be careful you know you’re doing that or you’ll introduce errors).\nIf you load up R studio and type this code in the console…\n…and then type this, it will split out the number 6, as shown below.\nWe can store anything in a variable. A whole table. A list of numbers. A single word. A whole book. All the books of the 18th century. Variables are really powerful. We’ll explore them at length.\nA quick note about the console: After this brief introduction, we won’t spend much time in R Studio actually writing code directly into the console. Instead, we’ll write code in fancied-up text files – interchangably called R Markdown or R Notebooks – as will be explained in the next chapter. But that code we write in those text files will still execute in the console, so it’s good to know how it works.\nThe real strength of any programming language is the external libraries (often called “packages”) that power it. The base language can do a lot, but it’s the external libraries that solve many specific problems – even making the base language easier to use.\nWith R, there are hundreds of free, useful libraries that make it easier to do data journalism, created by a community of thousands of R users in multiple fields who contribute to open-source coding projects.\nFor this class, we’ll make use of several external libraries.\nMost of them are part of a collection of libraries bundled into one “metapackage” called the Tidyverse that streamlines tasks like:\nTo install packages, we use the function install.packages().\nYou only need to install a library once, the first time you set up a new computer to do data journalism work. You never need to install it again, unless you want to update to a newer version of the package.\nTo install all of the Tidyverse libraries at once, the function is install.packages('tidyverse'). You can type it directly in the console.\nTo use the R Markdown files mentioned earlier, we also need to install a Tidyverse-related library that doesn’t load as part of the core Tidyverse package. The package is called, conveniently, rmarkdown. The code to install that is install.packages('rmarkdown')",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#about-libraries",
    "href": "r-basics.html#about-libraries",
    "title": "12  R Basics",
    "section": "",
    "text": "Loading data into R. (We’ll use the readr Tidyverse library)\nCleaning and reshaping the data before analysis. (We’ll use the the tidyr and dplyr Tidyverse libraries)\nData analysis. (We’ll use the dplyr Tidyverse library)\nData visualization (We’ll use the ggplot2 Tidyverse library)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "replication.html",
    "href": "replication.html",
    "title": "13  Data journalism in the age of replication",
    "section": "",
    "text": "13.1 The stylebook\nA single word in a single job ad for Buzzfeed News posted in 2017 offered an indication of a profound shift in how data journalism is both practiced and taught.\n“We’re looking for someone with a passion for news and a commitment to using data to find amazing, important stories — both quick hits and deeper analyses that drive conversations,” the posting seeking a data journalist says. It goes on to list five things BuzzFeed is looking for: Excellent collaborator, clear writer, deep statistical understanding, knowledge of obtaining and restructuring data.\nAnd then there’s this:\n“You should have a strong command of at least one toolset that (a) allows for filtering, joining, pivoting, and aggregating tabular data, and (b) enables reproducible workflows.”\nThe word you’re seeing more and more of? Reproducible. And it started in earnest in 2017 when data journalism crossed a major threshold in American journalism: It got it’s own section in the Associated Press Stylebook.\n“Data journalism has become a staple of reporting across beats and platforms,” the Data Journalism section of the Stylebook opens. “The ability to analyze quantitative information and present conclusions in an engaging and accurate way is no longer the domain of specialists alone.”\nThe AP’s Data Journalism section discusses how to request data and in what format, guidelines for scraping data from websites with automation, the ethics of using leaked or hacked data and other topics long part of data journalism conference talks.\nBut the third page of the section contains perhaps the most profound commandment: “As a general rule, all assertions in a story based on data analysis should be reproducible. The methodology description in the story or accompanying materials should provide a road map to replicate the analysis.”\nReproducible research – replication – is a cornerstone of scientific inquiry. Researchers across a range of academic disciplines use methods to find new knowledge and publish it in peer reviewed journals. And, when it works, other researchers take that knowledge and try it with their own samples in their own locations. Replication studies exist to take something from an “interesting finding” to a “theory” and beyond.\nIt doesn’t always work.\nReplication studies aren’t funded at nearly the level as new research. And, to the alarm of many, scores of studies can’t be replicated by others. Researchers across disciplines are finding that when their original studies are replicated, flaws are found, or the effects found aren’t as strong as the original. Because of this, academics across a number of disciplines have written about a replication crisis in their respective fields, particularly psychology, social science and medical research.\nIn Chapter 1 of the New Precision Journalism, Phil Meyer wrote that “we journalists would be wrong less often if we adapted to our own use some of the research tools of the social scientists.”\nMeyer would go on to write about how computers pouring over datasets too large to crunch by hand had changed social science from a discipline with “a few data and a lot of interpretation” into a much more meaningful and powerful area of study. If journalists could become comfortable with data and some basic statistics, they too could harness this power.\n“It used to be said that journalism is history in a hurry,” Meyer wrote. “The argument of this book is that to cope with the acceleration of social change in today’s world, journalism must become social science in a hurry.”\nHe wrote that in 1971. It might as well have been yesterday.\nJournalism doesn’t have a history of replication, but the concerns about credibility are substantially greater. Trust in media is at an all time low and shows no signs of improving. While the politics of the day have quite a bit to do with this mistrust of media, being more transparent about what journalists do can’t hurt.\nThe AP’s commandment that “Thou must replicate your findings” could, if taken seriously by the news business, have substantial impacts on how data journalism gets done in newsrooms and how data journalism gets taught, both at professional conferences and universities.\nHow? Two ways.\nIf the AP’s replication rules are to be followed, journalism needs to become much more serious about the tools and techniques used to do data journalism. The days of “point and click” tools to do “quick and dirty” analysis that get published are dying. The days of formal methods using documented steps are here.\nTroy Thibodeaux, the editor of the AP’s data journalism team, said the stylebook entry started when the data team found themselves answering the same questions over and over. With a grant from the Knight Foundation, the team began to document their own standards and turn that into a stylebook section.\nFrom the beginning, they had a fairly clear idea of what they wanted to do – think through a project and ask what the frequently asked questions are that came up. It was not going to be a soup-to-nuts guide to how to do a data project.\nWhen the section came out, eyebrows went up on the replication parts, surprising Thibodeaux.\n“From our perspective, this is a core value for us,” he said. “Just for our own benefit, we need to be able to have someone give us a second set of eyes. We benefit from that every day. We catch things for each other.”\nThibodeaux said the AP data team has two audiences when it comes to replication – they have the readers of the work, and members of the collective who may want to do their own work with the data.\n“This is something that’s essential to the way we work,” he said. “And it’s important in terms of transparency and credibility going forward. We thought it would be kind of unexceptionable.”",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data journalism in the age of replication</span>"
    ]
  },
  {
    "objectID": "replication.html#replication",
    "href": "replication.html#replication",
    "title": "13  Data journalism in the age of replication",
    "section": "13.2 Replication",
    "text": "13.2 Replication\nMeyer, now 86, said he’s delighted to see replication up for discussion now, but warned that we shouldn’t take it too far.\n“Making the analysis replicable was something I worried about from the very beginning,” he wrote in an email. So much so that in 1967, after publishing stories from his landmark survey after the Detroit riots, he shipped the data and backup materials about it to a social science data repository at the University of North Carolina.\nAnd, in doing so, he opened the door to others replicating his results. One scholar attempted to find fault with Meyer’s analysis by slicing the data ever thinner until the differences weren’t significant – gaming the analysis to criticize the stories.\nMeyer believes replication is vitally important, but doesn’t believe it should take on the trappings of science replication, where newsrooms take their own samples or re-survey a community. That would be prohibitively expensive.\nBut journalists should be sharing their data and analysis steps. And it doesn’t need to be complicated, he said.\n“Replication is a theoretical standard, not a requirement that every investigator duplicate his or her own work for every project,” he said. “Giving enough information in the report to enable another investigator to follow in your footsteps is enough. Just telling enough to make replication possible will build confidence.”\nBut as simple as that sounds, it’s not so simple. Ask social scientists.\nAndrew Gelman, a professor of statistics and political science and director of the Applied Statistics Center at Columbia University, wrote in the journal CHANCE that difficulties with replication in empirical research are pervasive.\n“When an outsider requests data from a published paper, the authors will typically not post or send their data files and code, but instead will point to their sources, so replicators have to figure out exactly what to do from there,” Gelman wrote. “End-to-end replicability is not the norm, even among scholars who actively advocate for the principles of open science.”\nSo goes science, so goes journalism.\nUntil a recent set of exceptions, journalists rarely shared data. The “nerd box” – a sidebar story that explains how a news organization did what they did – is a term that first appeared on NICAR-L, a email listserv of data journalists, in the 1990s.\nIt was a form born in print.\nAs newsrooms adapted to the internet, some news organizations began linking to their data sources if they were online. Often, the data used in stories were obtained through records requests. Sometimes, reporters created the data themselves.\nJournalism, more explicitly than science, is a competitive business. There have been arguments that nerd boxes and downloadable links give too much away to competitors.\nEnter the AP Stylebook.\nThe AP Stylebook argues explicitly for both internal and external replication. Externally, they argue that the “methodology description in the story or accompanying materials should provide a road map to replicate the analysis”, meaning someone else could do the replication post publication.\nInternally, the AP Stylebook says: “If at all possible, an editor or another reporter should attempt to reproduce the results of the analysis and confirm all findings before publication.”\nThere are two problems here.\nFirst is that journalism, unlike science, has no history of replication. There is no “scientific method” for stories. There is no standard “research methods” class taught at every journalism school, at least not where it comes to writing stories. And, beyond that, journalism school isn’t a requirement to get into the news business. In other words, journalism lacks the standards other disciplines have.\nThe second problem is, in many ways, worse: Except for the largest newsrooms, most news organizations lack editors who could replicate the analysis. Many don’t have a second person who would know what to do.\nNot having a second set of eyes in a newsroom is a problem, Thibodeaux acknowledges. Having a data journalism team “is an incredible luxury” at the AP, he said, and their rule is nothing goes on the wire without a second set of eyes.\nThibodeaux, for his part, wants to see fewer “lone nerds in the corner” – it’s too much pressure. That person gets too much credibility from people who don’t understand what they do, and they get too much blame when a mistake is made.\nSo what would replication look like in a newsroom? What does this mean for how newsrooms do data journalism on deadline?",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data journalism in the age of replication</span>"
    ]
  },
  {
    "objectID": "replication.html#goodbye-excel",
    "href": "replication.html#goodbye-excel",
    "title": "13  Data journalism in the age of replication",
    "section": "13.3 Goodbye Excel?",
    "text": "13.3 Goodbye Excel?\nFor decades, Excel has been the gateway drug for data journalists, the Swiss Army knife of data tools, the “One Tool You Can’t Live Without.” Investigative Reporters and Editors, an organization that trains investigative journalists, have built large amounts of their curricula around Excel. Of the journalism schools that teach data journalism, most of them begin and end with spreadsheets.\nThe Stylebook says at a minimum, today’s data journalists should keep a log that details:\n\nThe source of the data, making sure to work on a copy of the data and not the original file.\nData dictionaries or any other supporting documentation of the data.\n“Description of all steps required to transform the data and perform the analysis.”\n\nThe trouble with Excel (or Google Sheets) is, unless you are keeping meticulous notes on what steps you are taking, there’s no way to keep track. Many data journalists will copy and paste the values of a formula over the formula itself to prevent Excel from fouling up cell references when moving data around – a practical step that also cuts off another path to being able to replicate the results.\nAn increasing number of data journalists are switching to tools like analysis notebooks, which use languages like Python and R, to document their work. The notebooks, generally speaking, allow a data journalist to mix code and explanation in the same document.\nCombined with online sharing tools like GitHub, analysis notebooks seem to solve the problem of replication. But the number using them is small compared to those using spreadsheets. Recent examples of news organizations using analysis notebooks include the Los Angeles Times, the New York Times, FiveThirtyEight, and Buzzfeed.\nPeter Aldous, a data journalist at Buzzfeed recently published a story about how the online news site used machine learning to find airplanes being used to spy on people in American cities. Published with the story is the code Aldous used to build his case.\n“I think of it this way: As a journalist, I don’t like to simply trust what people tell me. Sometimes sources lie. Sometimes they’re just mistaken. So I like to verify what I’m told,” he wrote in an email. “By the same token, why should someone reading one of my articles believe my conclusions, if I don’t provide the evidence that explains how I reached them?”\nThe methodology document, associated code and source data took Aldous a few hours to create. The story, from the initial data work through the reporting required to make sense of it all, took a year. Aldous said there wasn’t a discussion about if the methodology would be published because it was assumed – “it’s written into our DNA at BuzzFeed News.”\n“My background is in science journalism, and before that (way back in the 1980s) in science,” Aldous said. “In science, there’s been a shift from descriptive methods sections to publishing data and analysis code for reproducible research. And I think we’re seeing a similar shift in data journalism. Simply saying what you’ve done is not as powerful as providing the means for others to repeat and build on your work.”\nThibodeaux said that what Buzzfeed and others do with analysis notebooks and code repositories that include their data is “lovely.”\n“That to me is the shining city on the hill,” Thibodeaux said. “We’re not going to get there, and I don’t think we have to for every story and every use case, and I don’t think it’s necessarily practical for every person working with data to get to that point.”\nThere’s a wide spectrum of approaches that still gets journalists to the essence of what the stylebook is trying to do, Thibodeaux said. There are many tools, many strategies, and the AP isn’t going to advocate for any single one of them, he said. They’re just arguing for transparency and replicability, even if that means doing more work.\n“There’s a certain burden that comes with transparency,” he said. “And I think we have to accept that burden.”\nThe question, Thibodeaux said, is what is sufficient? What’s enough transparency? What does someone need for replicability?\n“Maybe we do have to set a higher standard – the more critical the analysis is to the story, and the more complex that analysis is, that’s going to push the bar on what is a sufficient methodology statement,” he said. “And it could end up being a whole code repo in order to just say, this isn’t black magic, here’s how we got it if you’re so interested.”",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data journalism in the age of replication</span>"
    ]
  },
  {
    "objectID": "replication.html#receptivity-is-high",
    "href": "replication.html#receptivity-is-high",
    "title": "13  Data journalism in the age of replication",
    "section": "13.4 “Receptivity … is high”",
    "text": "13.4 “Receptivity … is high”\nThough written almost half a century ago, Meyer foresaw how data journalism was going to arrive in the newsroom.\n“For the new methods to gain currency in journalism, two things must happen,” he wrote. “Editors must feel the need strongly enough to develop the in-house capacity for systematic research … The second need, of course, is for the editors to be able to find the talent to fill this need.”\nMeyer optimistically wrote that journalism schools were prepared to provide that talent – they were not then, and only small handful are now – but students were unlikely to be drawn to these new skills if they didn’t see a chance to use those skills in their careers.\nIt’s taken 45 years, but we are now at this point.\n“The potential for receptivity, especially among the younger generation of newspaper managers, is high,” Meyer wrote.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data journalism in the age of replication</span>"
    ]
  },
  {
    "objectID": "replication.html#replication-in-notebooks",
    "href": "replication.html#replication-in-notebooks",
    "title": "13  Data journalism in the age of replication",
    "section": "13.5 Replication in notebooks",
    "text": "13.5 Replication in notebooks\nFor our purposes in this book, replication requires two things from you, the student: What and why. What is this piece of code doing, and why are you doing that here and now? What lead you to this place? That you can copy and paste code from this book or the internet is not impressive. What is necessary for learning is that you know what a piece of code is doing a thing and why you want to do that thing here.\nHow will we replicate? We’ll make use of special text files – R Markdown, also known as R Notebooks – that combine contextual text; the code we use to load, clean, analyze and visualize data; and the output of that code that allowed us to draw certain conclusions to use in stories.\nIn an R Notebook, there are two blocks: A block that uses markdown, which has no special notation, and a code block. The code blocks can run mulitple languages inside R Studio. There’s R, of course, but we could also run Python, a general purpose scripting language; and SQL, or Structured Query Language, the language of databases.\nFor the rest of the class, we’re going to be working in notebooks.\nIn notebooks, you will both run your code and explain each step, much as I am doing here in this online book. This entire book was produced with R markdown files.\nTo start a notebook in R Studio, you click on the green plus in the top left corner and go down to R Notebook.\n\nIn our first lab, we’ll go through the process of editing a markdown notebook.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data journalism in the age of replication</span>"
    ]
  },
  {
    "objectID": "aggregates.html",
    "href": "aggregates.html",
    "title": "14  Aggregates",
    "section": "",
    "text": "14.1 Libraries\nR is a statistical programming language that is purpose built for data analysis.\nBase R does a lot, but there are a mountain of external libraries that do things to make R better/easier/more fully featured. We already installed the tidyverse – or you should have if you followed the instructions for the last assignment – which isn’t exactly a library, but a collection of libraries. Together, they make up the Tidyverse. Individually, they are extraordinarily useful for what they do. We can load them all at once using the tidyverse name, or we can load them individually. Let’s start with individually.\nThe two libraries we are going to need for this assignment are readr and dplyr. The library readr reads different types of data in. For this assignment, we’re going to read in csv data or Comma Separated Values data. That’s data that has a comma between each column of data.\nThen we’re going to use dplyr to analyze it.\nTo use a library, you need to import it. Good practice – one I’m going to insist on – is that you put all your library steps at the top of your notebooks.\nThat code looks like this:\nlibrary(readr)\nTo load them both, you need to do this:\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\nBut, because those two libraries – and several others that we’re going to use over the course of this class – are so commonly used, there’s a shortcut to loading all of the libraries we’ll need:\nlibrary(tidyverse)\nYou can keep doing that for as many libraries as you need.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Aggregates</span>"
    ]
  },
  {
    "objectID": "aggregates.html#importing-data",
    "href": "aggregates.html#importing-data",
    "title": "14  Aggregates",
    "section": "14.2 Importing data",
    "text": "14.2 Importing data\nThe first thing we need to do is get some data to work with. We do that by reading it in. In our case, we’re going to read a datatable from a CSV file. A CSV is a stripped down version of a spreadsheet you might open in a program like Excel, in which each column is separated by a comma. There are other types of files that R can read, but CSVs are common and mostly easy to work with.\nThe CSV we’re going to work with is a list of tax-exempt churches in Maryland, from the Internal Revenue Service. It has information like the name of the organization, its address, city, state, zip code and different codes describing details about its status and organization. More information about those codes is available in the IRS documentation for exempt organizations, which you can find here.\nSo step 1 is to import the data. The code to import the data looks like this:\nmd_churches &lt;- read_csv(\"data/churches_md.csv\")\nLet’s unpack that.\nThe first part – md_churches – is the name of a variable we are creating.\nA variable is just a name that we’ll use to refer to some more complex thing. In this case, the more complex thing is the data we’re importing into R that will be stored as a dataframe, which is one way R stores data.\nWe can call this variable whatever we want. The variable name doesn’t matter, technically. We could use any word. You could use your first name, if you like. Generally, though, we want to give variables names that are descriptive of the thing they refer to. Which is why we’re calling this one md_churches. Variable names, by convention are one word all lower case (or two or more words connected by an underscore). You can end a variable with a number, but you can’t start one with a number.\nThe &lt;- bit, you’ll recall from the basics, is the variable assignment operator. It’s how we know we’re assigning something to a word. Think of the arrow as saying “Take everything on the right of this arrow and stuff it into the thing on the left.” So we’re creating an empty vessel called md_churches and stuffing all this data into it.\nread_csv() is a function, one that only works when we’ve loaded the tidyverse. A function is a little bit of computer code that takes in information and follows a series of pre-determined steps and spits it back out. A recipe to make pizza is a kind of function. We might call it make_pizza().\nThe function does one thing. It takes a preset collection of ingredients – flour, water, oil, cheese, tomato, salt – and passes them through each step outlined in a recipe, in order. Things like: mix flour and water and oil, knead, let it sit, roll it out, put tomato sauce and cheese on it, bake it in an oven, then take it out.\nThe output of our make pizza() function is a finished pie.\nWe’ll make use of a lot of pre-written functions from the tidyverse and other packages, and even write some of our own. Back to this line of code:\nmd_churches &lt;- read_csv(\"data/churches_md.csv\")\nInside of the read_csv() function, we’ve put the name of the file we want to load. Things we put inside of function, to customize what the function does, are called arguments.\nThe easiest thing to do, if you are confused about how to find your data, is to put your data in the same folder as as your notebook (you’ll have to save that notebook first). If you do that, then you just need to put the name of the file in there. If you put your data in a folder called “data” that sits next to your data notebook, your function would instead look like this:\n\nmd_churches &lt;- read_csv(\"data/churches_md.csv\")\n\nRows: 7314 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (16): EIN, NAME, ICO, STREET, CITY, STATE, ZIP, GROUP, SUBSECTION, RULIN...\ndbl (13): AFFILIATION, CLASSIFICATION, DEDUCTIBILITY, FOUNDATION, ORGANIZATI...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nIn this data set, each row represents a different church. Each column represents a different attribute of that church – its name, address, city, state, zip code and so on.\nAfter loading the data, it’s a good idea to get a sense of its shape. What does it look like? There are several ways we can examine it.\nBy looking in the R Studio environment window, we can see the number of rows (called “obs.”, which is short for observations), and the number of columns (called variables). We can double click on the dataframe name in the environment window, and explore it like a spreadsheet.\nThere are several useful functions for getting a sense of the dataset right in our markdown document.\nIf we run glimpse(md_churches), it will give us a list of the columns, the data type for each column and and the first few values for each column.\n\nglimpse(md_churches)\n\nRows: 7,314\nColumns: 29\n$ EIN              &lt;chr&gt; \"010060379\", \"010549919\", \"010549978\", \"010552565\", \"…\n$ NAME             &lt;chr&gt; \"HEALING AND DELIVERANCE MINISTRY INC\", \"TIME OF GRAC…\n$ ICO              &lt;chr&gt; NA, \"% MIGUEL A ROMERO\", \"% CARNAL BRADLEY PASTOR\", N…\n$ STREET           &lt;chr&gt; \"14103 BYRNE PARK DR\", \"501 ROSEMERE AVE\", \"5611 LANI…\n$ CITY             &lt;chr&gt; \"CLARKSBURG\", \"SILVER SPRING\", \"SUITLAND\", \"CROWNSVIL…\n$ STATE            &lt;chr&gt; \"MD\", \"MD\", \"MD\", \"MD\", \"MD\", \"MD\", \"MD\", \"MD\", \"MD\",…\n$ ZIP              &lt;chr&gt; \"20871-6337\", \"20904-3020\", \"20746-6216\", \"21032-0797…\n$ GROUP            &lt;chr&gt; \"0000\", \"0000\", \"0000\", \"1601\", \"0000\", \"8534\", \"1709…\n$ SUBSECTION       &lt;chr&gt; \"03\", \"03\", \"03\", \"03\", \"03\", \"03\", \"03\", \"03\", \"03\",…\n$ AFFILIATION      &lt;dbl&gt; 3, 3, 3, 9, 3, 9, 9, 3, 9, 3, 9, 3, 3, 3, 3, 3, 3, 3,…\n$ CLASSIFICATION   &lt;dbl&gt; 7000, 7000, 1000, 7000, 7000, 7000, 7000, 7000, 7000,…\n$ RULING           &lt;chr&gt; \"201407\", \"200406\", \"200207\", \"196310\", \"200203\", \"19…\n$ DEDUCTIBILITY    &lt;dbl&gt; 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ FOUNDATION       &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1…\n$ ACTIVITY         &lt;chr&gt; \"000000000\", \"000000000\", \"000000000\", \"029002000\", \"…\n$ ORGANIZATION     &lt;dbl&gt; 5, 1, 1, 5, 1, 1, 5, 1, 5, 1, 5, 1, 1, 1, 1, 1, 1, 1,…\n$ STATUS           &lt;chr&gt; \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\",…\n$ TAX_PERIOD       &lt;dbl&gt; NA, 202412, NA, NA, NA, NA, NA, NA, 202412, 202412, N…\n$ ASSET_CD         &lt;dbl&gt; 0, 7, 1, 0, 0, 0, 0, 0, 6, 5, 0, 0, 0, 0, 4, 0, 0, 0,…\n$ INCOME_CD        &lt;dbl&gt; 0, 6, 2, 0, 0, 0, 0, 0, 6, 4, 0, 0, 0, 0, 4, 0, 0, 0,…\n$ FILING_REQ_CD    &lt;chr&gt; \"06\", \"06\", \"06\", \"06\", \"06\", \"06\", \"06\", \"06\", \"06\",…\n$ PF_FILING_REQ_CD &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ ACCT_PD          &lt;chr&gt; \"12\", \"12\", \"12\", \"12\", \"12\", \"12\", \"12\", \"12\", \"12\",…\n$ ASSET_AMT        &lt;dbl&gt; NA, 5045077, NA, NA, NA, NA, NA, NA, 2876599, 665794,…\n$ INCOME_AMT       &lt;dbl&gt; NA, 1258914, NA, NA, NA, NA, NA, NA, 1034272, 103537,…\n$ REVENUE_AMT      &lt;dbl&gt; NA, 1258914, NA, NA, NA, NA, NA, NA, 1034272, 103537,…\n$ NTEE_CD          &lt;chr&gt; \"X20\", \"X21\", \"X20\", NA, \"X21\", NA, NA, \"X20\", NA, \"X…\n$ SORT_NAME        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"KENNETH HARR…\n$ zip5             &lt;dbl&gt; 20871, 20904, 20746, 21032, 20833, 21078, 20910, 2064…\n\n\nIf we type head(md_churches), it will print out the columns and the first six rows of data.\n\nhead(md_churches)\n\n# A tibble: 6 × 29\n  EIN       NAME     ICO   STREET CITY  STATE ZIP   GROUP SUBSECTION AFFILIATION\n  &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;\n1 010060379 HEALING… &lt;NA&gt;  14103… CLAR… MD    2087… 0000  03                   3\n2 010549919 TIME OF… % MI… 501 R… SILV… MD    2090… 0000  03                   3\n3 010549978 THE PLA… % CA… 5611 … SUIT… MD    2074… 0000  03                   3\n4 010552565 PATAPSC… &lt;NA&gt;  PO BO… CROW… MD    2103… 1601  03                   9\n5 010562898 SOLID U… % WE… 3001 … BROO… MD    2083… 0000  03                   3\n6 010572203 CHESAPE… % RE… 440 D… HVRE… MD    2107… 8534  03                   9\n# ℹ 19 more variables: CLASSIFICATION &lt;dbl&gt;, RULING &lt;chr&gt;, DEDUCTIBILITY &lt;dbl&gt;,\n#   FOUNDATION &lt;dbl&gt;, ACTIVITY &lt;chr&gt;, ORGANIZATION &lt;dbl&gt;, STATUS &lt;chr&gt;,\n#   TAX_PERIOD &lt;dbl&gt;, ASSET_CD &lt;dbl&gt;, INCOME_CD &lt;dbl&gt;, FILING_REQ_CD &lt;chr&gt;,\n#   PF_FILING_REQ_CD &lt;dbl&gt;, ACCT_PD &lt;chr&gt;, ASSET_AMT &lt;dbl&gt;, INCOME_AMT &lt;dbl&gt;,\n#   REVENUE_AMT &lt;dbl&gt;, NTEE_CD &lt;chr&gt;, SORT_NAME &lt;chr&gt;, zip5 &lt;dbl&gt;\n\n\nWe can also click on the data name in the R Studio environment window to explore it interactively.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Aggregates</span>"
    ]
  },
  {
    "objectID": "aggregates.html#group-by-and-count",
    "href": "aggregates.html#group-by-and-count",
    "title": "14  Aggregates",
    "section": "14.3 Group by and count",
    "text": "14.3 Group by and count\nSo what if we wanted to know how many churches were in each city?\nTo do that by hand, we’d have to take each of the 7,314 individual rows (or observations or records) and sort them into a pile. We’d put them in groups – one for each city – and then count them.\ndplyr has a group by function in it that does just this. A massive amount of data analysis involves grouping like things together and then doing simple things like counting them, or averaging them together. So it’s a good place to start.\nSo to do this, we’ll take our dataset and we’ll introduce a new operator: |&gt;. The best way to read that operator, in my opinion, is to interpret that as “and then do this.” This is called the “pipe operator” and it’s a huge part of writing R statements. So much so that there’s a keyboard shortcut for this: cmd-shift-m on the Mac and ctrl-shift-m on Windows. In order to enable this shortcut, you’ll need to set an option under Tools -&gt; Global Options, in the “Code” section. Make sure you check the box labeled “Use native pipe operator” and then click “Apply”, like so:\n\nDon’t like that character? R also has one that does the same thing: %&gt;%. They both work.\nWe’re going to establish a pattern that will come up again and again throughout this book: data |&gt; function. In English: take your data set and then do this specific action to it.\nThe first step of every analysis starts with the data being used. Then we apply functions to the data.\nIn our case, the pattern that you’ll use many, many times is: data |&gt; group_by(COLUMN NAME) |&gt; summarize(VARIABLE NAME = AGGREGATE FUNCTION(COLUMN NAME))\nIn our dataset, the column with city information is called CITY.\nHere’s the code to count the number of churches in each city:\n\nmd_churches |&gt;\n  group_by(CITY) |&gt;\n  summarise(\n    count = n()\n  )\n\n# A tibble: 490 × 2\n   CITY          count\n   &lt;chr&gt;         &lt;int&gt;\n 1 ABERDEEN         35\n 2 ABINGDON         19\n 3 ACCIDENT          6\n 4 ACCOKEEK         31\n 5 ADAMSTOWN         3\n 6 ADELPHI          20\n 7 ANNAPOLIS        69\n 8 ANNAPOLIS JCT     1\n 9 ARBUTUS           2\n10 ARNOLD            7\n# ℹ 480 more rows\n\n\nSo let’s walk through that.\nWe start with our dataset – md_churches – and then we tell it to group the data by a given field in the data. In this case, we wanted to group together all the cities, signified by the field name CITY, which you could get from using the glimpse() function. After we group the data, we need to count them up.\nIn dplyr, we use the summarize() function, which can do alot more than just count things.\nInside the parentheses in summarize, we set up the summaries we want. In this case, we just want a count of the number of classes for each term grouping. The line of code count = n(), says create a new field, called count and set it equal to the value of n(). n() is a function that counts the number of rows or records in each group. Why the letter n? The letter n is a common symbol used to denote a count of something. The number of things (or rows or observations or records) in a dataset? Statisticians call it n. There are n number of cities in this dataset.\nWhen we run that, we get a list of cities with a count next to them. But it’s not in any order.\nSo we’ll add another “and then do this” symbol – |&gt; – and use a new function called arrange(). Arrange does what you think it does – it arranges data in order. By default, it’s in ascending order – smallest to largest. But if we want to know the city with the most churches, we need to sort it in descending order. That looks like this:\n\nmd_churches |&gt;\n  group_by(CITY) |&gt;\n  summarise(\n    count = n()\n  ) |&gt;\n  arrange(desc(count))\n\n# A tibble: 490 × 2\n   CITY          count\n   &lt;chr&gt;         &lt;int&gt;\n 1 BALTIMORE      1258\n 2 SILVER SPRING   402\n 3 BOWIE           167\n 4 UPPR MARLBORO   155\n 5 LAUREL          153\n 6 GAITHERSBURG    150\n 7 HYATTSVILLE     144\n 8 FREDERICK       135\n 9 WALDORF         120\n10 ROCKVILLE       118\n# ℹ 480 more rows\n\n\nBaltimore, not surprisingly, has the most churches, and it’s not close.\nWe can, if we want, group by more than one thing. The churches data contains a column detailing the five-digit zip code called zip5.\nWe can group by city and zip code, like this:\n\nmd_churches |&gt;\n  group_by(CITY, zip5) |&gt;\n  summarise(\n    count = n()\n  ) |&gt;\n  arrange(desc(count))\n\n`summarise()` has grouped output by 'CITY'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 666 × 3\n# Groups:   CITY [490]\n   CITY           zip5 count\n   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;\n 1 BALTIMORE     21215   117\n 2 LANHAM        20706   112\n 3 SILVER SPRING 20904   105\n 4 FT WASHINGTON 20744    94\n 5 CLINTON       20735    92\n 6 BALTIMORE     21218    85\n 7 CAPITOL HGTS  20743    85\n 8 BALTIMORE     21217    84\n 9 HAGERSTOWN    21740    84\n10 UPPR MARLBORO 20772    76\n# ℹ 656 more rows",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Aggregates</span>"
    ]
  },
  {
    "objectID": "aggregates.html#other-summarization-methods-summing-mean-median-min-and-max",
    "href": "aggregates.html#other-summarization-methods-summing-mean-median-min-and-max",
    "title": "14  Aggregates",
    "section": "14.4 Other summarization methods: summing, mean, median, min and max",
    "text": "14.4 Other summarization methods: summing, mean, median, min and max\nIn the last example, we grouped like records together and counted them, but there’s so much more we can do to summarize each group.\nTo do that, we’ll load an use a different CSV file - this one of “mega-churches”. These are churches with large congregations, often in the thousands. This data set comes from Hartford International University and contains information about the church name, city, state, attendance, denomination and website. Let’s load it and take a look at the first few rows:\n\nmegachurches &lt;- read_csv(\"data/megachurches.csv\")\n\nRows: 1666 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): church_name, church_url, city, state, denomination\ndbl (1): size\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(megachurches)\n\n# A tibble: 6 × 6\n  church_name                     church_url      city  state  size denomination\n  &lt;chr&gt;                           &lt;chr&gt;           &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;       \n1 12Stone Church                  http://www.12s… Lawr… GA    17000 Wesleyan    \n2 3Circle Church                  http://www.3ci… Fair… AL     4962 Independent…\n3 Abba’s House/Central Baptist    http://www.abb… Hixs… TN     1750 Southern Ba…\n4 Abundant Faith Christian Center http://www.abu… Spri… IL     2900 Unknown (un…\n5 Abundant Life Baptist Church    http://www.abu… Lee'… MO     2000 Independent…\n6 Abundant Life Cathedral Church  http://www.alc… Hous… TX     5000 Independent…\n\n\nNow let’s say we wanted to know the total attendance for each denomination? For that, we could use the sum() function to add up all of the values in the column “size”. We group by denomination and put the column we want to total – size – inside the sum() function sum(size) and give it a name – total_size – like this:\n\nmegachurches |&gt;\n  group_by(denomination) |&gt;\n  summarise(\n    total_size = sum(size)\n  ) |&gt;\n  arrange(desc(total_size))\n\n# A tibble: 59 × 2\n   denomination                   total_size\n   &lt;chr&gt;                               &lt;dbl&gt;\n 1 Independent, Nondenominational    2159987\n 2 Southern Baptist Convention        926905\n 3 Unknown (unspecified)              605158\n 4 Baptist (unspecified)              408527\n 5 CHRISTIAN                          385635\n 6 Assemblies of God                  372857\n 7 Calvary Churches                   188213\n 8 United Methodist Church            133170\n 9 Evangelical Covenant               117800\n10 Four Square                         94765\n# ℹ 49 more rows\n\n\nWe can also calculate the average size of megachurches in each denomination using the mean() function, and add the median, too, like this:\n\nmegachurches |&gt;\n  group_by(denomination) |&gt;\n  summarise(\n    total_size = sum(size),\n    mean_size = mean(size),\n    median_size = median(size)\n  ) |&gt;\n  arrange(desc(total_size))\n\n# A tibble: 59 × 4\n   denomination                   total_size mean_size median_size\n   &lt;chr&gt;                               &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n 1 Independent, Nondenominational    2159987     4615.        3000\n 2 Southern Baptist Convention        926905     3565.        2500\n 3 Unknown (unspecified)              605158     3498.        2500\n 4 Baptist (unspecified)              408527     3349.        2500\n 5 CHRISTIAN                          385635     4646.        3012\n 6 Assemblies of God                  372857     3421.        2689\n 7 Calvary Churches                   188213     5228.        3500\n 8 United Methodist Church            133170     2718.        2183\n 9 Evangelical Covenant               117800    16829.        3000\n10 Four Square                         94765     3645.        2650\n# ℹ 49 more rows\n\n\nWe see something interesting here. The mean number of seats is higher than the median number in most cases, but the difference isn’t always huge. In some cases the mean gets skewed by larger or lower amounts. Examining both the median – which is less sensitive to extreme values – and the mean – which is more sensitive to extreme values – gives you a clearer picture of the composition of the data.\nWhat about the highest and lowest sizes for each denomination? For that, we can use the min() and max() functions.\n\nmegachurches |&gt;\n  group_by(denomination) |&gt;\n  summarise(\n    total_size = sum(size),\n    mean_size = mean(size),\n    median_size = median(size),\n    min_size = min(size),\n    max_size = max(size)\n  ) |&gt;\n  arrange(desc(total_size))\n\n# A tibble: 59 × 6\n   denomination               total_size mean_size median_size min_size max_size\n   &lt;chr&gt;                           &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 Independent, Nondenominat…    2159987     4615.        3000     1800    60000\n 2 Southern Baptist Conventi…     926905     3565.        2500     1300    28000\n 3 Unknown (unspecified)          605158     3498.        2500     1800    14000\n 4 Baptist (unspecified)          408527     3349.        2500     1800    21000\n 5 CHRISTIAN                      385635     4646.        3012     1833    25917\n 6 Assemblies of God              372857     3421.        2689     1700    20000\n 7 Calvary Churches               188213     5228.        3500     1800    16830\n 8 United Methodist Church        133170     2718.        2183     1800     8000\n 9 Evangelical Covenant           117800    16829.        3000     2000    85000\n10 Four Square                     94765     3645.        2650     1800     9032\n# ℹ 49 more rows\n\n\nIt would be interesting to see what the church with the largest size is. To do that, we can just use arrange() on the whole dataset, like this:`\n\nmegachurches |&gt;\n  arrange(desc(size))\n\n# A tibble: 1,666 × 6\n   church_name                church_url          city  state  size denomination\n   &lt;chr&gt;                      &lt;chr&gt;               &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;       \n 1 Life.Church                http://www.life.ch… Edmo… OK    85000 Evangelical…\n 2 Church of the Highlands    http://www.churcho… Birm… AL    60000 Independent…\n 3 Lakewood Church            http://www.lakewoo… Hous… TX    45000 Independent…\n 4 Crossroads Church          http://www.crossro… Cinc… OH    35253 Independent…\n 5 Gateway Church             http://www.gateway… Sout… TX    28000 Independent…\n 6 Saddleback Church          http://www.saddleb… Lake… CA    28000 Southern Ba…\n 7 Elevation Church           http://www.elevati… Matt… NC    26000 Independent…\n 8 Southeast Christian Church http://www.southea… Loui… KY    25917 CHRISTIAN   \n 9 Christ Fellowship Church   http://www.christf… Palm… FL    25000 Independent…\n10 North Point Ministries     http://www.northpo… Alph… GA    24273 Independent…\n# ℹ 1,656 more rows\n\n\n85,000 people is a big congregation. What else could we ask of this data?",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Aggregates</span>"
    ]
  },
  {
    "objectID": "filters.html",
    "href": "filters.html",
    "title": "15  Filters and selections",
    "section": "",
    "text": "15.1 Combining filters\nMore often than not, we have more data than we want. Sometimes we need to be rid of that data. In dplyr, there’s two ways to go about this: filtering and selecting.\nFiltering creates a subset of the data based on criteria. All records where the amount is greater than 150,000. All records that match “College Park”. Something like that. Filtering works with rows – when we filter, we get fewer rows back than we start with.\nSelecting simply returns only the fields named. So if you only want to see city and amount, you select those fields. When you look at your data again, you’ll have two columns. If you try to use one of your columns that you had before you used select, you’ll get an error. Selecting works with columns. You will have the same number of records when you are done, but fewer columns of data to work with.\nLet’s continue to work with the Maryland churches data we used in the previous chapter. First, we need to load the tidyverse:\nIf we want to see only those churches located in a particular city, we can use the filter function to isolate just those records. Filter works with something called a comparison operator. We need to filter all records equal to “COLLEGE PARK”. The comparison operators in R, like most programming languages, are == for equal to, != for not equal to, &gt; for greater than, &gt;= for greater than or equal to and so on.\nBe careful: = is not == and = is not “equal to”. = is an assignment operator in most languages – how things get named.\nAnd just like that, we have just College Park results, which we can verify looking at the head, the first six rows.\nWe also have more data than we might want. For example, we may only want to work with the church name and street.\nTo simplify our dataset, we can use select.\nAnd now we only have two columns of data for whatever analysis we might want to do.\nSo let’s say we wanted to see all the churches in College Park that are independent - in other words, that aren’t affiliated with a larger denomination. The data we have comes from the Internal Revenue Service and has a code book describing the values of the columns. The AFFILIATION column says that a code of 3 means independent. We can do this a number of ways. The first is we can chain together a whole lot of filters.\ncp_churches_ind &lt;- md_churches |&gt; filter(CITY == \"COLLEGE PARK\") |&gt; filter(AFFILIATION == 3)\n\nnrow(cp_churches_ind)\n\n[1] 28\nThat gives us 28 records But that’s repetitive, no? We can do better using a single filter and boolean operators – AND and OR. In this case, AND is & and OR is |.\nThe difference? With AND, all conditions must be true to be included. With OR, any of those conditions things can be true and it will be included.\nHere’s the difference.\ncp_churches_ind &lt;- md_churches |&gt; filter(CITY == \"COLLEGE PARK\" & AFFILIATION == 3)\n\nnrow(cp_churches_ind)\n\n[1] 28\nSo AND gives us the same answer we got before. What does OR give us?\ncp_churches_ind &lt;- md_churches |&gt; filter(CITY == \"COLLEGE PARK\" | AFFILIATION == 3)\n\nnrow(cp_churches_ind)\n\n[1] 4875\nSo there’s 4,875 rows that are EITHER College Park churches OR are independent. OR is additive; AND is restrictive.\nA general tip about using filter: it’s easier to work your way towards the filter syntax you need rather than try and write it once and trust the result. Each time you modify your filter, check the results to see if they make sense. This adds a little time to your process but you’ll thank yourself for doing it because it helps avoid mistakes.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Filters and selections</span>"
    ]
  },
  {
    "objectID": "workingwithdates.html",
    "href": "workingwithdates.html",
    "title": "16  Working with dates",
    "section": "",
    "text": "16.1 Making dates dates again\nOne of the most frustrating things in data is working with dates. Everyone has a different opinion on how to record them, and every software package on the planet has to sort it out. Dealing with it can be a little … confusing. And every dataset has something new to throw at you. So consider this an introduction.\nFirst, there’s the right way to display dates in data. Most of the rest of the world knows how to do this, but Americans aren’t taught it. The correct way to display dates is the following format: YYYY-MM-DD, or 2022-09-15. Any date that looks different should be converted into that format when you’re using R.\nLuckily, this problem is so common that the Tidyverse has an entire library for dealing with it: lubridate.\nWe’re going to do this two ways. First I’m going to show you how to use base R to solve a tricky problem. And then we’ll use a library called lubridate to solve a more common and less tricky problem. And then we’ll use a new library to solve most of the common problems before they start. If it’s not already installed, just run install.packages('lubridate')\nFirst, we’ll import tidyverse like we always do and our newly-installed lubridate.\nlibrary(tidyverse)\nlibrary(lubridate)\nLet’s start with a dataset of campaign expenses from Maryland political committees:\nmaryland_expenses &lt;- read_csv(\"data/maryland_expenses.csv\")\n\nRows: 97912 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (12): expenditure_date, payee_name, address, payee_type, committee_name,...\ndbl  (1): amount\nlgl  (1): expense_toward\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(maryland_expenses)\n\n# A tibble: 6 × 14\n  expenditure_date payee_name           address payee_type amount committee_name\n  &lt;chr&gt;            &lt;chr&gt;                &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;         \n1 3/12/2021        &lt;NA&gt;                 &lt;NA&gt;    Reimburse     350 Salling   Joh…\n2 3/29/2021        Dundalk Eagle Newsp… PO Box… Business/…    329 Salling   Joh…\n3 4/29/2021        Dundalk Eagle Newsp… PO Box… Business/…    400 Salling   Joh…\n4 5/18/2021        Dundalk Eagle Newsp… PO Box… Business/…    350 Salling   Joh…\n5 6/9/2021         Dundalk Heritage Fa… Dundal… Business/…    200 Salling   Joh…\n6 6/9/2021         Dundalk Heritage Fa… Dundal… Business/…    250 Salling   Joh…\n# ℹ 8 more variables: expense_category &lt;chr&gt;, expense_purpose &lt;chr&gt;,\n#   expense_toward &lt;lgl&gt;, expense_method &lt;chr&gt;, vendor &lt;chr&gt;, fundtype &lt;chr&gt;,\n#   comments &lt;chr&gt;, x14 &lt;chr&gt;\nTake a look at that first column, expenditure_date. It looks like a date, but see the &lt;chr right below the column name? That means R thinks it’s actually a character column. What we need to do is make it into an actual date column, which lubridate is very good at doing. It has a variety of functions that match the format of the data you have. In this case, the current format is m/d/y, and the lubridate function is called mdy that we can use with mutate:\nmaryland_expenses &lt;- maryland_expenses |&gt; mutate(expenditure_date=mdy(expenditure_date))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `expenditure_date = mdy(expenditure_date)`.\nCaused by warning:\n!  18 failed to parse.\n\nhead(maryland_expenses)\n\n# A tibble: 6 × 14\n  expenditure_date payee_name           address payee_type amount committee_name\n  &lt;date&gt;           &lt;chr&gt;                &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;         \n1 2021-03-12       &lt;NA&gt;                 &lt;NA&gt;    Reimburse     350 Salling   Joh…\n2 2021-03-29       Dundalk Eagle Newsp… PO Box… Business/…    329 Salling   Joh…\n3 2021-04-29       Dundalk Eagle Newsp… PO Box… Business/…    400 Salling   Joh…\n4 2021-05-18       Dundalk Eagle Newsp… PO Box… Business/…    350 Salling   Joh…\n5 2021-06-09       Dundalk Heritage Fa… Dundal… Business/…    200 Salling   Joh…\n6 2021-06-09       Dundalk Heritage Fa… Dundal… Business/…    250 Salling   Joh…\n# ℹ 8 more variables: expense_category &lt;chr&gt;, expense_purpose &lt;chr&gt;,\n#   expense_toward &lt;lgl&gt;, expense_method &lt;chr&gt;, vendor &lt;chr&gt;, fundtype &lt;chr&gt;,\n#   comments &lt;chr&gt;, x14 &lt;chr&gt;\nNow look at the expenditure_date column: R says it’s a date column and it looks like we want it to: YYYY-MM-DD. Accept no substitutes.\nLubridate has functions for basically any type of character date format: mdy, ymd, even datetimes like ymd_hms.\nThat’s less code and less weirdness, so that’s good.\nBut to get clean data, I’ve installed a library and created a new field so I can now start to work with my dates. That seems like a lot, but don’t think your data will always be perfect and you won’t have to do these things.\nStill, there’s got to be a better way. And there is.\nFortunately, readr anticipates some date formatting and can automatically handle many of these issues (indeed it uses lubridate under the hood). When you are importing a CSV file, be sure to use read_csv, not read.csv.\nBut you’re not done with lubridate yet. It has some interesting pieces parts we’ll use elsewhere.\nFor example, in spreadsheets you can extract portions of dates - a month, day or year - with formulas. You can do the same in R with lubridate. Let’s say we wanted to add up the total amount spent in each month in our Maryland expenses data.\nWe could use formatting to create a Month field but that would group all the Aprils ever together. We could create a year and a month together, but that would give us an invalid date object and that would create problems later. Lubridate has something called a floor date that we can use.\nSo to follow along here, we’re going to use mutate to create a month field, group by to lump them together, summarize to count them up and arrange to order them. We’re just chaining things together.\nmaryland_expenses |&gt;\n  mutate(month = floor_date(expenditure_date, \"month\")) |&gt;\n  group_by(month) |&gt;\n  summarise(total_amount = sum(amount)) |&gt;\n  arrange(desc(total_amount))\n\n# A tibble: 25 × 2\n   month      total_amount\n   &lt;date&gt;            &lt;dbl&gt;\n 1 2022-10-01    15827467.\n 2 2022-09-01     6603431.\n 3 2022-08-01     5892055.\n 4 2022-11-01     4715694.\n 5 2021-07-01     2242692.\n 6 2021-09-01     2212083.\n 7 2021-08-01     2086313.\n 8 2021-06-01     1827400.\n 9 2021-05-01     1341210.\n10 2021-01-01      772923.\n# ℹ 15 more rows\nSo the month of June 2022 had the most expenditures by far in this data.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Working with dates</span>"
    ]
  },
  {
    "objectID": "mutating.html",
    "href": "mutating.html",
    "title": "17  Mutating data",
    "section": "",
    "text": "17.1 Adding multiple calculations\nOften the data you have will prompt questions that it doesn’t immediately answer. Religious affiliation data, for example, might have raw counts of priests and parishes but we often don’t use those to make comparisons across time periods unless the numbers are small. We need ratios and percentages!\nTo do that in R, we can use dplyr and mutate to calculate new metrics in a new field using existing fields of data. That’s the essence of mutate - using the data you have to answer a new question.\nSo first we’ll import the tidyverse so we can read in our data and begin to work with it.\nNow we’ll import a dataset of Catholic Church statistics from selected years that is in the data folder in this chapter’s pre-lab directory. We’ll use this to explore ways to create new information from existing data.\nLet’s add a column called catholics_per_parish to calculate how many Catholics there are for each parish in each year. This helps us understand how parish size and resource distribution have changed over time. The code to calculate a ratio is pretty simple. Remember, with summarize, we used n() to count things. With mutate, we use very similar syntax to calculate a new value – a new column of data – using other values in our dataset.\nTo calculate this ratio, we need both the Catholic population and the number of parishes. We’ll use mutate to create the new column. The key here is to save the dataframe to itself so that our changes stick.\nBut what do you see right away? Do those numbers look like we expect them to? They might be large numbers with lots of decimal places. Let’s make them easier to read by rounding to the nearest whole number:\nNow, does this ordering do anything for us? No. Let’s fix that with arrange.\nSo now we have results ordered by catholics_per_parish with the highest ratio first. To see the lowest ratio first, we can reverse that arrange function - we don’t need to recalculate the column:\nThe U.S. Catholic Church had the fewest Catholics per parish in 1965, with about 2,494 Catholics per parish. By 2024, that ratio had increased to 4,137 Catholics per parish - a sign of both parish closures and changing demographics. Using mutate() allows us to create a comparison that helps us understand those trends.\nWe can also add multiple calculations at once. Let’s calculate both the Catholics per parish and the percentage of parishes without a resident priest:\ncara_stats &lt;- cara_stats |&gt;\n  mutate(\n    catholics_per_parish = round(catholic_population / parishes, 0),\n    pct_no_resident_priest = (parishes_without_resident_priest / parishes) * 100\n  ) |&gt; \n  arrange(desc(pct_no_resident_priest))\nThis shows us how the challenge of staffing parishes with resident priests has grown over time - rising from less than 3% of parishes in 1965 to more than 21% in 2024.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Mutating data</span>"
    ]
  },
  {
    "objectID": "mutating.html#another-use-of-mutate",
    "href": "mutating.html#another-use-of-mutate",
    "title": "17  Mutating data",
    "section": "17.2 Another use of mutate",
    "text": "17.2 Another use of mutate\nMutate is also useful for standardizing data - for example, making different spellings of, say, campaign spending recipients.\nLet’s load our Maryland campaign expenditures involving churches into a md_churches dataframe, and focus in particular on the Payee Name column.\n\nchurch_expenses &lt;- read_csv(\"data/md_church_expenditures.csv\")\n\nRows: 154 Columns: 32\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (18): Committee Name, Candidate Last Name, Candidate First Name, Candida...\ndbl  (3): TransactionID, Filing EntityID, Transaction Amount\nlgl (11): Candidate Suffix, Vendor Name, Vendor Type, Vendor Address Line1, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nchurch_expenses\n\n# A tibble: 154 × 32\n   TransactionID `Filing EntityID` `Committee Name`        `Candidate Last Name`\n           &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;                   &lt;chr&gt;                \n 1       3149378           1003374 Griffith, Melony Citiz… Griffith             \n 2       3058673           5007521 Baltimore County Democ… &lt;NA&gt;                 \n 3       3147639           1009796 Jackson, Michael A., F… JACKSON              \n 4       3058671           5007521 Baltimore County Democ… &lt;NA&gt;                 \n 5       3157755           2008639 District 14 Team Slate  &lt;NA&gt;                 \n 6       3051656           1000687 Turner, Veronica Frien… TURNER               \n 7       3503701           1011532 Charkoudian, Lorig Fri… CHARKOUDIAN          \n 8       3058670           5007521 Baltimore County Democ… &lt;NA&gt;                 \n 9       2864651           1007182 McCray, Cory Friends of MCCRAY               \n10       2862208           1009308 Charles, Nicholas Frie… CHARLES              \n# ℹ 144 more rows\n# ℹ 28 more variables: `Candidate First Name` &lt;chr&gt;,\n#   `Candidate Middle Name` &lt;chr&gt;, `Candidate Suffix` &lt;lgl&gt;,\n#   `Committee Type` &lt;chr&gt;, `Payee Name` &lt;chr&gt;, `Payee Type` &lt;chr&gt;,\n#   `Payee Address Line1` &lt;chr&gt;, `Payee Address Line2` &lt;chr&gt;,\n#   `Payee City` &lt;chr&gt;, `Payee State` &lt;chr&gt;, `Payee Zip Code` &lt;chr&gt;,\n#   `Vendor Name` &lt;lgl&gt;, `Vendor Type` &lt;lgl&gt;, `Vendor Address Line1` &lt;lgl&gt;, …\n\n\nYou’ll notice that the payee names have a mix of styles: title case, upper case and lower case. R will treat two names that are spelled the same but have different cases as distinct things, and that will mean that any aggregates we create based on Payee Name won’t be accurate.\nSo how can we fix that? Mutate - it’s not just for math! And a function called str_to_upper that will convert a character column into all uppercase.\n\nstandardized_church_expenses &lt;- church_expenses |&gt;\n  mutate(\n    payee_upper = str_to_upper(`Payee Name`)\n)\n\nThat creates a new column, payee_upper, with all uppercase letters. Now we can use that column to group and summarize our data.\nThere are lots of potential uses for standardization - addresses, zip codes, anything that can be misspelled or abbreviated.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Mutating data</span>"
    ]
  },
  {
    "objectID": "mutating.html#a-more-powerful-use",
    "href": "mutating.html#a-more-powerful-use",
    "title": "17  Mutating data",
    "section": "17.3 A more powerful use",
    "text": "17.3 A more powerful use\nMutate is even more useful when combined with some additional functions. Let’s keep rolling with our expenditure data. Take a look at our new payee_upper column: it contains the name of the recipient. It would be useful to have a separate denomination column indicating what sort of church it is. We can check to see if a denomination name is contained in that column and then populate a new column with the value we want, using the functions str_detect and case_when. The case_when function handles multiple variations, such as if a payee contains “Baptist” or “Catholic”, etc. Crucially, we can tell R to populate the new column with NA if it doesn’t find a match.\n\nstandardized_church_expenses &lt;- standardized_church_expenses |&gt;\n  mutate(\n    denomination = case_when(\n        str_detect(payee_upper, \"BAPTIST\") ~ \"Baptist\",\n        str_detect(payee_upper, \"CATHOLIC\") ~ \"Catholic\",\n        str_detect(payee_upper, \"METHODIST\") ~ \"Methodist\",\n        str_detect(payee_upper, \"PRESBYTERIAN\") ~ \"Presbyterian\",\n        str_detect(payee_upper, \"LUTHERAN\") ~ \"Lutheran\",\n        str_detect(payee_upper, \"EPISCOPAL\") ~ \"Episocopal\",\n        str_detect(payee_upper, \"CHURCH OF CHRIST\") ~ \"Church of Christ\",\n        str_detect(payee_upper, \"UNITARIAN\") ~ \"Unitarian\",\n        str_detect(payee_upper, \"TEMPLE\") ~ \"Jewish\",\n        .default = NA\n      )\n  )\n\nThere’s a lot going on here, so let’s unpack it. It starts out as a typical mutate statement, but case_when introduces some new things. Each line checks to see if the pattern is contained in the payee_upper column, followed by ~ and then a value for the new column for records that match that check. You can read it like this: “If we find ‘BAPTIST’ in the payee_upper column, then put ‘Baptist’ in the denomination column”, and so on for other denominations, and if we don’t match any word we’re looking for, make denomination NA.\nWe can then use our new denomination column in group_by statements to make summarizing easier.\n\nstandardized_church_expenses |&gt;\n  group_by(denomination) |&gt;\n  summarize(total = sum(`Transaction Amount`)) |&gt;\n  arrange(desc(total))\n\n# A tibble: 10 × 2\n   denomination      total\n   &lt;chr&gt;             &lt;dbl&gt;\n 1 Baptist          18988.\n 2 &lt;NA&gt;             13904.\n 3 Catholic         10353.\n 4 Methodist         3414 \n 5 Church of Christ  2400 \n 6 Lutheran          1750 \n 7 Presbyterian       424.\n 8 Jewish             360 \n 9 Unitarian          108.\n10 Episocopal         100 \n\n\nMost expenditures seem to have been paid to Baptist churches, although the large amount of NA transactions suggests that there are many churches we couldn’t categorize.\nMutate is there to make your data more useful and to make it easier for you to ask more and better questions of it.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Mutating data</span>"
    ]
  },
  {
    "objectID": "data-cleaning-part-i.html",
    "href": "data-cleaning-part-i.html",
    "title": "18  Data Cleaning Part I: Data smells",
    "section": "",
    "text": "18.1 Wrong Type\nAny time you are given a dataset from anyone, you should immediately be suspicious. Is this data what I think it is? Does it include what I expect? Is there anything I need to know about it? Will it produce the information I expect?\nOne of the first things you should do is give it the smell test.\nFailure to give data the smell test can lead you to miss stories and get your butt kicked on a competitive story.\nWith data smells, we’re trying to find common mistakes in data. For more on data smells, read the GitHub wiki post that started it all. Some common data smells are:\nNot all of these data smells are detectable in code. You may have to ask people about the data. You may have to compare it to another dataset yourself. Does the agency that uses the data produce reports from the data? Does your analysis match those reports? That will expose wrongly derived data, or wrong units, or mistakes you made with inclusion or exclusion.\nBut with several of these data smells, we can do them first, before we do anything else.\nWe’re going to examine several here as they apply to some Maryland state government payments data and Maryland state government grant & loan data.\nFirst, let’s look at Wrong Type Of Data.\nWe can sniff that out by looking at the output of readr.\nLet’s load the tidyverse.\n# Remove scientific notation\noptions(scipen=999)\n# Load the tidyverse\nlibrary(tidyverse)\nThis time, we’re going to load the data in a CSV format, which stands for comma separated values and is essentially a fancy structured text file. Each column in the csv is separated – “delimited” – by a comma from the next column.\nWe’re also going to introduce a new argument to our function that reads in the data, read_csv(), called “guess_max”. As R reads in the csv file, it will attempt to make some calls on what “data type” to assign to each field: number, character, date, and so on. The “guess_max” argument says: look at the values in the whatever number of rows we specify before deciding which data type to assign. In this case, we’ll pick 10.\n# Load the data\npayments &lt;- read_csv(\"data/State_of_Maryland_Payments_Data__FY2008_to_FY2024_20260122.csv.zip\", guess_max=10)\n\nMultiple files in zip: reading\n'State_of_Maryland_Payments_Data__FY2008_to_FY2024_20260122.csv'\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 392488 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): Agency Name, Vendor Name, Date, Category\ndbl (4): Fiscal Year, Vendor Zip, Amount, Fiscal Period\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nPay attention to the red warning that signals “one or more parsing issues.” It advises us to run the problems() function to see what went wrong. Let’s do that.\nproblems(payments)\n\n# A tibble: 370 × 5\n     row   col expected actual file \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;\n 1  9568     4 a double L4Z4B  \"\"   \n 2 10117     4 a double L4Y1Z  \"\"   \n 3 10476     4 a double 0LAND  \"\"   \n 4 12308     4 a double H4S    \"\"   \n 5 13084     4 a double N2G    \"\"   \n 6 15549     4 a double GOR    \"\"   \n 7 15772     4 a double N5V    \"\"   \n 8 15844     4 a double T1Y    \"\"   \n 9 15877     4 a double R2C    \"\"   \n10 15973     4 a double L4L34  \"\"   \n# ℹ 360 more rows\nIt produces a table of all the parsing problems. It has 369 rows, which means we have that some problems but not a huge number considering we have 369,000 rows. In almost every case here, the readr library has guessed that a given column was of a “double” data type – a number. It did it based on very limited information – only 10 rows. So, when it hit a value that looked like a date, or a character string, it didn’t know what to do. So it just didn’t read in that value correctly.\nThe easy way to fix this is to set the guess_max argument higher. It will take a little longer to load, but we’ll use every single row in the data set to guess the column type – all 322,138 of them.\npayments &lt;- read_csv(\"data/State_of_Maryland_Payments_Data__FY2008_to_FY2024_20260122.csv.zip\", guess_max=392488)\n\nMultiple files in zip: reading 'State_of_Maryland_Payments_Data__FY2008_to_FY2024_20260122.csv'\nRows: 392488 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Agency Name, Vendor Name, Vendor Zip, Date, Category\ndbl (3): Fiscal Year, Amount, Fiscal Period\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nNo parsing errors this time! You can see what the columns are using the glimpse function:\nglimpse(payments)\n\nRows: 392,488\nColumns: 8\n$ `Fiscal Year`   &lt;dbl&gt; 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, …\n$ `Agency Name`   &lt;chr&gt; \"AID TO UNIVERSITY OF MD MEDICAL SYSTEM\", \"BALTIMORE C…\n$ `Vendor Name`   &lt;chr&gt; \"U M M S\", \"3M PHJ3884\", \"4 IMPRINT\", \"A J STATIONERS\"…\n$ `Vendor Zip`    &lt;chr&gt; \"21273\", \"15250\", \"53201\", \"21226\", \"21228\", \"21210\", …\n$ Amount          &lt;dbl&gt; 9701191.00, 6212.00, 8312.96, 21810.09, 30195.57, 1002…\n$ `Fiscal Period` &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ Date            &lt;chr&gt; \"2008 Jan 01 12:00:00 AM\", \"2008 Jan 01 12:00:00 AM\", …\n$ Category        &lt;chr&gt; \"Vendor Payment\", \"Vendor Payment\", \"Vendor Payment\", …\nThings that should be characters – like agency name, vendor name – are characters (chr). Things that should be numbers (dbl) – like amount and fiscal year – are numbers. We’ve seen before that sometimes dates aren’t defined as date datatypes by R - we can fix that using lubridate.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data Cleaning Part I: Data smells</span>"
    ]
  },
  {
    "objectID": "data-cleaning-part-i.html#wrong-spatial-data",
    "href": "data-cleaning-part-i.html#wrong-spatial-data",
    "title": "18  Data Cleaning Part I: Data smells",
    "section": "18.2 Wrong Spatial Data",
    "text": "18.2 Wrong Spatial Data\nThe second smell we can find in code is wrong spatial data. Spatial data means data that refers to some geography; in this dataset the only geographical element is the vendor’s zip code. Zip codes should be, at a minimum, five characters long (although composed of numbers, zip codes aren’t used as numbers).\nWe can check to see if any of the zip codes are less than five characters by using a function called str_length inside a filter:\n\npayments |&gt;\n  group_by(`Vendor Zip`) |&gt;\n  filter(str_length(`Vendor Zip`) &lt; 5) |&gt; \n  summarise(\n    count=n()\n  ) |&gt;\n  arrange(desc(count))\n\n# A tibble: 560 × 2\n   `Vendor Zip` count\n   &lt;chr&gt;        &lt;int&gt;\n 1 2241          2047\n 2 8873           435\n 3 2284           356\n 4 7921           297\n 5 4915           197\n 6 8650           196\n 7 7188           194\n 8 7101           191\n 9 1441           133\n10 7102           112\n# ℹ 550 more rows\n\n\nSo, yes, we definitely have some zip codes that are less than 5 characters long, which is not good, particularly because we don’t have any other geographical information (such as a state) that would tell us whether we’re missing a leading zero or some other character.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data Cleaning Part I: Data smells</span>"
    ]
  },
  {
    "objectID": "data-cleaning-part-i.html#gaps-in-data-missing-data",
    "href": "data-cleaning-part-i.html#gaps-in-data-missing-data",
    "title": "18  Data Cleaning Part I: Data smells",
    "section": "18.3 Gaps in data & Missing data",
    "text": "18.3 Gaps in data & Missing data\nLet’s now look at gaps in data. These often occur when you have a date or time element in your data, but there are other potential gaps, too. To illustrate those, we’re going to introduce some Maryland state grant and loan data from 2009 forward. Let’s load it and take a look:\n\nmd_grants_loans &lt;- read_csv(\"data/State_of_Maryland_Grant_and_Loan_Data__FY2009_to_FY2024_20260122.csv\")\n\nRows: 21226 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): Grantor, Grantee, Zip Code, Description, Category, Date\ndbl (2): Fiscal Year, Fiscal Period\nnum (1): Amount\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nEach row represents a recipient of state grant or loan, along with information about their location and the state agency that provided the money. When we talk about gaps, often they indicate the administrative rules. Here’s an example: let’s count the number of payments in each category (Grant or Loan) by year in this dataset:\n\nmd_grants_loans |&gt; \n  group_by(`Fiscal Year`, Category) |&gt; \n  summarize(count = n()) |&gt; \n  arrange(`Fiscal Year`)\n\n`summarise()` has grouped output by 'Fiscal Year'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 37 × 3\n# Groups:   Fiscal Year [16]\n   `Fiscal Year` Category count\n           &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt;\n 1          2009 Grant      819\n 2          2010 Grant      612\n 3          2010 Loan        76\n 4          2010 &lt;NA&gt;         1\n 5          2011 Grant      990\n 6          2011 Loan        45\n 7          2011 &lt;NA&gt;         1\n 8          2012 Grant     1100\n 9          2012 Loan        58\n10          2013 Grant      986\n# ℹ 27 more rows\n\n\nWe can see a couple of issues here: first, there is no loan data for FY 2009. That’s mentioned in the source page for the data. It’s good to be aware of all gaps in data, but they don’t always represent a problem. Second, and more problematic, there are a few records where the Category is NA - that data is missing. There also are some inconsistent values - there are 50 records in FY2013 with the category of “L” (probably loans) and one in FY 2017 that is listed as “Contract”.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data Cleaning Part I: Data smells</span>"
    ]
  },
  {
    "objectID": "data-cleaning-part-i.html#unusual-outliers",
    "href": "data-cleaning-part-i.html#unusual-outliers",
    "title": "18  Data Cleaning Part I: Data smells",
    "section": "18.4 Unusual Outliers",
    "text": "18.4 Unusual Outliers\nAny time you are going to focus on a column for analysis, you should check for unusual values. Are there any unusually large values or unusually small values? Are there any values that raise immediate questions about the data? Let’s look at the smallest amounts in the grants and loan data.\n\nmd_grants_loans |&gt; \n  arrange(Amount)\n\n# A tibble: 21,226 × 9\n   Grantor          Grantee `Zip Code` `Fiscal Year` Amount Description Category\n   &lt;chr&gt;            &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;   \n 1 Department of N… Land P… 21285               2009   60   Catherine … Grant   \n 2 Maryland Depart… UNITED… 21031               2012   96.0 FOOD SERVI… Grant   \n 3 Conservation Re… Easter… 21601               2018  186.  Rolling Vi… Grant   \n 4 Maryland Depart… FAMILY… 20877               2013  304   RTTT - EAR… Grant   \n 5 Maryland Depart… Washin… 21740               2017  361   Hold Harml… Grant   \n 6 Maryland Depart… THE CH… 21215-3211          2012  362.  FOOD SERVI… Grant   \n 7 Maryland Depart… ARCHDI… 21227               2012  379.  MARYLAND M… Grant   \n 8 Maryland Depart… HUMAN … 21158               2013  387   CASH FOR C… Grant   \n 9 Department of N… The Co… 22209               2012  402.  R Creighto… Grant   \n10 Governor's Offi… Court … 21204               2018  411.  Children's… Grant   \n# ℹ 21,216 more rows\n# ℹ 2 more variables: `Fiscal Period` &lt;dbl&gt;, Date &lt;chr&gt;\n\n\nThere are two grants for less than $100, which might not be problematic at all, but given that just two of 19,000 are for very small amounts you might wonder if there are suggested amounts for applicants and how tiny ones get evaluated compared to very large requests. As journalists, we should be skeptical of information put in front of us and ask why or what it says about the data itself.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data Cleaning Part I: Data smells</span>"
    ]
  },
  {
    "objectID": "data-cleaning-part-ii.html",
    "href": "data-cleaning-part-ii.html",
    "title": "19  Data Cleaning Part II: Janitor",
    "section": "",
    "text": "19.1 Cleaning headers\nThe necessary bane of every data journalist’s existence is data cleaning.\nEvery developer, every data system, every agency, they all have opinions about how data gets collected. Some decisions make sense from the outside. Some decisions are based entirely on internal politics: who is creating the data, how they are creating it, why they are creating it. Is it automated? Is it manual? Are data normalized? Are there free form fields where users can just type into or does the system restrict them to choices?\nYour journalistic questions – what you want the data to tell you – are almost never part of that equation.\nSo cleaning data is the process of fixing issues in your data so you can answer the questions you want to answer. Data cleaning is a critical step that you can’t skip. A standard metric is that 80 percent of the time working with data will be spent cleaning and verifying data, and 20 percent the more exciting parts like analysis and visualization.\nThe tidyverse has a lot of built-in tools for data cleaning. We’re also going to make use of a new library, called janitor that has a bunch of great functions for cleaning data. Let’s load those now.\nLet’s continue with our Maryland grants and loans data that we worked with in the previous chapter.\nThere are a number of issues with this data set that might get in the way of asking questions and receiving accurate answers. They are:\nLet’s get cleaning. Our goal will be to build up one block of code that does all the necessary cleaning in order to answer this question: which organization with “CHURCH” in its name has gotten the most amount of grant/loan money from the state, and what’s the zip code where it is located?\nOne of the first places we can start with cleaning data is cleaning the column names (or headers).\nEvery system has their own way of recording headers, and every developer has their own thoughts of what a good idea is within it. R is most happy when headers are lower case, without special characters.\nIf column headers start with a number, or have a space in between two words, you have to set them off with backticks when using them in a function. Generally speaking, we want one word (or words separated by an underscore), all lowercase, that don’t start with numbers.\nThe janitor library makes fixing headers trivially simple with the function clean_names()\n# cleaning function\ncleaned_md_grants_loans &lt;- md_grants_loans |&gt;\n  clean_names()\n\n# display the cleaned dataset\ncleaned_md_grants_loans\n\n# A tibble: 21,226 × 9\n   grantor              grantee zip_code fiscal_year amount description category\n   &lt;chr&gt;                &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;   \n 1 Commerce/Maryland T… PRINCE… 20772           2017 1.28e5 Maryland T… Grant   \n 2 Department of Healt… Associ… 21201           2010 9.34e4 Minority O… Grant   \n 3 Maryland Department… WESTED… 94107-1…        2014 1.61e6 GRANTS FOR… Grant   \n 4 Department of Healt… Maryla… 21075           2010 2.14e5 Babies Bor… Grant   \n 5 Department of Natur… Anacos… 20710           2017 1.74e5 Payments m… Grant   \n 6 Department of Busin… Washin… 21740           2009 5.59e4 grant fund… Grant   \n 7 Boards and Commissi… Domest… 21045           2014 1.72e5 Domestic V… Grant   \n 8 MD Small Business D… Dacore… 20601           2018 1.04e5 Maryland S… Loan    \n 9 Maryland Higher Edu… Mount … 21727           2015 1.75e6 Sellinger … Grant   \n10 Department of Busin… Olney … 20830           2010 2.16e5 Grant fund… Grant   \n# ℹ 21,216 more rows\n# ℹ 2 more variables: fiscal_period &lt;dbl&gt;, date &lt;chr&gt;\nThis function changed Zip Code to zip_code and generally got rid of capital letters and replaced spaces with underscores. If we wanted to rename a column, we can use a tidyverse function rename() to do that. Let’s change grantor to source as an example. NOTE: when using rename(), the new name comes first.\n# cleaning function\ncleaned_md_grants_loans &lt;- md_grants_loans |&gt;\n  clean_names() |&gt; \n  rename(source = grantor)\n\n# display the cleaned dataset\ncleaned_md_grants_loans\n\n# A tibble: 21,226 × 9\n   source grantee zip_code fiscal_year amount description category fiscal_period\n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt;\n 1 Comme… PRINCE… 20772           2017 1.28e5 Maryland T… Grant                1\n 2 Depar… Associ… 21201           2010 9.34e4 Minority O… Grant                1\n 3 Maryl… WESTED… 94107-1…        2014 1.61e6 GRANTS FOR… Grant                1\n 4 Depar… Maryla… 21075           2010 2.14e5 Babies Bor… Grant                1\n 5 Depar… Anacos… 20710           2017 1.74e5 Payments m… Grant                1\n 6 Depar… Washin… 21740           2009 5.59e4 grant fund… Grant                1\n 7 Board… Domest… 21045           2014 1.72e5 Domestic V… Grant                1\n 8 MD Sm… Dacore… 20601           2018 1.04e5 Maryland S… Loan                 1\n 9 Maryl… Mount … 21727           2015 1.75e6 Sellinger … Grant                1\n10 Depar… Olney … 20830           2010 2.16e5 Grant fund… Grant                1\n# ℹ 21,216 more rows\n# ℹ 1 more variable: date &lt;chr&gt;",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data Cleaning Part II: Janitor</span>"
    ]
  },
  {
    "objectID": "data-cleaning-part-ii.html#changing-capitalization",
    "href": "data-cleaning-part-ii.html#changing-capitalization",
    "title": "19  Data Cleaning Part II: Janitor",
    "section": "19.2 Changing capitalization",
    "text": "19.2 Changing capitalization\nRight now the source, grantee and description columns have inconsistent capitalization. We can fix that using a mutate statement and a function that changes the case of text called str_to_upper(). We’ll use the same columns, overwriting what’s in there since all we’re doing is changing case.\n\n# cleaning function\ncleaned_md_grants_loans &lt;- md_grants_loans |&gt;\n  clean_names() |&gt; \n  rename(source = grantor) |&gt; \n  mutate(source = str_to_upper(source), grantee = str_to_upper(grantee), description = str_to_upper(description))\n\n# display the cleaned dataset\ncleaned_md_grants_loans\n\n# A tibble: 21,226 × 9\n   source grantee zip_code fiscal_year amount description category fiscal_period\n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt;\n 1 COMME… PRINCE… 20772           2017 1.28e5 MARYLAND T… Grant                1\n 2 DEPAR… ASSOCI… 21201           2010 9.34e4 MINORITY O… Grant                1\n 3 MARYL… WESTED… 94107-1…        2014 1.61e6 GRANTS FOR… Grant                1\n 4 DEPAR… MARYLA… 21075           2010 2.14e5 BABIES BOR… Grant                1\n 5 DEPAR… ANACOS… 20710           2017 1.74e5 PAYMENTS M… Grant                1\n 6 DEPAR… WASHIN… 21740           2009 5.59e4 GRANT FUND… Grant                1\n 7 BOARD… DOMEST… 21045           2014 1.72e5 DOMESTIC V… Grant                1\n 8 MD SM… DACORE… 20601           2018 1.04e5 MARYLAND S… Loan                 1\n 9 MARYL… MOUNT … 21727           2015 1.75e6 SELLINGER … Grant                1\n10 DEPAR… OLNEY … 20830           2010 2.16e5 GRANT FUND… Grant                1\n# ℹ 21,216 more rows\n# ℹ 1 more variable: date &lt;chr&gt;\n\n\nWhat this does is make it so that using group_by will result in fewer rows due to inconsistent capitalization. It won’t fix misspellings, but working off a single case style definitely helps.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data Cleaning Part II: Janitor</span>"
    ]
  },
  {
    "objectID": "data-cleaning-part-ii.html#duplicates",
    "href": "data-cleaning-part-ii.html#duplicates",
    "title": "19  Data Cleaning Part II: Janitor",
    "section": "19.3 Duplicates",
    "text": "19.3 Duplicates\nOne of the most difficult problems to fix in data is duplicate records in the data. They can creep in with bad joins, bad data entry practices, mistakes – all kinds of reasons. A duplicated record isn’t always there because of an error, but you need to know if it’s there before making that determination.\nSo the question is, do we have any records repeated?\nHere we’ll use a function called get_dupes from the janitor library to check for fully repeated records in our cleaned data set.\n\ncleaned_md_grants_loans |&gt;\n  get_dupes()\n\nNo variable names specified - using all columns.\n\n\n# A tibble: 58 × 10\n   source grantee zip_code fiscal_year amount description category fiscal_period\n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt;\n 1 BOARD… CASA C… 21740           2009 6.62e4 SERVICES T… Grant                1\n 2 BOARD… CASA C… 21740           2009 6.62e4 SERVICES T… Grant                1\n 3 BOARD… FAMILY… 21218           2009 2.18e5 PRE-ADJUDI… Grant                1\n 4 BOARD… FAMILY… 21218           2009 2.18e5 PRE-ADJUDI… Grant                1\n 5 BOARD… HEARTL… 21705           2009 5.32e4 UNDERSERVE… Grant                1\n 6 BOARD… HEARTL… 21705           2009 5.32e4 UNDERSERVE… Grant                1\n 7 BOARD… MARYLA… 21012           2009 9.72e4 CAPACITY B… Grant                1\n 8 BOARD… MARYLA… 21012           2009 9.72e4 CAPACITY B… Grant                1\n 9 BOARD… MARYLA… 21012           2009 9.72e4 SEXUAL ASS… Grant                1\n10 BOARD… MARYLA… 21012           2009 9.72e4 SEXUAL ASS… Grant                1\n# ℹ 48 more rows\n# ℹ 2 more variables: date &lt;chr&gt;, dupe_count &lt;int&gt;\n\n\nAnd the answer is … maybe? Because the original dataset doesn’t have a unique identifier for each grant, it’s possible that we have duplicates here, as many as 58. If we could confirm that these actually are duplicates, we can fix this by adding the function distinct() to our cleaning script. This will keep only one copy of each unique record in our table. But we’d need to confirm that first.\n\n# cleaning function\ncleaned_md_grants_loans &lt;- md_grants_loans |&gt;\n  clean_names() |&gt; \n  rename(source = grantor) |&gt; \n  mutate(source = str_to_upper(source), grantee = str_to_upper(grantee), description = str_to_upper(description)) |&gt; \n  distinct()\n\n# display the cleaned dataset\ncleaned_md_grants_loans\n\n# A tibble: 21,197 × 9\n   source grantee zip_code fiscal_year amount description category fiscal_period\n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt;\n 1 COMME… PRINCE… 20772           2017 1.28e5 MARYLAND T… Grant                1\n 2 DEPAR… ASSOCI… 21201           2010 9.34e4 MINORITY O… Grant                1\n 3 MARYL… WESTED… 94107-1…        2014 1.61e6 GRANTS FOR… Grant                1\n 4 DEPAR… MARYLA… 21075           2010 2.14e5 BABIES BOR… Grant                1\n 5 DEPAR… ANACOS… 20710           2017 1.74e5 PAYMENTS M… Grant                1\n 6 DEPAR… WASHIN… 21740           2009 5.59e4 GRANT FUND… Grant                1\n 7 BOARD… DOMEST… 21045           2014 1.72e5 DOMESTIC V… Grant                1\n 8 MD SM… DACORE… 20601           2018 1.04e5 MARYLAND S… Loan                 1\n 9 MARYL… MOUNT … 21727           2015 1.75e6 SELLINGER … Grant                1\n10 DEPAR… OLNEY … 20830           2010 2.16e5 GRANT FUND… Grant                1\n# ℹ 21,187 more rows\n# ℹ 1 more variable: date &lt;chr&gt;",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data Cleaning Part II: Janitor</span>"
    ]
  },
  {
    "objectID": "data-cleaning-part-ii.html#cleaning-strings",
    "href": "data-cleaning-part-ii.html#cleaning-strings",
    "title": "19  Data Cleaning Part II: Janitor",
    "section": "19.4 Cleaning strings",
    "text": "19.4 Cleaning strings\nThe rest of the problems with this data set all have to do with inconsistent format of values in a few of the columns. To fix these problems, we’re going to make use of mutate() in concert with “string functions” – special functions that allow us to clean up columns stored as character strings. The tidyverse package stringr has lots of useful string functions, more than we’ll learn in this chapter.\nLet’s start by cleaning up the zip field. Remember, some of the rows had a five-digit ZIP code, while others had a nine-digit ZIP code, separated by a hyphen or not.\nWe’re going to write code that tells R to make a new column for our zips, keeping the first five digits on the left, and get rid of anything after that by using mutate() in concert with str_sub(), from the stringr package.\n\n# cleaning function\ncleaned_md_grants_loans &lt;- md_grants_loans |&gt;\n  clean_names() |&gt; \n  rename(source = grantor) |&gt; \n  mutate(source = str_to_upper(source), grantee = str_to_upper(grantee), description = str_to_upper(description)) |&gt; \n  distinct() |&gt;\n  mutate(zip5 = str_sub(zip_code, start=1L, end=5L))\n\n\n# display the cleaned dataset\ncleaned_md_grants_loans\n\n# A tibble: 21,197 × 10\n   source grantee zip_code fiscal_year amount description category fiscal_period\n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt;\n 1 COMME… PRINCE… 20772           2017 1.28e5 MARYLAND T… Grant                1\n 2 DEPAR… ASSOCI… 21201           2010 9.34e4 MINORITY O… Grant                1\n 3 MARYL… WESTED… 94107-1…        2014 1.61e6 GRANTS FOR… Grant                1\n 4 DEPAR… MARYLA… 21075           2010 2.14e5 BABIES BOR… Grant                1\n 5 DEPAR… ANACOS… 20710           2017 1.74e5 PAYMENTS M… Grant                1\n 6 DEPAR… WASHIN… 21740           2009 5.59e4 GRANT FUND… Grant                1\n 7 BOARD… DOMEST… 21045           2014 1.72e5 DOMESTIC V… Grant                1\n 8 MD SM… DACORE… 20601           2018 1.04e5 MARYLAND S… Loan                 1\n 9 MARYL… MOUNT … 21727           2015 1.75e6 SELLINGER … Grant                1\n10 DEPAR… OLNEY … 20830           2010 2.16e5 GRANT FUND… Grant                1\n# ℹ 21,187 more rows\n# ℹ 2 more variables: date &lt;chr&gt;, zip5 &lt;chr&gt;\n\n\nLet’s break down that last line of code. It says: take the value in each zip column and extract the first character on the left (1L) through the fifth character on the left (5L), and then use that five-digit zip to populate a new zip5 column.\nIf we arrange the zip5 column we can see that there are some non-digits in there, so let’s make those NA. For that, we’re going to use case_when(), a function that let’s us say if a value meets a certain condition, then change it, and if it doesn’t, don’t change it.\n\n# cleaning function\ncleaned_md_grants_loans &lt;- md_grants_loans |&gt;\n  clean_names() |&gt; \n  rename(source = grantor) |&gt; \n  mutate(source = str_to_upper(source), grantee = str_to_upper(grantee), description = str_to_upper(description)) |&gt; \n  distinct() |&gt;\n  mutate(zip5 = str_sub(zip_code, start=1L, end=5L)) |&gt;\n  mutate(zip5 = case_when(\n    zip5 == \"Vario\" ~ NA,\n    zip5 == \"UB7 O\" ~ NA,\n    zip5 == \"UB7 \" ~ NA,\n    .default = zip5\n  ))\n\n# display the cleaned dataset\ncleaned_md_grants_loans\n\n# A tibble: 21,197 × 10\n   source grantee zip_code fiscal_year amount description category fiscal_period\n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt;\n 1 COMME… PRINCE… 20772           2017 1.28e5 MARYLAND T… Grant                1\n 2 DEPAR… ASSOCI… 21201           2010 9.34e4 MINORITY O… Grant                1\n 3 MARYL… WESTED… 94107-1…        2014 1.61e6 GRANTS FOR… Grant                1\n 4 DEPAR… MARYLA… 21075           2010 2.14e5 BABIES BOR… Grant                1\n 5 DEPAR… ANACOS… 20710           2017 1.74e5 PAYMENTS M… Grant                1\n 6 DEPAR… WASHIN… 21740           2009 5.59e4 GRANT FUND… Grant                1\n 7 BOARD… DOMEST… 21045           2014 1.72e5 DOMESTIC V… Grant                1\n 8 MD SM… DACORE… 20601           2018 1.04e5 MARYLAND S… Loan                 1\n 9 MARYL… MOUNT … 21727           2015 1.75e6 SELLINGER … Grant                1\n10 DEPAR… OLNEY … 20830           2010 2.16e5 GRANT FUND… Grant                1\n# ℹ 21,187 more rows\n# ℹ 2 more variables: date &lt;chr&gt;, zip5 &lt;chr&gt;\n\n\nThat last bit is a little complex, so let’s break it down.\nWhat the code above says, in English, is this: Look at all the values in the zip5 column. If the value is “Vario”, then (that’s what the “~” means, then) replace it with NA. Same for the other variations. If it’s anything other than that (that’s what “TRUE” means, otherwise), then keep the existing value in that column.\nInstead of specifying the exact value, we can also solve the problem by using something more generalizable, using a function called str_detect(), which allows us to search parts of words.\nThe second line of our case_when() function below now says, in English: look in the city column. If you find that one of the values starts with “UB7” (the “^” symbol means “starts with”), then (the tilde ~ means then) change it to NA.\n\n# cleaning function\ncleaned_md_grants_loans &lt;- md_grants_loans |&gt;\n  clean_names() |&gt; \n  rename(source = grantor) |&gt; \n  mutate(source = str_to_upper(source), grantee = str_to_upper(grantee), description = str_to_upper(description)) |&gt; \n  distinct() |&gt;\n  mutate(zip5 = str_sub(zip_code, start=1L, end=5L)) |&gt;\n  mutate(zip5 = case_when(\n    zip5 == \"Vario\" ~ NA,\n    str_detect(zip5, \"^UB7\") ~ NA,\n    .default = zip5\n  ))\n\n# display the cleaned dataset\ncleaned_md_grants_loans\n\n# A tibble: 21,197 × 10\n   source grantee zip_code fiscal_year amount description category fiscal_period\n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt;\n 1 COMME… PRINCE… 20772           2017 1.28e5 MARYLAND T… Grant                1\n 2 DEPAR… ASSOCI… 21201           2010 9.34e4 MINORITY O… Grant                1\n 3 MARYL… WESTED… 94107-1…        2014 1.61e6 GRANTS FOR… Grant                1\n 4 DEPAR… MARYLA… 21075           2010 2.14e5 BABIES BOR… Grant                1\n 5 DEPAR… ANACOS… 20710           2017 1.74e5 PAYMENTS M… Grant                1\n 6 DEPAR… WASHIN… 21740           2009 5.59e4 GRANT FUND… Grant                1\n 7 BOARD… DOMEST… 21045           2014 1.72e5 DOMESTIC V… Grant                1\n 8 MD SM… DACORE… 20601           2018 1.04e5 MARYLAND S… Loan                 1\n 9 MARYL… MOUNT … 21727           2015 1.75e6 SELLINGER … Grant                1\n10 DEPAR… OLNEY … 20830           2010 2.16e5 GRANT FUND… Grant                1\n# ℹ 21,187 more rows\n# ℹ 2 more variables: date &lt;chr&gt;, zip5 &lt;chr&gt;\n\n\nWe’ve gotten the source and zip code data as clean as we can, and now we can answer our question: which grantee with the word “CHURCH” in its name has gotten the most amount of grant/loan money from the state, and what is the zip code for it? A good rule of thumb is that you should only spend time cleaning fields that are critical to the specific analysis you want to do.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data Cleaning Part II: Janitor</span>"
    ]
  },
  {
    "objectID": "open-refine.html",
    "href": "open-refine.html",
    "title": "20  Data Cleaning Part III: Open Refine",
    "section": "",
    "text": "20.1 Refinr, Open Refine in R\nGather ’round kids and let me tell you a tale. Back in the previous century, Los Angeles Times journalists Sara Fritz and Dwight Morris wanted to answer this seemingly simple question: what do political campaigns spend their money on?\nWhile campaigns are required to list a purpose of each expenditure, the problem is that they can choose what words to use. There’s no standard dictionary or drop-down menu to choose from. Want to call that donut purchase “Food”? Sure. What about “Supplies for volunteers”? Works for me. How about “Meals”? Mom might disagree, but the FEC won’t.\nIn order to answer their initial question, the reporters had to standardize their data. In other words, all food-related purchases had to be labeled “Food”. All travel expenses had to be “Travel”. It took them months - many months - to do this for every federal candidate.\nI tell you this because if they had Open Refine, it would have taken them a week or two, not months.\nI did data standardization before Open Refine, and every time I think about it, I get mad.\nFortunately (unfortunately?) several columns in the data we’ll work with are flawed in the same way that the LA Times’ data was, so we can do this work in a better, faster way.\nWe’re going to explore two ways into Open Refine: Through R, and through Open Refine itself.\nWhat is Open Refine?\nOpen Refine is a software program that has tools – algorithms – that find small differences in text and helps you fix them quickly. How Open Refine finds those small differences is through something called clustering. The algorithms behind clustering are not exclusive to Open Refine, so they can be used elsewhere.\nEnter refinr, a package that contains the same clustering algorithms as Open Refine but all within R. Go ahead and install it if you haven’t already by opening the console and running install.packages(\"refinr\"). Then we can load libraries as we do.\nlibrary(tidyverse)\nlibrary(refinr)\nlibrary(janitor)\nLet’s load that Maryland state government grants and loan data that we’ve been working with, and to make our standardization work easier we’ll change all the grantees to upper-case.\nNow let’s try and group and count the number of grants by recipient. To make it a bit more manageable, let’s use another string function from stringr and filter for recipients that start with the uppercase “W” or lowercase “w” using the function str_detect() with a regular expression.\nThe filter function in the codeblock below says: look in the city column, and pluck out any value that starts with (the “^” symbol means “starts with”) a lowercase “w” OR (the vertical “|”, called a pipe, means OR) an uppercase “W”.\nmd_grant_loans |&gt;\n  group_by(Grantee) |&gt;\n  summarise(\n    count=n()\n  ) |&gt;\n  filter(str_detect(Grantee, '^w|^W')) |&gt;\n  arrange(Grantee)\n\n# A tibble: 365 × 2\n   Grantee                          count\n   &lt;chr&gt;                            &lt;int&gt;\n 1 W. F. DELAUTER & SON, INC.           1\n 2 W.R. GRACE & CO.                     1\n 3 WABASH MARKETS OF MARYLAND, INC.     1\n 4 WAGABOUT                             1\n 5 WAH,LLC                              1\n 6 WALBROOK MILL APARTMENTS             1\n 7 WALDEN SIERRA                        1\n 8 WALDEN SIERRA INC                    1\n 9 WALDEN SIERRA, INC.                 31\n10 WALDORF ARETE, LLC                   1\n# ℹ 355 more rows\nThere are several problems in this data that will prevent proper grouping and summarizing - you can see multiple versions of “Walden Sierra”, for example. We’ve learned several functions to fix this manually, but that could take awhile.\nBy using the Open Refine package for R, refinr, our hope is that it can identify and standardize the data with a little more ease.\nThe first merging technique that’s part of the refinr package we’ll try is the key_collision_merge.\nThe key collision merge function takes each string and extracts the key parts of it. It then puts every key in a bin based on the keys matching.\nOne rule you should follow when using this is: do not overwrite your original fields. Always work on a copy. If you overwrite your original field, how will you know if it did the right thing? How can you compare it to your original data? To follow this, I’m going to mutate a new field called clean_city and put the results of key collision merge there.\ncleaned_md_grant_loans &lt;- md_grant_loans |&gt;\n  mutate(grantee_clean=key_collision_merge(Grantee)) |&gt;\n  select(Grantee, grantee_clean, everything())\n\ncleaned_md_grant_loans\n\n# A tibble: 21,226 × 10\n   Grantee     grantee_clean Grantor `Zip Code` `Fiscal Year` Amount Description\n   &lt;chr&gt;       &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      \n 1 PRINCE GEO… PRINCE GEORG… Commer… 20772               2017 1.28e5 Maryland T…\n 2 ASSOCIATED… ASSOCIATED B… Depart… 21201               2010 9.34e4 Minority O…\n 3 WESTED/PUB… WESTED/PUBLI… Maryla… 94107-1242          2014 1.61e6 GRANTS FOR…\n 4 MARYLAND P… MARYLAND PAT… Depart… 21075               2010 2.14e5 Babies Bor…\n 5 ANACOSTIA … ANACOSTIA WA… Depart… 20710               2017 1.74e5 Payments m…\n 6 WASHINGTON… WASHINGTON C… Depart… 21740               2009 5.59e4 grant fund…\n 7 DOMESTIC V… DOMESTIC VIO… Boards… 21045               2014 1.72e5 Domestic V…\n 8 DACORE INV… DACORE INVES… MD Sma… 20601               2018 1.04e5 Maryland S…\n 9 MOUNT ST M… MOUNT ST MAR… Maryla… 21727               2015 1.75e6 Sellinger …\n10 OLNEY THEA… OLNEY THEATR… Depart… 20830               2010 2.16e5 Grant fund…\n# ℹ 21,216 more rows\n# ℹ 3 more variables: Category &lt;chr&gt;, `Fiscal Period` &lt;dbl&gt;, Date &lt;chr&gt;\nTo examine changes refinr made, let’s examine the changes it made to cities that start with the letter “W”.\ncleaned_md_grant_loans |&gt;\n  group_by(Grantee, grantee_clean) |&gt;\n  summarise(\n    count=n()\n  ) |&gt;\n  filter(str_detect(Grantee, '^w|^W')) |&gt;\n  arrange(Grantee)\n\n`summarise()` has grouped output by 'Grantee'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 365 × 3\n# Groups:   Grantee [365]\n   Grantee                          grantee_clean                    count\n   &lt;chr&gt;                            &lt;chr&gt;                            &lt;int&gt;\n 1 W. F. DELAUTER & SON, INC.       W. F. DELAUTER & SON, INC.           1\n 2 W.R. GRACE & CO.                 W.R. GRACE & CO.                     1\n 3 WABASH MARKETS OF MARYLAND, INC. WABASH MARKETS OF MARYLAND, INC.     1\n 4 WAGABOUT                         WAGABOUT                             1\n 5 WAH,LLC                          WAH,LLC                              1\n 6 WALBROOK MILL APARTMENTS         WALBROOK MILL APARTMENTS             1\n 7 WALDEN SIERRA                    WALDEN SIERRA, INC.                  1\n 8 WALDEN SIERRA INC                WALDEN SIERRA, INC.                  1\n 9 WALDEN SIERRA, INC.              WALDEN SIERRA, INC.                 31\n10 WALDORF ARETE, LLC               WALDORF ARETE, LLC                   1\n# ℹ 355 more rows\nYou can see several changes on the first page of results, including that refinr consolidated all the Walden Sierra entries into a single one in grantee_clean, which is pretty smart. Other potential changes, grouping together “WALTER’S ART MUSEUM” and “THE WALTERS ART MUSEUM”, didn’t happen. Key collision will do well with different cases, but all of our records are upper case.\nThere’s another merging algorithim that’s part of refinr that works a bit differently, called n_gram_merge(). Let’s try applying that one.\ncleaned_md_grant_loans &lt;- md_grant_loans |&gt;\n  mutate(grantee_clean=n_gram_merge(Grantee)) |&gt;\n  select(Grantee, grantee_clean, everything())\n\ncleaned_md_grant_loans\n\n# A tibble: 21,226 × 10\n   Grantee     grantee_clean Grantor `Zip Code` `Fiscal Year` Amount Description\n   &lt;chr&gt;       &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      \n 1 PRINCE GEO… PRINCE GEORG… Commer… 20772               2017 1.28e5 Maryland T…\n 2 ASSOCIATED… ASSOCIATED B… Depart… 21201               2010 9.34e4 Minority O…\n 3 WESTED/PUB… WESTED/PUBLI… Maryla… 94107-1242          2014 1.61e6 GRANTS FOR…\n 4 MARYLAND P… MARYLAND PAT… Depart… 21075               2010 2.14e5 Babies Bor…\n 5 ANACOSTIA … ANACOSTIA WA… Depart… 20710               2017 1.74e5 Payments m…\n 6 WASHINGTON… WASHINGTON C… Depart… 21740               2009 5.59e4 grant fund…\n 7 DOMESTIC V… DOMESTIC VIO… Boards… 21045               2014 1.72e5 Domestic V…\n 8 DACORE INV… DACORE INVES… MD Sma… 20601               2018 1.04e5 Maryland S…\n 9 MOUNT ST M… MOUNT ST MAR… Maryla… 21727               2015 1.75e6 Sellinger …\n10 OLNEY THEA… OLNEY THEATR… Depart… 20830               2010 2.16e5 Grant fund…\n# ℹ 21,216 more rows\n# ℹ 3 more variables: Category &lt;chr&gt;, `Fiscal Period` &lt;dbl&gt;, Date &lt;chr&gt;\nTo examine changes refinr made with this algorithm, let’s again look at recipients that start with the letter “W”. We see there wasn’t a substantial change from the previous method, and it even missed a few the first method got.\ncleaned_md_grant_loans |&gt;\n  group_by(Grantee, grantee_clean) |&gt;\n  summarise(\n    count=n()\n  ) |&gt;\n  filter(str_detect(Grantee, '^w|^W')) |&gt;\n  arrange(Grantee)\n\n`summarise()` has grouped output by 'Grantee'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 365 × 3\n# Groups:   Grantee [365]\n   Grantee                          grantee_clean                    count\n   &lt;chr&gt;                            &lt;chr&gt;                            &lt;int&gt;\n 1 W. F. DELAUTER & SON, INC.       W. F. DELAUTER & SON, INC.           1\n 2 W.R. GRACE & CO.                 W.R. GRACE & CO.                     1\n 3 WABASH MARKETS OF MARYLAND, INC. WABASH MARKETS OF MARYLAND, INC.     1\n 4 WAGABOUT                         WAGABOUT                             1\n 5 WAH,LLC                          WAH,LLC                              1\n 6 WALBROOK MILL APARTMENTS         WALBROOK MILL APARTMENTS             1\n 7 WALDEN SIERRA                    WALDEN SIERRA, INC.                  1\n 8 WALDEN SIERRA INC                WALDEN SIERRA, INC.                  1\n 9 WALDEN SIERRA, INC.              WALDEN SIERRA, INC.                 31\n10 WALDORF ARETE, LLC               WALDORF ARETE, LLC                   1\n# ℹ 355 more rows\nThis method also made some good changes, but not in every case. No single method will be perfect and often a combination is necessary.\nThat’s how you use the Open Refine r package, refinr.\nNow let’s upload the data to the interactive version of OpenRefine, which really shines at this task.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Data Cleaning Part III: Open Refine</span>"
    ]
  },
  {
    "objectID": "open-refine.html#manually-cleaning-data-with-open-refine",
    "href": "open-refine.html#manually-cleaning-data-with-open-refine",
    "title": "20  Data Cleaning Part III: Open Refine",
    "section": "20.2 Manually cleaning data with Open Refine",
    "text": "20.2 Manually cleaning data with Open Refine\nOpen Refine is free software. You should download and install it; the most recent version is 3.6.0. Refinr is great for quick things on smaller datasets that you can check to make sure it’s not up to any mischief.\nFor bigger datasets, Open Refine is the way to go. And it has a lot more tools than refinr does (by design).\nAfter you install it, run it. (If you are on a Mac it might tell you that it can’t run the program. Go to System Preferences -&gt; Security & Privacy -&gt; General and click “Open Anyway”.) Open Refine works in the browser, and the app spins up a small web server visible only on your computer to interact with it. A browser will pop up automatically.\nYou first have to import your data into a project. Click the choose files button and upload a csv of the Maryland state grants and loans.\n\n\n\n\n\n\n\n\n\nAfter your data is loaded into the app, you’ll get a screen to look over what the data looks like. On the top right corner, you’ll see a button to create the project. Click that.\n\n\n\n\n\n\n\n\n\nOpen Refine has many, many tools. We’re going to use one piece of it, as a tool for data cleaning. To learn how to use it, we’re going to clean the “Grantee” field.\nFirst, let’s make a copy of the original Grantee column so that we can preserve the original data while cleaning the new one.\nClick the dropdown arrow next to the Grantee column, choose “edit column” &gt; “Add column based on this column”:\n\n\n\n\n\n\n\n\n\nOn the window that pops up, type “grantee_clean” in the “new column name” field. Then hit the OK button. We’ll work on that new column.\n\n\n\n\n\n\n\n\n\nNow, let’s get to work cleaning the grantee_clean column.\nNext to the grantee_clean field name, click the down arrow, then facet, then text facet.\n\n\n\n\n\n\n\n\n\nAfter that, a new box will appear on the left. It tells us how many unique recipient_names there are: 8,956 (you may need to . And, there’s a button on the right of the box that says Cluster.\n\n\n\n\n\n\n\n\n\nClick the cluster button. A new window will pop up, a tool to help us identify things that need to be cleaned, and quickly clean them.\n\n\n\n\n\n\n\n\n\nThe default “method” used is a clustering algorithim called “key collision”, using the fingerprint function. This is the same method we used with the refinr package above.\nAt the top, you’ll see which method was used, and how many clusters that algorithm identified. There are several different methods, each of which work slightly differently and produce different results.\n\n\n\n\n\n\n\n\n\nThen, below that, you can see what those clusters are. Right away, we can see how useful this program is. It identified 23 rows that have some variation on “University of Maryland - Baltimore” in the grantee_clean field. It proposed changing them all to “UNIVERSITY OF MARYLAND BALTIMORE”.\nUsing human judgement, you can say if you agree with the cluster. If you do, click the “merge” checkbox. When it merges, the new result will be what it says in New Cell Value. Most often, that’s the row with the most common result. You also can manually edit the “New Cell Value” if you want it to be something else:\nNow begins the fun part: You have to look at all the clusters found and decide if they are indeed valid. The key collision method is very good, and very conservative. You’ll find that most of them are usually valid.\nBe careful! If you merge two things that aren’t supposed to be together, it will change your data in a way that could lead to inaccurate results.\nWhen you’re done, click Merge Selected and Re-Cluster.\nIf any new clusters come up, evaluate them. Repeat until either no clusters come up or the clusters that do come up are ones you reject.\nNow. Try a new method, maybe the “nearest neighbor levenshtein” method. Notice that it finds even more clusters - using a slightly different approach.\nRinse and repeat.\nYou’ll keep doing this, and if the dataset is reasonably clean, you’ll find the end.\nWhen you’re finished cleaning, click “Merge Selected & Close”.\nThen, export the data as a csv so you can load it back into R.\n\n\n\n\n\n\n\n\n\nA question for all data analysts – if the dataset is bad enough, can it ever be cleaned?\nThere’s no good answer. You have to find it yourself.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Data Cleaning Part III: Open Refine</span>"
    ]
  }
]