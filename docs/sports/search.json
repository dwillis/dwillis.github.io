[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sports Data Analysis and Visualization",
    "section": "",
    "text": "1 Throwing cold water on hot takes\nWhy do teams struggle? There are lots of potential reasons: injuries, athletes in the wrong position, poor execution. Or it could be external factors: well-prepared opponents, the weather, the altitude or, of course, the refs.\nYou could turn the question around: why do teams succeed? Again, there are plenty of possibilities that get tossed around on talk radio, on the sports pages and across social media. A lot of hot takes.\nThe more fundamental question that this course will empower you to answer is this: what do teams and athletes do? Using data, you’ll learn to ask questions and visualize the answers, ranging across sports and scenarios. What did the 2021-22 Maryland men’s lacrosse team do well en route to the national championship? How has the transfer portal (and additional eligibility) changed the nature of programs? In football, do penalties have any relationship on scoring?\nTo get into these and other questions, we’ll use a lot of different tools and techniques, but this class rests on three pillars:\nDo you need to be a math whiz to read this book? No. I’m not one either. What we’re going to look at is pretty basic, but that’s also why it’s so powerful.\nDo you need to be a computer science major to write code? Nope. I’m not one of those either. But anyone can think logically, and write simple code that is repeatable and replicable.\nDo you need to be an artist to create compelling visuals? I think you see where this is going. No. I can barely draw stick figures, but I’ve been paid to make graphics in my career. With a little graphic design know how, you can create publication worthy graphics with code.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Throwing cold water on hot takes</span>"
    ]
  },
  {
    "objectID": "index.html#requirements-and-conventions",
    "href": "index.html#requirements-and-conventions",
    "title": "Sports Data Analysis and Visualization",
    "section": "1.1 Requirements and Conventions",
    "text": "1.1 Requirements and Conventions\nThis book is all in the R statistical language. To follow along, you’ll do the following:\n\nInstall the R language on your computer. Go to the R Project website, click download R and select a mirror closest to your location. Then download the version for your computer.\nInstall RStudio Desktop. The free version is great.\n\nGoing forward, you’ll see passages like this:\n\ninstall.packages(\"tidyverse\")\n\nDon’t do it now, but that is code that you’ll need to run in your RStudio. When you see that, you’ll know what to do: click the green arrow.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Throwing cold water on hot takes</span>"
    ]
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "Sports Data Analysis and Visualization",
    "section": "1.2 About this book",
    "text": "1.2 About this book\nThis book is the collection of class materials for the Fall 2024 JOUR479X course in the Philip Merrill College of Journalism at the University of Maryland. There’s some things you should know about it:\n\nIt is free for students.\nThe topics will remain the same but the text is going to be constantly tinkered with.\nWhat is the work of the author is copyright Derek Willis 2024 & Matt Waite 2019-2023.\nThe text is Attribution-NonCommercial-ShareAlike 4.0 International Creative Commons licensed. That means you can share it and change it, but only if you share your changes with the same license and it cannot be used for commercial purposes. I’m not making money on this so you can’t either.\nAs such, the whole book – authored in Quarto – is open sourced on Github. Pull requests welcomed!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Throwing cold water on hot takes</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "2  The very basics",
    "section": "",
    "text": "2.1 Adding libraries, part 1\nR is a programming language, one specifically geared toward statistical analysis. Like all programming languages, it has certain built-in functions and you can interact with it in multiple ways. The first, and most basic, is the console.\nThink of the console like talking directly to R. It’s direct, but it has some drawbacks and some quirks we’ll get into later. For now, try typing this into the console and hit enter:\nCongrats, you’ve run some code. It’s not very complex, and you knew the answer before hand, but you get the idea. We can compute things. We can also store things. In programming languages, these are called variables. We can assign things to variables using &lt;-. And then we can do things with them. The &lt;- is a called an assignment operator.\nNow assign a different number to the variable number. Try running number * number again. Get what you expected?\nWe can have as many variables as we can name. We can even reuse them (but be careful you know you’re doing that or you’ll introduce errors). Try this in your console.\nWe can store anything in a variable. A whole table. An array of numbers. Every college basketball game played in the last 10 years. A single word. A whole book. All the books of the 18th century. They’re really powerful. We’ll explore them at length.\nThe real strength of any given programming language is the external libraries that power it. The base language can do a lot, but it’s the external libraries that solve many specific problems – even making the base language easier to use.\nFor this class, we’re going to need several external libraries.\nThe first library we’re going to use is called Swirl. So in the console, type install.packages('swirl') and hit enter. That installs swirl.\nNow, to use the library, type library(swirl) and hit enter. That loads swirl. Then type swirl() and hit enter. Now you’re running swirl. Follow the directions on the screen. When you are asked, you want to install course 1 R Programming: The basics of programming in R. Then, when asked, you want to do option 1, R Programming, in that course.\nWhen you are finished with the course – it will take just a few minutes – it will first ask you if you want credit on Coursera. You do not. Then type 0 to exit (it will not be very clear that’s what you do when you are done).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The very basics</span>"
    ]
  },
  {
    "objectID": "intro.html#adding-libraries-part-2",
    "href": "intro.html#adding-libraries-part-2",
    "title": "2  The very basics",
    "section": "2.2 Adding libraries, part 2",
    "text": "2.2 Adding libraries, part 2\nWe’ll mostly use two libraries for analysis – dplyr and ggplot2. To get them, and several other useful libraries, we can install a single collection of libraries called the tidyverse. Type this into your console: install.packages('tidyverse')\nNOTE: This is a pattern. You should always install libraries in the console.\nThen, to help us with learning and replication, we’re going to use R Notebooks. So we need to install that library. Type this into your console: install.packages('rmarkdown')",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The very basics</span>"
    ]
  },
  {
    "objectID": "intro.html#notebooks",
    "href": "intro.html#notebooks",
    "title": "2  The very basics",
    "section": "2.3 Notebooks",
    "text": "2.3 Notebooks\nFor the rest of the class, we’re going to be working in notebooks. In notebooks, you will both run your code and explain each step, much as I am doing here.\nTo start a notebook, you click on the green plus in the top left corner and go down to R Notebook. Do that now.\n\n\n\n\n\n\n\n\n\nYou will see that the notebook adds a lot of text for you. It tells you how to work in notebooks – and you should read it. The most important parts are these:\nTo add text, simply type. To add code you can click on the Insert button on the toolbar or by pressing Cmd+Option+I on Mac or Ctl+Alt+I on Windows.\nHighlight all that text and delete it. You should have a blank document. This document is called a R Markdown file – it’s a special form of text, one that you can style, and one you can include R in the middle of it. Markdown is a simple markup format that you can use to create documents. So first things first, let’s give our notebook a big headline. Add this:\n# My awesome notebook\nNow, under that, without any markup, just type This is my awesome notebook.\nUnder that, you can make text bold by writing It is **really** awesome.\nIf you want it italics, just do this on the next line: No, it's _really_ awesome. I swear.\nTo see what it looks like without the markup, click the Preview or Knit button in the toolbar. That will turn your notebook into a webpage, with the formatting included.\nThroughout this book, we’re going to use this markdown to explain what we are doing and, more importantly, why we are doing it. Explaining your thinking is a vital part of understanding what you are doing.\nThat explaination, plus the code, is the real power of notebooks. To add a block of code, follow the instructions from above: click on the Insert button on the toolbar or by pressing Cmd+Option+I on Mac or Ctl+Alt+I on Windows.\nIn that window, use some of the code from above and add two numbers together. To see it run, click the green triangle on the right. That runs the chunk. You should see the answer to your addition problem.\nAnd that, just that, is the foundation you need to start this book.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The very basics</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "3  Data, structures and types",
    "section": "",
    "text": "3.1 Rows and columns\nData are everywhere (and data is plural of datum, thus the use of are in that statement). It surrounds you. Every time you use your phone, you are creating data. Lots of it. Your online life. Any time you buy something. It’s everywhere. Sports, like life, is no different. Sports is drowning in data, and more comes along all the time.\nIn sports, and in this class, we’ll be dealing largely with two kinds of data: event level data and summary data. It’s not hard to envision event level data in sports. A pitch in baseball. A hit. A play in football. A pass in soccer. They are the events that make up the game. Combine them together – summarize them – and you’ll have some notion of how the game went. What we usually see is summary data – who wants to scroll through 50 pitches to find out a player went 2-3 with a double and an RBI? Who wants to scroll through hundreds of pitches to figure out the Rays beat the Yankees?\nTo start with, we need to understand the shape of data.\nData, oversimplifying it a bit, is information organized. Generally speaking, it’s organized into rows and columns. Rows, generally, are individual elements. A team. A player. A game. Columns, generally, are components of the data, sometimes called variables. So if each row is a player, the first column might be their name. The second is their position. The third is their batting average. And so on.\nOne of the critical components of data analysis, especially for beginners, is having a mental picture of your data. What does each row mean? What does each column in each row signify? How many rows do you have? How many columns?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data, structures and types</span>"
    ]
  },
  {
    "objectID": "data.html#types",
    "href": "data.html#types",
    "title": "3  Data, structures and types",
    "section": "3.2 Types",
    "text": "3.2 Types\nThere are scores of data types in the world, and R has them. In this class, we’re primarily going to be dealing with data frames, and each element of our data frames will have a data type.\nTypically, they’ll be one of four types of data:\n\nNumeric: a number, like the number of touchdown passes in a season or a batting average.\nCharacter: Text, like a name, a team, a conference.\nDate: Fully formed dates – 2019-01-01 – have a special date type. Elements of a date, like a year (ex. 2019) are not technically dates, so they’ll appear as numeric data types.\nLogical: Rare, but every now and then we’ll have a data type that’s Yes or No, True or False, etc.\n\nQuestion: Is a zip code a number? Is a jersey number a number? Trick question, because the answer is no. Numbers are things we do math on. If the thing you want is not something you’re going to do math on – can you add two phone numbers together? – then make it a character type. If you don’t, most every software system on the planet will drop leading zeros. For example, every zip code in Boston starts with 0. If you record that as a number, your zip code will become a four digit number, which isn’t a zip code anymore.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data, structures and types</span>"
    ]
  },
  {
    "objectID": "data.html#a-simple-way-to-get-data",
    "href": "data.html#a-simple-way-to-get-data",
    "title": "3  Data, structures and types",
    "section": "3.3 A simple way to get data",
    "text": "3.3 A simple way to get data\nOne good thing about sports is that there’s lots of interest in it. And that means there’s outlets that put sports data on the internet. Now I’m going to show you a trick to getting it easily.\nThe site sports-reference.com takes NCAA (and other league) stats and puts them online. For instance, here’s their page on Maryland’s men’s basketball’s game logs, which you should open now.\nNow, in a new tab, log into Google Docs/Drive and open a new spreadsheet (type sheet.new in the url bar). In the first cell of the first row, copy and paste this formula in:\n=IMPORTHTML(\"https://www.sports-reference.com/cbb/schools/maryland/men/2025-gamelogs.html\", \"table\", 1)\nThe spreadsheet might request that you “allow access” to complete the import. You should. If it worked right, you’ve got the data from that page in a spreadsheet.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data, structures and types</span>"
    ]
  },
  {
    "objectID": "data.html#cleaning-the-data",
    "href": "data.html#cleaning-the-data",
    "title": "3  Data, structures and types",
    "section": "3.4 Cleaning the data",
    "text": "3.4 Cleaning the data\nThe first thing we need to do is recognize that we don’t have data, really. We have the results of a formula. You can tell by putting your cursor on that field, where you’ll see the formula again. This is where you’d look:\n\n\n\n\n\n\n\n\n\nThe solution is easy:\nEdit &gt; Select All or type command/control A Edit &gt; Copy or type command/control c Edit &gt; Paste Special &gt; Values Only or type command/control shift v\nYou can verify that it worked by looking in that same row 1 column A, where you’ll see the formula is gone.\n\n\n\n\n\n\n\n\n\nNow you have data, but your headers are all wrong. You want your headers to be one line – not two, like they have. And the header names repeat – first for our team, then for theirs. So you have to change each header name to be UsORB or TeamORB and OpponentORB instead of just ORB.\nAfter you’ve done that, note we have repeating headers. There’s two ways to deal with that – you could just hightlight it and go up to Edit &gt; Delete Rows XX-XX depending on what rows you highlighted. That’s the easy way with our data.\nBut what if you had hundreds of repeating headers like that? Deleting them would take a long time.\nYou can use sorting to get rid of anything that’s not data. So click on Data &gt; Sort Range. You’ll want to check the “Data has header row” field. Then hit Sort.\n\n\n\n\n\n\n\n\n\nNow all you need to do is search through the data for where your junk data – extra headers, blanks, etc. – got sorted and delete it. After you’ve done that, you can export it for use in R. Go to File &gt; Download as &gt; Comma Separated Values. Remember to put it in the same directory as your R Notebook file so you can import the data easily.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data, structures and types</span>"
    ]
  },
  {
    "objectID": "aggregates.html",
    "href": "aggregates.html",
    "title": "4  Aggregates",
    "section": "",
    "text": "4.1 Basic data analysis: Group By and Count\nR is a statistical programming language that is purpose built for data analysis.\nBase R does a lot, but there are a mountain of external libraries that do things to make R better/easier/more fully featured. We already installed the tidyverse – or you should have if you followed the instructions for the last assignment – which isn’t exactly a library, but a collection of libraries. Together, they make up the tidyverse. Individually, they are extraordinarily useful for what they do. We can load them all at once using the tidyverse name, or we can load them individually. Let’s start with individually.\nThe two libraries we are going to need for this assignment are readr and dplyr. The library readr reads different types of data in as a dataframe. For this assignment, we’re going to read in csv data or Comma Separated Values data. That’s data that has a comma between each column of data.\nThen we’re going to use dplyr to analyze it.\nTo use a library, you need to import it. Good practice – one I’m going to insist on – is that you put all your library steps at the top of your notebook.\nThat code looks like this:\nTo load them both, you need to run that code twice:\nYou can keep doing that for as many libraries as you need. I’ve seen notebooks with 10 or more library imports.\nBut the tidyverse has a neat little trick. We can load most of the libraries we’ll need for the whole semester with one line:\nFrom now on, if that’s not the first line of your notebook, you’re probably doing it wrong.\nThe first thing we need to do is get some data to work with. We do that by reading it in. In our case, we’re going to read data from a csv file – a comma-separated values file.\nThe CSV file we’re going to read from is a Basketball Reference page of advanced metrics for NBA players this past season. The Sports Reference sites are a godsend of data, a trove of stuff, and we’re going to use it a lot in this class.\nSo step 2, after setting up our libraries, is most often going to be importing data. In order to analyze data, we need data, so it stands to reason that this would be something we’d do very early.\nThe code looks something like this, but hold off copying it just yet:\nnbaplayers &lt;- read_csv(\"~/SportsData/nbaadvancedplayers2425.csv\")\nLet’s unpack that.\nThe first part – nbaplayers – is the name of your variable. A variable is just a name of a thing that stores stuff. In this case, our variable is a data frame, which is R’s way of storing data (technically it’s a tibble, which is the tidyverse way of storing data, but the differences aren’t important and people use them interchangeably). We can call this whatever we want. I always want to name data frames after what is in it. In this case, we’re going to import a dataset of NBA players. Variable names, by convention are one word all lower case. You can end a variable with a number, but you can’t start one with a number.\nThe &lt;- bit is the variable assignment operator. It’s how we know we’re assigning something to a word. Think of the arrow as saying “Take everything on the right of this arrow and stuff it into the thing on the left.” So we’re creating an empty vessel called nbaplayers and stuffing all this data into it.\nThe read_csv bits are pretty obvious, except for one thing. What happens in the quote marks is the path to the data. In there, I have to tell R where it will find the data. The easiest thing to do, if you are confused about how to find your data, is to put your data in the same folder as as your notebook (you’ll have to save that notebook first). If you do that, then you just need to put the name of the file in there (nbaadvancedplayers2122.csv). In my case, in my home directory (that’s the ~ part), there is a folder called SportsData that has the file called nbaadvancedplayers2122.csv in it. Some people – insane people – leave the data in their downloads folder. The data path then would be ~/Downloads/nameofthedatafilehere.csv on PC or Mac.\nWhat you put in there will be different from mine. So your first task is to import the data.\nnbaplayers &lt;- read_csv(\"data/nbaadvancedplayers2425.csv\")\n\nRows: 735 Columns: 30\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Player, Team, Pos, Player-additional\ndbl (25): Rk, Age, G, GS, MP, PER, TS%, 3PAr, FTr, ORB%, DRB%, TRB%, AST%, S...\nlgl  (1): Awards\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nNow we can inspect the data we imported. What does it look like? To do that, we use head(nbaplayers) to show the headers and the first six rows of data. If we wanted to see them all, we could just simply enter nbaplayers and run it.\nTo get the number of records in our dataset, we run nrow(nbaplayers)\nhead(nbaplayers)\n\n# A tibble: 6 × 30\n     Rk Player        Age Team  Pos       G    GS    MP   PER `TS%` `3PAr`   FTr\n  &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1     1 Mikal Brid…    28 NYK   SF       82    82  3036  14   0.585  0.391 0.1  \n2     2 Josh Hart      29 NYK   SG       77    77  2897  16.5 0.611  0.327 0.266\n3     3 Anthony Ed…    23 MIN   SG       79    79  2871  20.1 0.595  0.503 0.308\n4     4 Devin Book…    28 PHO   SG       75    75  2795  19.3 0.589  0.388 0.34 \n5     5 James Hard…    35 LAC   PG       79    79  2789  20   0.582  0.516 0.446\n6     6 DeMar DeRo…    35 SAC   SF       77    77  2768  17.7 0.569  0.196 0.337\n# ℹ 18 more variables: `ORB%` &lt;dbl&gt;, `DRB%` &lt;dbl&gt;, `TRB%` &lt;dbl&gt;, `AST%` &lt;dbl&gt;,\n#   `STL%` &lt;dbl&gt;, `BLK%` &lt;dbl&gt;, `TOV%` &lt;dbl&gt;, `USG%` &lt;dbl&gt;, OWS &lt;dbl&gt;,\n#   DWS &lt;dbl&gt;, WS &lt;dbl&gt;, `WS/48` &lt;dbl&gt;, OBPM &lt;dbl&gt;, DBPM &lt;dbl&gt;, BPM &lt;dbl&gt;,\n#   VORP &lt;dbl&gt;, Awards &lt;lgl&gt;, `Player-additional` &lt;chr&gt;\nnrow(nbaplayers)\n\n[1] 735\nAnother way to look at nrow – we have 735 players from this season in our dataset.\nWhat if we wanted to know how many players there were by position? To do that by hand, we’d have to take each of the 735 records and sort them into a pile. We’d put them in groups and then count them.\ndplyr has a group by function in it that does just this. A massive amount of data analysis involves grouping like things together at some point. So it’s a good place to start.\nSo to do this, we’ll take our dataset and we’ll introduce a new operator: |&gt;. The best way to read that operator, in my opinion, is to interpret that as “and then do this.”\nAfter we group them together, we need to count them. We do that first by saying we want to summarize our data (a count is a part of a summary). To get a summary, we have to tell it what we want. So in this case, we want a count. To get that, let’s create a thing called total and set it equal to n(), which is dplyrs way of counting something.\nHere’s the code:\nnbaplayers |&gt; \n  group_by(Pos) |&gt;\n  summarise(\n    total = n()\n  )\n\n# A tibble: 5 × 2\n  Pos   total\n  &lt;chr&gt; &lt;int&gt;\n1 C       136\n2 PF      134\n3 PG      135\n4 SF      139\n5 SG      191\nSo let’s walk through that. We start with our dataset – nbaplayers – and then we tell it to group the data by a given field in the data which we get by looking at either the output of head or you can look in the environment where you’ll see nbaplayers.\nIn this case, we wanted to group together positions, signified by the field name Pos. After we group the data, we need to count them up. In dplyr, we use summarize which can do more than just count things. Inside the parentheses in summarize, we set up the summaries we want. In this case, we just want a count of the positions: total = n(), says create a new field, called total and set it equal to n(), which might look weird, but it’s common in stats. The number of things in a dataset? Statisticians call in n. There are n number of players in this dataset. So n() is a function that counts the number of things there are.\nAnd when we run that, we get a list of positions with a count next to them. But it’s not in any order. So we’ll add another And Then Do This |&gt; and use arrange. Arrange does what you think it does – it arranges data in order. By default, it’s in ascending order – smallest to largest. But if we want to know the position with the most players, we need to sort it in descending order. That looks like this:\nnbaplayers |&gt;\n  group_by(Pos) |&gt;\n  summarise(\n    total = n()\n  ) |&gt; arrange(desc(total))\n\n# A tibble: 5 × 2\n  Pos   total\n  &lt;chr&gt; &lt;int&gt;\n1 SG      191\n2 SF      139\n3 C       136\n4 PG      135\n5 PF      134\nSo the most common position in the NBA? Shooting guard, followed by small forward.\nWe can, if we want, group by more than one thing. Which team has the most of a single position? To do that, we can group by the team – called Tm in the data – and position, or Pos in the data:\nnbaplayers |&gt;\n  group_by(Team, Pos) |&gt;\n  summarise(\n    total = n()\n  ) |&gt; arrange(desc(total))\n\n`summarise()` has grouped output by 'Team'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 158 × 3\n# Groups:   Team [32]\n   Team  Pos   total\n   &lt;chr&gt; &lt;chr&gt; &lt;int&gt;\n 1 2TM   SG       20\n 2 2TM   PG       15\n 3 2TM   C        14\n 4 2TM   PF       14\n 5 2TM   SF       14\n 6 CHO   SG       11\n 7 OKC   SG        8\n 8 ATL   SG        7\n 9 CLE   SG        7\n10 GSW   SG        7\n# ℹ 148 more rows\nSo wait, what team is 2TM?\nValuable lesson: whoever collects the data has opinions on how to solve problems. In this case, Basketball Reference, when a player get’s traded, records stats for the player’s first team, their second team, and a combined season total for a team called 2TM, meaning Total. Is there a team abbreviated 2TM? No. So ignore them here.\nCharlotte had 11 shooting guards! You can learn a bit about how a team is assembled by looking at these simple counts.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Aggregates</span>"
    ]
  },
  {
    "objectID": "aggregates.html#basic-data-analysis-group-by-and-count",
    "href": "aggregates.html#basic-data-analysis-group-by-and-count",
    "title": "4  Aggregates",
    "section": "",
    "text": "For this walkthrough:\n   Download csv file",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Aggregates</span>"
    ]
  },
  {
    "objectID": "aggregates.html#other-aggregates-mean-and-median",
    "href": "aggregates.html#other-aggregates-mean-and-median",
    "title": "4  Aggregates",
    "section": "4.2 Other aggregates: Mean and median",
    "text": "4.2 Other aggregates: Mean and median\nIn the last example, we grouped some data together and counted it up, but there’s so much more you can do. You can do multiple measures in a single step as well.\nSticking with our NBA player data, we can calculate any number of measures inside summarize. Here, we’ll use R’s built in mean and median functions to calculate … well, you get the idea.\nLet’s look just a the number of minutes each position gets.\n\nnbaplayers |&gt;\n  group_by(Pos) |&gt;\n  summarise(\n    count = n(),\n    mean_minutes = mean(MP),\n    median_minutes = median(MP)\n  )\n\n# A tibble: 5 × 4\n  Pos   count mean_minutes median_minutes\n  &lt;chr&gt; &lt;int&gt;        &lt;dbl&gt;          &lt;dbl&gt;\n1 C       136         856.           606.\n2 PF      134         894.           724 \n3 PG      135         962.           717 \n4 SF      139         866.           648 \n5 SG      191         935.           732 \n\n\nLet’s look at centers. The average center plays 855 minutes and the median is 606 minutes.\nWhy?\nLet’s let sort help us.\n\nnbaplayers |&gt; arrange(desc(MP))\n\n# A tibble: 735 × 30\n      Rk Player       Age Team  Pos       G    GS    MP   PER `TS%` `3PAr`   FTr\n   &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1     1 Mikal Bri…    28 NYK   SF       82    82  3036  14   0.585  0.391 0.1  \n 2     2 Josh Hart     29 NYK   SG       77    77  2897  16.5 0.611  0.327 0.266\n 3     3 Anthony E…    23 MIN   SG       79    79  2871  20.1 0.595  0.503 0.308\n 4     4 Devin Boo…    28 PHO   SG       75    75  2795  19.3 0.589  0.388 0.34 \n 5     5 James Har…    35 LAC   PG       79    79  2789  20   0.582  0.516 0.446\n 6     6 DeMar DeR…    35 SAC   SF       77    77  2768  17.7 0.569  0.196 0.337\n 7     7 Trae Young    26 ATL   PG       76    76  2739  18.3 0.567  0.467 0.408\n 8     8 Tyler Her…    25 MIA   SG       77    77  2725  19.7 0.605  0.486 0.237\n 9     9 OG Anunoby    27 NYK   PF       74    74  2706  15.4 0.591  0.448 0.22 \n10    10 Jalen Gre…    22 HOU   SG       82    82  2697  15.1 0.544  0.46  0.234\n# ℹ 725 more rows\n# ℹ 18 more variables: `ORB%` &lt;dbl&gt;, `DRB%` &lt;dbl&gt;, `TRB%` &lt;dbl&gt;, `AST%` &lt;dbl&gt;,\n#   `STL%` &lt;dbl&gt;, `BLK%` &lt;dbl&gt;, `TOV%` &lt;dbl&gt;, `USG%` &lt;dbl&gt;, OWS &lt;dbl&gt;,\n#   DWS &lt;dbl&gt;, WS &lt;dbl&gt;, `WS/48` &lt;dbl&gt;, OBPM &lt;dbl&gt;, DBPM &lt;dbl&gt;, BPM &lt;dbl&gt;,\n#   VORP &lt;dbl&gt;, Awards &lt;lgl&gt;, `Player-additional` &lt;chr&gt;\n\n\nThe player with the most minutes on the floor is a small forward. So that means there’s Mikal Bridges rolling up 3,036 minutes in a season, and then there’s OKC sensation Alex Reese. Never heard of Alex Reese? Might be because he logged two minute in one game this season.\nThat’s a huge difference.\nSo when choosing a measure of the middle, you have to ask yourself – could I have extremes? Because a median won’t be sensitive to extremes. It will be the point at which half the numbers are above and half are below. The average or mean will be a measure of the middle, but if you have a bunch of pine riders and then one ironman superstar, the average will be wildly skewed.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Aggregates</span>"
    ]
  },
  {
    "objectID": "aggregates.html#even-more-aggregates",
    "href": "aggregates.html#even-more-aggregates",
    "title": "4  Aggregates",
    "section": "4.3 Even more aggregates",
    "text": "4.3 Even more aggregates\nThere’s a ton of things we can do in summarize – we’ll work with more of them as the course progresses – but here’s a few other questions you can ask.\nWhich position in the NBA plays the most minutes? And what is the highest and lowest minute total for that position? And how wide is the spread between minutes? We can find that with sum to add up the minutes to get the total minutes, min to find the minimum minutes, max to find the maximum minutes and sd to find the standard deviation in the numbers.\n\nnbaplayers |&gt; \n  group_by(Pos) |&gt; \n  summarise(\n    total = sum(MP), \n    avgminutes = mean(MP), \n    minminutes = min(MP),\n    maxminutes = max(MP),\n    stdev = sd(MP)) |&gt; arrange(desc(total))\n\n# A tibble: 5 × 6\n  Pos    total avgminutes minminutes maxminutes stdev\n  &lt;chr&gt;  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 SG    178577       935.          3       2897  786.\n2 PG    129889       962.          4       2789  791.\n3 SF    120323       866.          5       3036  795.\n4 PF    119786       894.          2       2706  760.\n5 C     116389       856.          3       2674  734.\n\n\nSo again, no surprise, shooting guards spend the most minutes on the floor in the NBA. They average 934 minutes, but we noted why that’s trouble. The minimum is a one-minute wonder, max is some team failing at load management, and the standard deviation is a measure of how spread out the data is. In this case, not the highest spread among positions, but pretty high. So you know you’ve got some huge minutes players and a bunch of bench players.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Aggregates</span>"
    ]
  },
  {
    "objectID": "mutating.html",
    "href": "mutating.html",
    "title": "5  Mutating data",
    "section": "",
    "text": "5.1 A more complex example\nOne of the most common data analysis techniques is to look at change over time. The most common way of comparing change over time is through percent change. The math behind calculating percent change is very simple, and you should know it off the top of your head. The easy way to remember it is:\n(new - old) / old\nOr new minus old divided by old. Your new number minus the old number, the result of which is divided by the old number. To do that in R, we can use dplyr and mutate to calculate new metrics in a new field using existing fields of data.\nSo first we’ll import the tidyverse so we can read in our data and begin to work with it.\nNow you’ll need a common and simple dataset of total attendance at NCAA football games over the last few seasons.\nYou’ll import it something like this.\nIf you want to see the first six rows – handy to take a peek at your data – you can use the function head.\nThe code to calculate percent change is pretty simple. Remember, with summarize, we used n() to count things. With mutate, we use very similar syntax to calculate a new value using other values in our dataset. So in this case, we’re trying to do (new-old)/old, but we’re doing it with fields. If we look at what we got when we did head, you’ll see there’s `2024` as the new data, and we’ll use `2023` as the old data. So we’re looking at one year. Then, to help us, we’ll use arrange again to sort it, so we get the fastest growing school over one year.\nWhat do we see right away? Do those numbers look like we expect them to? No. They’re a decimal expressed as a percentage. So let’s fix that by multiplying by 100.\nNow, does this ordering do anything for us? No. Let’s fix that with arrange.\nSo who had the most growth in 2024 compared to the year before? SMU, followed by Ohio State and Colorado State. How about Central Michigan at #6!\nThere’s metric in basketball that’s easy to understand – shooting percentage. It’s the number of shots made divided by the number of shots attempted. Simple, right? Except it’s a little too simple. Because what about three point shooters? They tend to be more vailable because the three point shot is worth more. What about players who get to the line? In shooting percentage, free throws are nowhere to be found.\nBasketball nerds, because of these weaknesses, have created a new metric called True Shooting Percentage. True shooting percentage takes into account all aspects of a players shooting to determine who the real shooters are.\nUsing dplyr and mutate, we can calculate true shooting percentage. So let’s look at a new dataset, one of every college basketball player’s season stats in 2024-25 season. It’s a dataset of 5,688 players, and we’ve got 59 variables – one of them is True Shooting Percentage, but we’re going to ignore that.\nImport it like this:\nplayers &lt;- read_csv(\"data/players25.csv\")\n\nRows: 5818 Columns: 57\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (10): Team, Player, Class, Height, Hometown, High School, Summary, Pos, ...\ndbl (47): #, Weight, G, GS, MP, FG, FGA, FG%, 3P, 3PA, 3P%, 2P, 2PA, 2P%, eF...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nThe basic true shooting percentage formula is (Points / (2*(FieldGoalAttempts + (.44 * FreeThrowAttempts)))) * 100. Let’s talk that through. Points divided by a lot. It’s really field goal attempts plus 44 percent of the free throw attempts. Why? Because that’s about what a free throw is worth, compared to other ways to score. After adding those things together, you double it. And after you divide points by that number, you multiply the whole lot by 100.\nIn our data, we need to be able to find the fields so we can complete the formula. To do that, one way is to use the Environment tab in R Studio. In the Environment tab is a listing of all the data you’ve imported, and if you click the triangle next to it, it’ll list all the field names, giving you a bit of information about each one.\nSo what does True Shooting Percentage look like in code?\nLet’s think about this differently. Who had the best true shooting season last year?\nplayers |&gt;\n  mutate(trueshooting = (PTS/(2*(FGA + (.44*FTA))))*100) |&gt;\n  arrange(desc(trueshooting))\n\n# A tibble: 5,818 × 58\n   Team    Player   `#` Class Height Weight Hometown `High School` Summary     G\n   &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;         &lt;chr&gt;   &lt;dbl&gt;\n 1 Wiscon… Isaac…    15 JR    6-2       165 Oregon,… Oregon (WI)   0.2 Pt…    13\n 2 TCU Ho… Cole …    35 SR    6-3       180 San Jos… Bellarmine C… 3.0 Pt…     1\n 3 TCU Ho… Drew …    30 FR    6-2       180 Dallas,… Highland Par… 1.5 Pt…     2\n 4 Stanfo… Derin…     1 SO    6-4       190 Istanbu… The Ashevill… 0.6 Pt…     5\n 5 St. Bo… Jack …    15 SO    6-0       172 Olean, … Olean (NY)    3.0 Pt…     1\n 6 Seattl… Eric …    19 FR    5-10      167 Beijing… Ruamrudee In… 3.0 Pt…     1\n 7 Samfor… Corey…    10 FR    6-4       175 Fairbur… Landmark Chr… 1.5 Pt…     2\n 8 Queens… Aneek…    85 SO    5-10      145 Foster … San Mateo (C… 3.0 Pt…     1\n 9 Old Do… CJ Pa…    15 FR    6-6       182 Norfolk… Maury (VA)    1.0 Pt…     3\n10 Oklaho… Jake …    30 SR    6-3       187 Norman,… Loyola Acade… 0.4 Pt…     7\n# ℹ 5,808 more rows\n# ℹ 48 more variables: GS &lt;dbl&gt;, MP &lt;dbl&gt;, FG &lt;dbl&gt;, FGA &lt;dbl&gt;, `FG%` &lt;dbl&gt;,\n#   `3P` &lt;dbl&gt;, `3PA` &lt;dbl&gt;, `3P%` &lt;dbl&gt;, `2P` &lt;dbl&gt;, `2PA` &lt;dbl&gt;, `2P%` &lt;dbl&gt;,\n#   `eFG%` &lt;dbl&gt;, FT &lt;dbl&gt;, FTA &lt;dbl&gt;, `FT%` &lt;dbl&gt;, ORB &lt;dbl&gt;, DRB &lt;dbl&gt;,\n#   TRB &lt;dbl&gt;, AST &lt;dbl&gt;, STL &lt;dbl&gt;, BLK &lt;dbl&gt;, TOV &lt;dbl&gt;, PF &lt;dbl&gt;, PTS &lt;dbl&gt;,\n#   Pos &lt;chr&gt;, PER &lt;dbl&gt;, `TS%` &lt;dbl&gt;, `3PAr` &lt;dbl&gt;, FTr &lt;dbl&gt;, PProd &lt;dbl&gt;,\n#   `ORB%` &lt;dbl&gt;, `DRB%` &lt;dbl&gt;, `TRB%` &lt;dbl&gt;, `AST%` &lt;dbl&gt;, `STL%` &lt;dbl&gt;, …\nYou’ll be forgiven if you did not hear about Winthrop’s shooting sensation Henry Harrison. He played in seven games, took three shots and actually hit them all. They all happened to be three pointers, which is three more three pointer than I’ve hit in college basketball. So props to him. Does that mean he had the best true shooting season in college basketball in the 2023-24 season?\nNot hardly.\nWe’ll talk about how to narrow the pile and filter out data in the next chapter.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mutating data</span>"
    ]
  },
  {
    "objectID": "mutating.html#a-more-complex-example",
    "href": "mutating.html#a-more-complex-example",
    "title": "5  Mutating data",
    "section": "",
    "text": "For this walkthrough:\n   Download csv file",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mutating data</span>"
    ]
  },
  {
    "objectID": "filtering.html",
    "href": "filtering.html",
    "title": "6  Filters and selections",
    "section": "",
    "text": "6.1 Selecting data to make it easier to read\nMore often than not, we have more data than we want. Sometimes we need to be rid of that data. In dplyr, there’s two ways to go about this: filtering and selecting.\nFiltering creates a subset of the data based on criteria. All records where the count is greater than 10. All records that match “Maryland”. Something like that.\nSelecting simply returns only the fields named. So if you only want to see School and Attendance, you select those fields. When you look at your data again, you’ll have two columns. If you try to use one of your columns that you had before you used select, you’ll get an error.\nLet’s work with our football attendance data to show some examples.\nFirst we’ll need the tidyverse.\nNow import the data.\nSo, first things first, let’s say we don’t care about all this Air Force, Akron, Alabama crap and just want to see Dear Old Maryland We do that with filter and then we pass it a condition.\nBefore we do that, a note about conditions. Most of the conditional operators you’ll understand – greater than and less than are &gt; and &lt;. The tough one to remember is equal to. In conditional statements, equal to is == not =. If you haven’t noticed, = is a variable assignment operator, not a conditional statement. So equal is == and NOT equal is !=.\nSo if you want to see Institutions equal to Maryland, you do this:\nOr if we want to see schools that had more than half a million people buy tickets to a football game last season, we do the following. NOTE THE BACKTICKS.\nBut what if we want to see all of the Power Five conferences? We could use conditional logic in our filter. The conditional logic operators are | for OR and & for AND. NOTE: AND means all conditions have to be met. OR means any of the conditions work. So be careful about boolean logic.\nBut that’s a lot of repetitive code. And a lot of typing. And typing is the devil. So what if we could create a list and pass it into the filter? It’s pretty simple.\nWe can create a new variable – remember variables can represent just about anything – and create a list. To do that we use the c operator, which stands for concatenate. That just means take all the stuff in the parenthesis after the c and bunch it into a list.\nNote here: text is in quotes. If they were numbers, we wouldn’t need the quotes.\nNow with a list, we can use the %in% operator. It does what you think it does – it gives you data that matches things IN the list you give it.\nSo now we have our Power Five list. What if we just wanted to see attendance from the most recent season and ignore all the rest? Select to the rescue.\nattendance |&gt; filter(Conference %in% powerfive) |&gt; select(Institution, Conference, `2024`)\n\n# A tibble: 69 × 3\n   Institution    Conference `2024`\n   &lt;chr&gt;          &lt;chr&gt;       &lt;dbl&gt;\n 1 Alabama        SEC        700539\n 2 Arizona        Big 12     327230\n 3 Arizona St.    Big 12     293901\n 4 Arkansas       SEC        480478\n 5 Auburn         SEC        704344\n 6 Baylor         Big 12     253810\n 7 Boston College ACC        279236\n 8 BYU            Big 12     377091\n 9 California     ACC        274214\n10 Cincinnati     Big 12     219688\n# ℹ 59 more rows\nIf you have truly massive data, Select has tools to help you select fields that start_with the same things or ends with a certain word. The documentation will guide you if you need those someday. For 90 plus percent of what we do, just naming the fields will be sufficient.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Filters and selections</span>"
    ]
  },
  {
    "objectID": "filtering.html#using-conditional-filters-to-set-limits",
    "href": "filtering.html#using-conditional-filters-to-set-limits",
    "title": "6  Filters and selections",
    "section": "6.2 Using conditional filters to set limits",
    "text": "6.2 Using conditional filters to set limits\nLet’s return to the problem of one-hit wonders in basketball mucking up our true shooting analysis. How can we set limits in something like a question of who had the best season? Let’s grab every player from last season.\nFor this walkthrough:\n   Download csv file\n\nLet’s get set up similar to the previous chapter.\n\nplayers &lt;- read_csv(\"data/players25.csv\")\n\nRows: 5818 Columns: 57\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (10): Team, Player, Class, Height, Hometown, High School, Summary, Pos, ...\ndbl (47): #, Weight, G, GS, MP, FG, FGA, FG%, 3P, 3PA, 3P%, 2P, 2PA, 2P%, eF...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nplayers |&gt;\n  mutate(trueshooting = (PTS/(2*(FGA + (.44*FTA))))*100) |&gt;\n  arrange(desc(trueshooting))\n\n# A tibble: 5,818 × 58\n   Team    Player   `#` Class Height Weight Hometown `High School` Summary     G\n   &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;         &lt;chr&gt;   &lt;dbl&gt;\n 1 Wiscon… Isaac…    15 JR    6-2       165 Oregon,… Oregon (WI)   0.2 Pt…    13\n 2 TCU Ho… Cole …    35 SR    6-3       180 San Jos… Bellarmine C… 3.0 Pt…     1\n 3 TCU Ho… Drew …    30 FR    6-2       180 Dallas,… Highland Par… 1.5 Pt…     2\n 4 Stanfo… Derin…     1 SO    6-4       190 Istanbu… The Ashevill… 0.6 Pt…     5\n 5 St. Bo… Jack …    15 SO    6-0       172 Olean, … Olean (NY)    3.0 Pt…     1\n 6 Seattl… Eric …    19 FR    5-10      167 Beijing… Ruamrudee In… 3.0 Pt…     1\n 7 Samfor… Corey…    10 FR    6-4       175 Fairbur… Landmark Chr… 1.5 Pt…     2\n 8 Queens… Aneek…    85 SO    5-10      145 Foster … San Mateo (C… 3.0 Pt…     1\n 9 Old Do… CJ Pa…    15 FR    6-6       182 Norfolk… Maury (VA)    1.0 Pt…     3\n10 Oklaho… Jake …    30 SR    6-3       187 Norman,… Loyola Acade… 0.4 Pt…     7\n# ℹ 5,808 more rows\n# ℹ 48 more variables: GS &lt;dbl&gt;, MP &lt;dbl&gt;, FG &lt;dbl&gt;, FGA &lt;dbl&gt;, `FG%` &lt;dbl&gt;,\n#   `3P` &lt;dbl&gt;, `3PA` &lt;dbl&gt;, `3P%` &lt;dbl&gt;, `2P` &lt;dbl&gt;, `2PA` &lt;dbl&gt;, `2P%` &lt;dbl&gt;,\n#   `eFG%` &lt;dbl&gt;, FT &lt;dbl&gt;, FTA &lt;dbl&gt;, `FT%` &lt;dbl&gt;, ORB &lt;dbl&gt;, DRB &lt;dbl&gt;,\n#   TRB &lt;dbl&gt;, AST &lt;dbl&gt;, STL &lt;dbl&gt;, BLK &lt;dbl&gt;, TOV &lt;dbl&gt;, PF &lt;dbl&gt;, PTS &lt;dbl&gt;,\n#   Pos &lt;chr&gt;, PER &lt;dbl&gt;, `TS%` &lt;dbl&gt;, `3PAr` &lt;dbl&gt;, FTr &lt;dbl&gt;, PProd &lt;dbl&gt;,\n#   `ORB%` &lt;dbl&gt;, `DRB%` &lt;dbl&gt;, `TRB%` &lt;dbl&gt;, `AST%` &lt;dbl&gt;, `STL%` &lt;dbl&gt;, …\n\n\nIn that season, we’ve got several players that can lay claim to the title of One Shot One Three True Shooting champion.\nIn most contests, like the batting title in Major League Baseball, there’s a minimum number of X to qualify. In baseball, it’s at bats. In basketball, it attempts. So let’s set a floor and see how it changes. What if we said you had to have played 100 minutes in a season? The top players in college basketball play more than 1000 minutes in a season. So 100 is not that much. Let’s try it and see.\n\nplayers |&gt;\n  mutate(trueshooting = (PTS/(2*(FGA + (.44*FTA))))*100) |&gt;\n  arrange(desc(trueshooting)) |&gt;\n  filter(MP &gt; 100)\n\n# A tibble: 3,682 × 58\n   Team    Player   `#` Class Height Weight Hometown `High School` Summary     G\n   &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;         &lt;chr&gt;   &lt;dbl&gt;\n 1 Arizon… Conra…    55 SO    6-0       165 Granoll… Club Joventu… 1.6 Pt…    22\n 2 Syracu… Nahee…    10 SR    7-4       248 Philade… Plymouth Whi… 2.4 Pt…    18\n 3 Butler… Augus…     0 SO    6-8       220 Salvado… NBA Academy … 5.1 Pt…    10\n 4 UConn … Samso…    35 SR    6-10      205 Lomé, T… The Patrick … 7.5 Pt…    34\n 5 Dayton… Isaac…    13 JR    6-11      251 Port Al… Alberni Dist… 3.2 Pt…    21\n 6 Utah S… Isaac…    23 FR    6-8       240 Idaho F… Hillcrest (I… 2.1 Pt…    19\n 7 Northw… Lado …     0 SR    6-10      215 Dallas,… Dallas Lakel… 2.4 Pt…    18\n 8 Purdue… Will …    44 SO    7-2       260 Stockho… Riksbasketgy… 1.8 Pt…    22\n 9 Baylor… Josh …    17 JR    6-10      230 Asaba, … NBA Academy … 7.4 Pt…    23\n10 Milwau… Dariu…    34 SR    6-8       225 Murray,… Murray (KY)   3.0 Pt…    32\n# ℹ 3,672 more rows\n# ℹ 48 more variables: GS &lt;dbl&gt;, MP &lt;dbl&gt;, FG &lt;dbl&gt;, FGA &lt;dbl&gt;, `FG%` &lt;dbl&gt;,\n#   `3P` &lt;dbl&gt;, `3PA` &lt;dbl&gt;, `3P%` &lt;dbl&gt;, `2P` &lt;dbl&gt;, `2PA` &lt;dbl&gt;, `2P%` &lt;dbl&gt;,\n#   `eFG%` &lt;dbl&gt;, FT &lt;dbl&gt;, FTA &lt;dbl&gt;, `FT%` &lt;dbl&gt;, ORB &lt;dbl&gt;, DRB &lt;dbl&gt;,\n#   TRB &lt;dbl&gt;, AST &lt;dbl&gt;, STL &lt;dbl&gt;, BLK &lt;dbl&gt;, TOV &lt;dbl&gt;, PF &lt;dbl&gt;, PTS &lt;dbl&gt;,\n#   Pos &lt;chr&gt;, PER &lt;dbl&gt;, `TS%` &lt;dbl&gt;, `3PAr` &lt;dbl&gt;, FTr &lt;dbl&gt;, PProd &lt;dbl&gt;,\n#   `ORB%` &lt;dbl&gt;, `DRB%` &lt;dbl&gt;, `TRB%` &lt;dbl&gt;, `AST%` &lt;dbl&gt;, `STL%` &lt;dbl&gt;, …\n\n\nNow you get Radford’s TJ NeSmith, who played in seven games and was on the floor for 113 minutes. So he played a little bit, but not a lot. But in that time, he only attempted 32 shots, and made 75 percent of them. In other words, when he shot, he probably scored. He just rarely shot.\nSo is 100 minutes our level? Here’s the truth – there’s not really an answer here. We’re picking a cutoff. If you can cite a reason for it and defend it, then it probably works.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Filters and selections</span>"
    ]
  },
  {
    "objectID": "filtering.html#top-list",
    "href": "filtering.html#top-list",
    "title": "6  Filters and selections",
    "section": "6.3 Top list",
    "text": "6.3 Top list\nOne last little dplyr trick that’s nice to have in the toolbox is a shortcut for selecting only the top values for your dataset. Want to make a Top 10 List? Or Top 25? Or Top Whatever You Want? It’s easy.\nSo what are the top 10 Power Five schools by season attendance. All we’re doing here is chaining commands together with what we’ve already got. We’re filtering by our list of Power Five conferences, we’re selecting the three fields we need, now we’re going to arrange it by total attendance and then we’ll introduce the new function: top_n. The top_n function just takes a number. So we want a top 10 list? We do it like this:\n\nattendance |&gt; filter(Conference %in% powerfive) |&gt; select(Institution, Conference, `2024`) |&gt; arrange(desc(`2024`)) |&gt; top_n(10)\n\nSelecting by 2024\n\n\n# A tibble: 10 × 3\n   Institution Conference `2024`\n   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;\n 1 Ohio St.    Big Ten    936550\n 2 Michigan    Big Ten    884382\n 3 Penn St.    Big Ten    864665\n 4 Texas       SEC        817852\n 5 Texas A&M   SEC        719927\n 6 Tennessee   SEC        713405\n 7 LSU         SEC        708645\n 8 Auburn      SEC        704344\n 9 Alabama     SEC        700539\n10 Florida     SEC        630116\n\n\nThat’s all there is to it. Just remember – for it to work correctly, you need to arrange your data BEFORE you run top_n. Otherwise, you’re just getting the first 10 values in the list. The function doesn’t know what field you want the top values of. You have to do it.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Filters and selections</span>"
    ]
  },
  {
    "objectID": "transforming.html",
    "href": "transforming.html",
    "title": "7  Transforming data",
    "section": "",
    "text": "7.1 Making long data wide\nSometimes long data needs to be wide, and sometimes wide data needs to be long. I’ll explain.\nYou are soon going to discover that long before you can visualize data, you need to have it in a form that the visualization library can deal with. One of the ways that isn’t immediately obvious is how your data is cast. Most of the data you will encounter will be wide – each row will represent a single entity with multiple measures for that entity. So think of states. Your row of your dataset could have the state name, population, average life expectancy and other demographic data.\nBut what if your visualization library needs one row for each measure? So state, data type and the data. Maryland, Population, 6,177,224. That’s one row. Then the next row is Maryland, Average Life Expectancy, 78.5 That’s the next row. That’s where recasting your data comes in.\nWe can use a library called tidyr to pivot_longer or pivot_wider the data, depending on what we need. We’ll use a dataset of college football attendance to demonstrate.\nFirst we need some libraries.\nNow we’ll load the data.\nSo as you can see, each row represents a school, and then each column represents a year. This is great for calculating the percent change – we can subtract a column from a column and divide by that column. But later, when we want to chart each school’s attendance over the years, we have to have each row be one team for one year. Maryland in 2013, then Maryland in 2014, and Maryland in 2015 and so on.\nTo do that, we use pivot_longer because we’re making wide data long. Since all of the columns we want to make rows start with 20, we can use that in our cols directive. Then we give that column a name – Year – and the values for each year need a name too. Those are the attendance figure. We can see right away how this works.\nWe’ve gone from 147 rows to more than 1,700, but that’s expected when we have 10+ years for each team.\nWe can reverse this process using pivot_wider, which makes long data wide.\nWhy do any of this?\nIn some cases, you’re going to be given long data and you need to calculate some metric using two of the years – a percent change for instance. So you’ll need to make the data wide to do that. You might then have to re-lengthen the data now with the percent change. Some project require you to do all kinds of flexing like this. It just depends on the data.\nSo let’s take what we made above and turn it back into wide data.\nlongdata &lt;- attendance |&gt; pivot_longer(cols = starts_with(\"20\"), names_to = \"Year\", values_to = \"Attendance\")\n\nlongdata\n\n# A tibble: 1,752 × 4\n   Institution Conference Year  Attendance\n   &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt;\n 1 Air Force   MWC        2013      228562\n 2 Air Force   MWC        2014      168967\n 3 Air Force   MWC        2015      156158\n 4 Air Force   MWC        2016      177519\n 5 Air Force   MWC        2017      174924\n 6 Air Force   MWC        2018      166205\n 7 Air Force   MWC        2019      162505\n 8 Air Force   MWC        2020        5600\n 9 Air Force   MWC        2021      136984\n10 Air Force   MWC        2022      188482\n# ℹ 1,742 more rows\nTo pivot_wider, we just need to say where our column names are coming from – the Year – and where the data under it should come from – Attendance.\nlongdata |&gt; pivot_wider(names_from = Year, values_from = Attendance)\n\nWarning: Values from `Attendance` are not uniquely identified; output will contain\nlist-cols.\n• Use `values_fn = list` to suppress this warning.\n• Use `values_fn = {summary_fun}` to summarise duplicates.\n• Use the following dplyr code to identify duplicates.\n  {data} |&gt;\n  dplyr::summarise(n = dplyr::n(), .by = c(Institution, Conference, Year)) |&gt;\n  dplyr::filter(n &gt; 1L)\n\n\n# A tibble: 144 × 14\n   Institution     Conference   `2013` `2014` `2015` `2016` `2017` `2018` `2019`\n   &lt;chr&gt;           &lt;chr&gt;        &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt;\n 1 Air Force       MWC          &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; \n 2 Akron           MAC          &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; \n 3 Alabama         SEC          &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; \n 4 Appalachian St. FBS Indepen… &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; \n 5 Appalachian St. Sun Belt     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; \n 6 Arizona         Big 12       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; \n 7 Arizona St.     Big 12       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; \n 8 Arkansas        SEC          &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; \n 9 Arkansas St.    Sun Belt     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; \n10 Army West Point The American &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; \n# ℹ 134 more rows\n# ℹ 5 more variables: `2020` &lt;list&gt;, `2021` &lt;list&gt;, `2022` &lt;list&gt;,\n#   `2023` &lt;list&gt;, `2024` &lt;list&gt;\nAnd just like that, we’re back.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Transforming data</span>"
    ]
  },
  {
    "objectID": "transforming.html#why-this-matters",
    "href": "transforming.html#why-this-matters",
    "title": "7  Transforming data",
    "section": "7.2 Why this matters",
    "text": "7.2 Why this matters\nThis matters because certain visualization types need wide or long data. A significant hurdle you will face for the rest of the semester is getting the data in the right format for what you want to do.\nSo let me walk you through an example using this data.\nLet’s look at Maryland’s attendance over the time period. In order to do that, I need long data because that’s what the charting library, ggplot2, needs. You’re going to learn a lot more about ggplot later. Since this data is organized by school and conference, I also need to remove records that have no attendance data (because we have a Maryland in the ACC row).\n\nmaryland &lt;- longdata |&gt; filter(Institution == \"Maryland\") |&gt; filter(!is.na(Attendance))\n\nNow that we have long data for just Maryland, we can chart it.\n\nggplot(maryland, aes(x=Year, y=Attendance, group=1)) + \n  geom_line() + \n  scale_y_continuous(labels = scales::comma) + \n  labs(x=\"Year\", y=\"Attendance\", title=\"Attendance Up and Down Under Locksley\", subtitle=\"Heading back to the 300,000 mark?\", caption=\"Source: NCAA | By Derek Willis\", color = \"Outcome\") +\n  theme_minimal() + \n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 10),\n    axis.title.y = element_blank(),\n    axis.text = element_text(size = 7),\n    axis.ticks = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    legend.position=\"bottom\"\n  )",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Transforming data</span>"
    ]
  },
  {
    "objectID": "significancetests.html",
    "href": "significancetests.html",
    "title": "8  Significance tests",
    "section": "",
    "text": "8.1 Accepting the null hypothesis\nNow that we’ve worked with data a little, it’s time to start asking more probing questions of our data. One of the most probing questions we can ask – one that so few sports journalists ask – is if the difference between this thing and the normal thing is real.\nWe have a perfect natural experiment going on in sports right now to show how significance tests work. The NBA, to salvage a season and get to the playoffs, put their players in a bubble – more accurately a hotel complex at Disney World in Orlando – and had them play games without fans.\nSo are the games different from other regular season games that had fans?\nTo answer this, we need to understand that a significance test is a way to determine if two numbers are significantly different from each other. Generally speaking, we’re asking if a subset of data – a sample – is different from the total data pool – the population. Typically, this relies on data being in a normal distribution.\nIf it is, then we know certain things about it. Like the mean – the average – will be a line right at the peak of cases. And that 66 percent of cases will be in that red area – the first standard deviation.\nA significance test will determine if a sample taken from that group is different from the total.\nSignificance testing involves stating a hypothesis. In our case, our hypothesis is that there is a difference between bubble games without people and regular games with people.\nIn statistics, the null hypothesis is the opposite of your hypothesis. In this case, that there is no difference between fans and no fans.\nWhat we’re driving toward is a metric called a p-value, which is the probability that you’d get your sample mean if the null hypothesis is true. So in our case, it’s the probability we’d see the numbers we get if there was no difference between fans and no fans. If that probability is below .05, then we consider the difference significant and we reject the null hypothesis.\nSo let’s see. We’ll need a log of every game last NBA season. In this data, there’s a field called COVID, which labels the game as a regular game or a bubble game.\nLoad the tidyverse.\nAnd import the data.\nFirst, let’s just look at scoring. Here’s a theory: fans make players nervous. The screaming makes players tense up, and tension makes for bad shooting. An alternative to this: screaming fans make you defend harder. So my hypothesis is that not only is the scoring different, it’s lower.\nFirst things first, let’s create a new field, called totalpoints and add the two scores together. We’ll need this, so we’re going to make this a new dataframe called points.\nTypically speaking, with significance tests, the process involves creating two different means and then running a bunch of formulas on them. R makes this easy by giving you a t.test function, which does all the work for you. What we have to tell it is what is the value we are testing, over which groups, and from what data. It looks like this:\nNow let’s talk about the output. I prefer to read these bottom up. So at the bottom, it says that the mean number of points score in an NBA game With Fans is 222.89. The mean scored in games Without Fans is 231.35. That means teams are scoring almost 8.5 points MORE without fans on average.\nBut, some games are defenseless track meets, some games are defensive slugfests. We learned that averages can be skewed by extremes. So the next thing we need to look at is the p-value. Remember, this is the probability that we’d get this sample mean – the without fans mean – if there was no difference between fans and no fans.\nThe probability? 4.099e-07 or 4.099 x 10 to the -7 power. Don’t remember your scientific notation? That’s .00000004099. The decimal, seven zeros and the number.\nRemember, if the probability is below .05, then we determine that this number is statistically significant. We’ll talk more about statistical significance soon, but in this case, statistical significance means that our hypothesis is correct: points are different without fans than with. And since our hypothesis is correct, we reject the null hypothesis and we can confidently say that bubble teams are scoring more than they were when fans packed arenas.\nSo what does it look like when your hypothesis is wrong?\nLet’s test another thing that may have been impacted by bubble games: home court advantage. If you’re the home team, but you’re not at home, does it affect you? It has to, right? Your fans aren’t there. Home and away are just positions on the scoreboard. It can’t matter, can it?\nMy hypothesis is that home court is no longer an advantage, and the home team will score less relative to the away team.\nFirst things first: We need to make a dataframe where Team is the home team. And then we’ll create a differential between the home team and away team. If home court is an advantage, the differential should average out to be positive – the home team scores more than the away team.\nhomecourt &lt;- logs |&gt; filter(is.na(HomeAway) == TRUE) |&gt; mutate(differential = TeamScore - OpponentScore)\nNow let’s test it.\nt.test(differential ~ COVID, data=homecourt)\n\n\n    Welch Two Sample t-test\n\ndata:  differential by COVID\nt = 0.36892, df = 107.84, p-value = 0.7129\nalternative hypothesis: true difference in means between group With Fans and group Without Fans is not equal to 0\n95 percent confidence interval:\n -2.301628  3.354268\nsample estimates:\n   mean in group With Fans mean in group Without Fans \n                  2.174047                   1.647727\nSo again, start at the bottom. With Fans, the home team averages 2.17 more points than the away team. Without fans, they average 1.64 more.\nIf you are a bad sportswriter or a hack sports talk radio host, you look at this and scream “the bubble killed home court!”\nBut two things: first, the home team is STILL, on average, scoring more than the away team on the whole.\nAnd two: Look at the p-value. It’s .7129. Is that less than .05? No, no it is not. So that means we have to accept the null hypothesis that there is no difference between fans and no fans when it comes to the difference between the home team and the away team’s score.\nNow, does this mean that the bubble hasn’t impacted the magic of home court? Not necessarily. What it’s saying is that the variance between one and the other is too large to be able to say that they’re different. It could just be random noise that’s causing the difference, and so it’s not real. More to the point, it’s saying that this metric isn’t capable of telling you that there’s no home court in the bubble.\nWe’re going to be analyzing these bubble games for years trying to find the true impact of fans.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Significance tests</span>"
    ]
  },
  {
    "objectID": "correlations.html",
    "href": "correlations.html",
    "title": "9  Correlations and regression",
    "section": "",
    "text": "9.1 A more predictive example\nThroughout sports, you will find no shortage of opinions. From people yelling at their TV screens to an entire industry of people paid to have opinions, there are no shortage of reasons why this team sucks and that player is great. They may have their reasons, but a better question is, does that reason really matter?\nCan we put some numbers behind that? Can we prove it or not?\nThis is what we’re going to start to answer. And we’ll do it with correlations and regressions.\nFirst, we need data from the 2024 women’s college soccer season.\nThen load the tidyverse.\nNow import the data.\nTo do this, we need all college soccer teams and their season stats from last year. How much, over the course of a season, does a thing matter? That’s the question you’re going to answer.\nIn our case, we want to know how much does a team’s fouls influence the number of goals they score in a season? How much difference can we explain in goals with fouls?\nWe’re going to use two different methods here and they’re closely related. Correlations – specifically the Pearson Correlation Coefficient – is a measure of how related two numbers are in a linear fashion. In other words – if our X value goes up one, what happens to Y? If it also goes up 1, that’s a perfect correlation. X goes up 1, Y goes up 1. Every time. Correlation coefficients are a number between 0 and 1, with zero being no correlation and 1 being perfect correlation if our data is linear. We’ll soon go over scatterplots to visually determine if our data is linear, but for now, we have a hypothesis: More fouls are bad. Fouls hurt. So if a team gets lots of them, they should have worse outcomes than teams that get few of them. That is an argument for a linear relationship between them.\nBut is there one?\nWe’re going create a new dataframe called newcorrelations that takes our data that we imported and adds a column called differential which is the difference between goals and defensive_goals, and then we’ll use correlations to see how related those two things are.\nIn R, there is a cor function, and it works much the same as mean or median. So we want to see if differential is correlated with fouls, which is the yards of penalties a team gets in a game. We do that by referencing differential and fouls and specifying we want a pearson correlation. The number we get back is the correlation coefficient.\nSo on a scale of -1 to 1, where 0 means there’s no relationship at all and 1 or -1 means a perfect relationship, fouls and whether or not the team scores more goals than it gives up are at 0.01806104 You could say they’re 1.8 percent related toward the positive – more fouls, the higher your differential. Another way to say it? They’re almost 98 percent not related.\nWhat about the number of yellow cards instead of fouls? Do more aggressive defensive teams also score more?\nSo wait, what does this all mean?\nIt means that when you look at every game in college soccer, the number of goals and yellow cards do have a small impact on the score difference between your team and the other team. But the relationship is barely anything at all. For yellow cards, it’s a negative correlation of about 7 percent. So like 93+ percent plus not related. So neither fouls nor yellow cards have much of any relationship with the difference in goal-scoring.\nNormally, at this point, you’d quit while you were ahead. A correlation coefficient that shows there’s no relationship between two things means stop. It’s pointless to go on. But let’s put this fully to rest.\nEnter regression. Regression is how we try to fit our data into a line that explains the relationship the best. Regressions will help us predict things as well – if we have a team that has so many fouls, what kind of point differential could we expect? So regressions are about prediction, correlations are about description. Correlations describe a relationship. Regressions help us predict what that relationship means and what it might look like in the real world. Specifically, it tells us how much of the change in a dependent variable can be explained by the independent variable.\nAnother thing regressions do is give us some other tools to evaluate if the relationship is real or not.\nHere’s an example of using linear modeling to look at fouls. Think of the ~ character as saying “is predicted by”. The output looks like a lot, but what we need is a small part of it.\nThere’s three things we need here:\nSo fouls are totally meaningless to the outcome of a game.\nYou can see the problem in a graph. On the X axis is fouls, on the y is point differential. If these elements had a strong relationship, we’d see a clear pattern moving from right to left, sloping down. On the left would be the teams with few fouls and a positive point differential. On right would be teams with high fouls and negative point differentials. Do you see that below?\nSo we’ve firmly established that fouls aren’t predictive. But what is?\nSo instead of looking at fouls, let’s make a new metric: shots on goal. Can we predict the score differential by looking at the shots they put on goal?\nFirst, let’s look at the correlation coefficent.\nnewcorrelations |&gt; \n  summarise(correlation = cor(differential, so_g, method=\"pearson\"))\n\n# A tibble: 1 × 1\n  correlation\n        &lt;dbl&gt;\n1       0.602\nAnswer: 60 percent. Not a perfect relationship, but pretty good. But how meaningful is that relationship and how predictive is it?\nnet &lt;- lm(differential ~ so_g, data = newcorrelations)\nsummary(net)\n\n\nCall:\nlm(formula = differential ~ so_g, data = newcorrelations)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.2804 -1.0389  0.0988  1.2365  5.8234 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.13264    0.04233  -50.38   &lt;2e-16 ***\nso_g         0.37923    0.00626   60.58   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.808 on 6449 degrees of freedom\nMultiple R-squared:  0.3627,    Adjusted R-squared:  0.3626 \nF-statistic:  3670 on 1 and 6449 DF,  p-value: &lt; 2.2e-16\nFirst we check p-value: 0.00000000000000022. That’s sixteen zeros between the decimal and 22. Is that less than .05? Uh, yeah. So this is really, really, really not random. But anyone who has watched a game of soccer knows this is true. It makes intuitive sense.\nSecond, Adjusted R-squared: 0.3626. So we can predict about 36 percent of the difference in the score differential by simply looking at the shots on goal the team has.\nThird, the coefficients: In this case, our y=mx+b formula looks like y = 0.37923x + -2.13264. So if we were applying this, let’s look at Maryland’s 3-2 loss to George Mason on Sept. 1, 2024. Maryland’s shots on goal in that game? 9. What does our model say the point differential should have been?\n(0.37923*9)+-2.13264 \n\n[1] 1.28043\nSo by our model, Maryland should have won by at least one goal. Some games are closer than others. But when you can explain 36 percent of the difference, this is the kind of result you get. What would improve the model? Using more data to start. And using more inputs.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlations and regression</span>"
    ]
  },
  {
    "objectID": "multipleregression.html",
    "href": "multipleregression.html",
    "title": "10  Multiple regression",
    "section": "",
    "text": "Last chapter, we looked at correlations and linear regression to predict how one element of a game would predict the score. But we know that a single variable, in all but the rarest instances, is not going to be that predictive. We need more than one. Enter multiple regression. Multiple regression lets us add – wait for it – multiple predictors to our equation to help us get a better fit to reality.\nThat presents it’s own problems. So let’s get set up. The dataset we’ll use is all men’s college basketball games between 2011 and 2025.\nFor this walkthrough:\n   Download csv file\n\nWe need the tidyverse.\n\nlibrary(tidyverse)\n\nAnd the data.\n\nlogs &lt;- read_csv(\"data/cbblogs1125.csv.zip\")\n\nMultiple files in zip: reading 'cbblogs1125.csv'\nRows: 169224 Columns: 61\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (10): Season, GameType, TeamFullName, Opponent, HomeAway, W_L, OT, URL,...\ndbl  (50): file_source, Game, TeamScore, OpponentScore, TeamFG, TeamFGA, Tea...\ndate  (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nOne way to show how successful a basketball team was for a game is to show the differential between the team’s score and the opponent’s score. Score a lot more than the opponent = good, score a lot less than the opponent = bad. And, relatively speaking, the more the better. So let’s create that differential. Let’s add in net rebounds. And because we’ll need it later, let’s add the turnover margin.\n\nlogs &lt;- logs |&gt; mutate(\n  Differential = TeamScore - OpponentScore, \n  NetRebounds = TeamTotalRebounds - OpponentTotalRebounds,\n  TurnoverMargin = TeamTurnovers - OpponentTurnovers)\n\nThe linear model code we used before is pretty straight forward. Its field is predicted by field. Here’s a simple linear model that looks at predicting a team’s point differential by looking at their net turnovers.\n\nrebounds &lt;- lm(Differential ~ NetRebounds, data=logs)\nsummary(rebounds)\n\n\nCall:\nlm(formula = Differential ~ NetRebounds, data = logs)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-61.642  -8.532  -0.207   8.248  94.358 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.697139   0.031515   22.12   &lt;2e-16 ***\nNetRebounds 1.054990   0.003255  324.14   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.94 on 169218 degrees of freedom\n  (4 observations deleted due to missingness)\nMultiple R-squared:  0.3831,    Adjusted R-squared:  0.3831 \nF-statistic: 1.051e+05 on 1 and 169218 DF,  p-value: &lt; 2.2e-16\n\n\nRemember: There’s a lot here, but only some of it we care about. What is the Adjusted R-squared value? What’s the p-value and is it less than .05? In this case, we can predict about 38 percent of the difference in differential with the net rebounds in the game.\nTo add more predictors to this mix, we merely add them. But it’s not that simple, as you’ll see in a moment. So first, let’s look at adding turnover margin to our prediction model:\n\nmodel1 &lt;- lm(Differential ~ NetRebounds + TurnoverMargin, data=logs)\nsummary(model1)\n\n\nCall:\nlm(formula = Differential ~ NetRebounds + TurnoverMargin, data = logs)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-46.307  -6.873  -0.064   6.830  49.523 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     0.338141   0.025092   13.48   &lt;2e-16 ***\nNetRebounds     1.184100   0.002621  451.72   &lt;2e-16 ***\nTurnoverMargin -1.546417   0.004933 -313.48   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.29 on 169217 degrees of freedom\n  (4 observations deleted due to missingness)\nMultiple R-squared:  0.6097,    Adjusted R-squared:  0.6097 \nF-statistic: 1.322e+05 on 2 and 169217 DF,  p-value: &lt; 2.2e-16\n\n\nFirst things first: What is the adjusted R-squared? This model predicts about 61 percent of the differential.\nSecond: what is the p-value and is it less than .05?\nThird: Compare the residual standard error. We went from 12.94 to 10.3. The meaning of this is both really opaque and also simple – by adding data, we reduced the amount of error in our model. Residual standard error is the total distance between what our model would predict and what we actually have in the data. So lots of residual error means the distance between reality and our model is wider. So the width of our predictive range in this example shrank while we improved the amount of the difference we could predict. That’s good, and not always going to be the case.\nOne of the more difficult things to understand about multiple regression is the issue of multicollinearity. What that means is that there is significant correlation overlap between two variables – the two are related to each other as well as to the target output – and all you are doing by adding both of them is adding error with no real value to the R-squared. In pure statistics, we don’t want any multicollinearity at all. Violating that assumption limits the applicability of what you are doing. So if we have some multicollinearity, it limits our scope of application to college basketball. We can’t say this will work for every basketball league and level everywhere. What we need to do is see how correlated each value is to each other and throw out ones that are highly co-correlated.\nSo to find those, we have to create a correlation matrix that shows us how each value is correlated to our outcome variable, but also with each other. We can do that in the Hmisc library. We install that in the console with install.packages(\"Hmisc\")\n\nlibrary(Hmisc)\n\nWe can pass in every numeric value to the Hmisc library and get a correlation matrix out of it, but since we have a large number of values – and many of them character values – we should strip that down and reorder them. So that’s what I’m doing here. I’m saying give me all the columns with numeric values, except for Game, and then show me the differential, net yards, turnover margin and then everything else.\n\nsimplelogs &lt;- logs |&gt; select_if(is.numeric) |&gt; select(-Game) |&gt; select(Differential, NetRebounds, TurnoverMargin, TeamFGPCT, TeamTotalRebounds, OpponentFGPCT, OpponentTotalRebounds)\n\nBefore we proceed, what we’re looking to do is follow the Differential column down, looking for correlation values near 1 or -1. Correlations go from -1, meaning perfect negative correlation, to 0, meaning no correlation, to 1, meaning perfect positive correlation. So we’re looking for numbers near 1 or -1 for their predictive value. BUT: We then need to see if that value is also highly correlated with something else. If it is, we have a decision to make.\nWe get our correlation matrix like this:\n\ncormatrix &lt;- rcorr(as.matrix(simplelogs))\n\ncormatrix$r\n\n                      Differential NetRebounds TurnoverMargin   TeamFGPCT\nDifferential             1.0000000   0.6189143    -0.37292019  0.60659177\nNetRebounds              0.6189143   1.0000000     0.15712085  0.36075626\nTurnoverMargin          -0.3729202   0.1571208     1.00000000 -0.02617653\nTeamFGPCT                0.6065918   0.3607563    -0.02617653  1.00000000\nTeamTotalRebounds        0.4796608   0.7474168     0.09997749  0.02222265\nOpponentFGPCT           -0.6124710  -0.3688248     0.02892920 -0.11829996\nOpponentTotalRebounds   -0.4265115  -0.7180688    -0.13111165 -0.51822144\n                      TeamTotalRebounds OpponentFGPCT OpponentTotalRebounds\nDifferential                 0.47966080  -0.612471013          -0.426511535\nNetRebounds                  0.74741681  -0.368824848          -0.718068849\nTurnoverMargin               0.09997749   0.028929201          -0.131111653\nTeamFGPCT                    0.02222265  -0.118299958          -0.518221439\nTeamTotalRebounds            1.00000000  -0.534652332          -0.074323920\nOpponentFGPCT               -0.53465233   1.000000000          -0.006469967\nOpponentTotalRebounds       -0.07432392  -0.006469967           1.000000000\n\n\nNotice right away – NetRebounds is highly correlated. But NetRebounds is also highly correlated with TeamTotalRebounds. And that makes sense: TeamTotalRebounds feeds into NetRebounds. Including both of these measures would be pointless – they would add error without adding much in the way of predictive power.\n\nYour turn: What else do you see? What other values have predictive power and aren’t co-correlated? Add or remove some of the columns above and re-run the correlation matrix.\n\nWe can add more just by simply adding them. Let’s add the average FG PCT for both the team and opponent. They’re correlated to Differential, but not as much as you might expect.\n\nmodel2 &lt;- lm(Differential ~ NetRebounds + TurnoverMargin + TeamFGPCT + OpponentFGPCT, data=logs)\nsummary(model2)\n\n\nCall:\nlm(formula = Differential ~ NetRebounds + TurnoverMargin + TeamFGPCT + \n    OpponentFGPCT, data = logs)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.955  -3.650  -0.026   3.626  37.661 \n\nCoefficients:\n                 Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept)      0.168687   0.117743    1.433    0.152    \nNetRebounds      0.655831   0.001604  408.886   &lt;2e-16 ***\nTurnoverMargin  -1.317830   0.002637 -499.805   &lt;2e-16 ***\nTeamFGPCT       89.952654   0.190823  471.392   &lt;2e-16 ***\nOpponentFGPCT  -90.064048   0.190918 -471.742   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.454 on 169215 degrees of freedom\n  (4 observations deleted due to missingness)\nMultiple R-squared:  0.8904,    Adjusted R-squared:  0.8904 \nF-statistic: 3.438e+05 on 4 and 169215 DF,  p-value: &lt; 2.2e-16\n\n\nGo down the list:\nWhat is the Adjusted R-squared now? What is the p-value and is it less than .05? What is the Residual standard error?\nThe final thing we can do with this is predict things. Look at our coefficients table. See the Estimates? We can build a formula from that, same as we did with linear regressions.\nHow does this apply in the real world? Let’s pretend for a minute that you are Buzz Williams, and you want to win conference titles. To do that, we need to know what attributes of a team we should emphasize. We can do that by looking at what previous Big Ten conference champions looked like.\nSo if our goal is to predict a conference champion team, we need to know what those teams did. Here’s the regular season conference champions in this dataset since the 2017 season:\n\nlogs |&gt; \n  filter(Team == \"Michigan\" & Season == '2020-2021' | Team == \"Wisconsin\" & Season == '2019-2020' | Team == \"Michigan State\" & Season == '2018-2019' | Team == \"Michigan State\" & Season == '2017-2018' | Team == 'Illinois' & Season == '2021-2022' | Team == 'Purdue' & Season == '2022-2023' | Team == 'Purdue' & Season == '2023-2024' | Team == \"Michigan State\" & Season == '2024-2025') |&gt; \n  summarise(\n    meanNetRebounds = mean(NetRebounds),\n    meanTurnoverMargin = mean(TurnoverMargin),\n    meanTeamFGPCT = mean(TeamFGPCT),\n    meanOpponentFGPCT = mean(OpponentFGPCT)\n  )\n\n# A tibble: 1 × 4\n  meanNetRebounds meanTurnoverMargin meanTeamFGPCT meanOpponentFGPCT\n            &lt;dbl&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;\n1            8.16               1.40         0.468             0.401\n\n\nNow it’s just plug and chug.\n\n# (netrebounds estimate * meanNetRebounds) + (turnover margin estimate * meanTurnoverMargin) + (TeamFGPCT estimate * meanTeamFGPCT) + (OpponentFGPCT estimate * meanOpponentFGPCT) + Intercept\n(0.655831*8.155235) + (-1.317830*1.397112) + (89.952654*0.46787) + (-90.064048*0.4012419) + 0.168687\n\n[1] 9.624665\n\n\nSo a team with those numbers is going to average scoring 9.6 more points per game than their opponent. Not a ton, but hey, the Big Ten has been a competitive conference lately.\nHow does that compare to Maryland in 2024-25 season?\n\nlogs |&gt; \n  filter(\n    Team == \"Maryland\" & Season == '2024-2025'\n    ) |&gt; \n  summarise(\n    meanNetRebounds = mean(NetRebounds),\n    meanTurnoverMargin = mean(TurnoverMargin),\n    meanTeamFGPCT = mean(TeamFGPCT),\n    meanOpponentFGPCT = mean(OpponentFGPCT)\n  )\n\n# A tibble: 1 × 4\n  meanNetRebounds meanTurnoverMargin meanTeamFGPCT meanOpponentFGPCT\n            &lt;dbl&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;\n1            2.47              -3.44         0.469             0.415\n\n\n\n(0.655831*2.472222) + (-1.317830*-3.444444) + (89.952654*0.4687778) + (-90.064048*0.4150278) + 0.168687\n\n[1] 11.11796\n\n\nBy this model, it predicted UMD would, on average, outscore its opponents by 11 points over that season. The reality?\n\nlogs |&gt; \n     filter(\n         Team == \"Maryland\" & Season == '2024-2025'\n     ) |&gt; summarise(avg_score = mean(TeamScore), avg_opp = mean(OpponentScore))\n\n# A tibble: 1 × 2\n  avg_score avg_opp\n      &lt;dbl&gt;   &lt;dbl&gt;\n1      81.1    67.2\n\n\nWe outscored them by nearly 14 points on average, which suggests that perhaps Maryland found a way to be even more successful outside of these parameters. What would you change?",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multiple regression</span>"
    ]
  },
  {
    "objectID": "residuals.html",
    "href": "residuals.html",
    "title": "11  Residuals",
    "section": "",
    "text": "11.1 Fouls\nWhen looking at a linear model of your data, there’s a measure you need to be aware of called residuals. The residual is the distance between what the model predicted and what the real outcome is. Take our model at the end of the correlation and regression chapter. Our model predicted Maryland’s women soccer should have outscored George Mason by a goal a year ago. The match was a 3-2 loss. So our residual is -2.\nResiduals can tell you several things, but most important is if a linear model the right model for your data. If the residuals appear to be random, then a linear model is appropriate. If they have a pattern, it means something else is going on in your data and a linear model isn’t appropriate.\nResiduals can also tell you who is under-performing and over-performing the model. And the more robust the model – the better your r-squared value is – the more meaningful that label of under or over-performing is.\nLet’s go back to our model for men’s college basketball. For our predictor, let’s use Net FG Percentage - the difference between the two teams’ shooting success.\nThen load the tidyverse.\nFirst, let’s make the columns we’ll need.\nNow let’s create our model.\nWe’ve seen this output before, but let’s review because if you are using scatterplots to make a point, you should do this. First, note the Min and Max residual at the top. A team has under-performed the model by 51 points (!), and a team has overperformed it by 70 points (!!). The median residual, where half are above and half are below, is just slightly below the fit line. Close here is good.\nNext: Look at the Adjusted R-squared value. What that says is that 66 percent of a team’s scoring differential can be predicted by their FG percentage margin.\nLast: Look at the p-value. We are looking for a p-value smaller than .05. At .05, we can say that our correlation didn’t happen at random. And, in this case, it REALLY didn’t happen at random. But if you know a little bit about basketball, it doesn’t surprise you that the more you shoot better than your opponent, the more you win by. It’s an intuitive result.\nWhat we want to do now is look at those residuals. We want to add them to our individual game records. We can do that by creating two new fields – predicted and residuals – to our dataframe like this:\nUh, oh. What’s going on here? When you get a message like this, where R is complaining about the size of the data, it most likely means that your model is using some columns that have NA values. In this case, the number of columns looks small - perhaps 3 - so let’s just get rid of those rows by using the calculated columns from our model:\nNow we can try re-running the code to add the predicted and residuals columns:\nNow we can sort our data by those residuals. Sorting in descending order gives us the games where teams overperformed the model. To make it easier to read, I’m going to use select to give us just the columns we need to see and limit our results to Big Ten games.\nSo looking at this table, what you see here are the teams who scored more than their FG percentage margin would indicate. One of the predicted values should jump off the page at you.\nLook at that Maryland-Northwestern game from 2020. The Wildcats shot better than the Terps, and the model predicted Northwestern would win by 17 points. Instead, Maryland won by 11!\nBut, before we can bestow any validity on this model, we need to see if this linear model is appropriate. We’ve done that some looking at our p-values and R-squared values. But one more check is to look at the residuals themselves. We do that by plotting the residuals with the predictor. We’ll get into plotting soon, but for now just seeing it is enough.\nThe lack of a shape here – the seemingly random nature – is a good sign that a linear model works for our data. If there was a pattern, that would indicate something else was going on in our data and we needed a different model.\nAnother way to view your residuals is by connecting the predicted value with the actual value.\nThe blue line here separates underperformers from overperformers.\nNow let’s look at it where it doesn’t work as well: the total number of fouls\nfouls &lt;- logs |&gt; \n  mutate(\n    differential = TeamScore - OpponentScore, \n    TotalFouls = TeamPersonalFouls+OpponentPersonalFouls\n  )\npfit &lt;- lm(differential ~ TotalFouls, data = fouls)\nsummary(pfit)\n\n\nCall:\nlm(formula = differential ~ TotalFouls, data = fouls)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-95.709 -10.567   0.008   9.645 106.574 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.831495   0.199862   19.17   &lt;2e-16 ***\nTotalFouls  -0.070755   0.005468  -12.94   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.47 on 169218 degrees of freedom\n  (4 observations deleted due to missingness)\nMultiple R-squared:  0.0009884, Adjusted R-squared:  0.0009825 \nF-statistic: 167.4 on 1 and 169218 DF,  p-value: &lt; 2.2e-16\nSo from top to bottom:\nSo what we can say about this model is that it’s statistically significant, but doesn’t really explain much. It’s not meaningless, but on its own the total number of fouls doesn’t go very far in explaining the point differential. Normally, we’d stop right here – why bother going forward with a predictive model that isn’t terribly predictive? But let’s do it anyway. Oh, and see that “(4 observations deleted due to missingness)” bit? That means we need to lose some incomplete data again.\nfouls &lt;- fouls |&gt; filter(!is.na(TotalFouls))\nfouls$predicted &lt;- predict(pfit)\nfouls$residuals &lt;- residuals(pfit)\nfouls |&gt; arrange(desc(residuals)) |&gt; select(Team, Opponent, W_L, TeamScore, OpponentScore, TotalFouls, residuals)\n\n# A tibble: 169,220 × 7\n   Team              Opponent W_L   TeamScore OpponentScore TotalFouls residuals\n   &lt;chr&gt;             &lt;chr&gt;    &lt;chr&gt;     &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1 Bryant            &lt;NA&gt;     W           147            39         34     107. \n 2 Southern          &lt;NA&gt;     W           116            12         37     103. \n 3 McNeese State     &lt;NA&gt;     W           140            37         33     102. \n 4 Western Carolina  &lt;NA&gt;     W           141            39         30     100. \n 5 Appalachian State &lt;NA&gt;     W           135            34         35      99.6\n 6 Kansas City       &lt;NA&gt;     W           119            19         24      97.9\n 7 Purdue Fort Wayne &lt;NA&gt;     W           130            34         19      93.5\n 8 James Madison     &lt;NA&gt;     W           135            40         29      93.2\n 9 Grambling         &lt;NA&gt;     W           147            52         20      92.6\n10 Utah              Mississ… W           143            49         30      92.3\n# ℹ 169,210 more rows\nFirst, note all of the biggest misses here are all blowout games. The worst games of the season, the worst being Bryant vs. Thomas. The model missed that differential by … 107 points. The margin of victory? 108 points. In other words, this model is not great! But let’s look at it anyway.\nWell … it actually says that a linear model is appropriate. Which an important lesson – just because your residual plot says a linear model works here, that doesn’t say your linear model is good. There are other measures for that, and you need to use them.\nHere’s the segment plot of residuals – you’ll see some really long lines. That’s a bad sign. Another bad sign? A flat fit line. It means there’s no relationship between these two things. Which we already know.\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Residuals</span>"
    ]
  },
  {
    "objectID": "residuals.html#fouls",
    "href": "residuals.html#fouls",
    "title": "11  Residuals",
    "section": "",
    "text": "Our min and max go from -95 to positive 107\nOur adjusted R-squared is … 0.0009825. Not much at all.\nOur p-value is … is less than than .05, so that’s something.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Residuals</span>"
    ]
  },
  {
    "objectID": "zscores.html",
    "href": "zscores.html",
    "title": "12  Z-scores",
    "section": "",
    "text": "12.1 Calculating a Z score in R\nZ-scores are a handy way to standardize numbers so you can compare things across groupings or time. In this class, we may want to compare teams by year, or era. We can use z-scores to answer questions like who was the greatest X of all time, because a z-score can put them in context to their era.\nA z-score is a measure of how a particular stat is from the mean. It’s measured in standard deviations from that mean. A standard deviation is a measure of how much variation – how spread out – numbers are in a data set. What it means here, with regards to z-scores, is that zero is perfectly average. If it’s 1, it’s one standard deviation above the mean, and 34 percent of all cases are between 0 and 1.\nIf you think of the normal distribution, it means that 84.3 percent of all case are below that 1. If it were -1, it would mean the number is one standard deviation below the mean, and 84.3 percent of cases would be above that -1. So if you have numbers with z-scores of 3 or even 4, that means that number is waaaaaay above the mean.\nSo let’s use last year’s Maryland women’s basketball team, which if haven’t been paying attention to current events, was talented but had a few struggles.\nFor this we’ll need the logs of all college basketball games last season.\nLoad the tidyverse.\nlibrary(tidyverse)\nAnd load the data.\ngamelogs &lt;- read_csv(\"data/wbblogs24.csv\")\n\nRows: 11425 Columns: 60\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (10): Season, GameType, TeamFullName, Opponent, HomeAway, W_L, OT, URL,...\ndbl  (49): Game, TeamScore, OpponentScore, TeamFG, TeamFGA, TeamFGPCT, Team3...\ndate  (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nThe first thing we need to do is select some fields we think represent team quality and a few things to help us keep things straight. So I’m going to pick shooting percentage, rebounding and the opponent version of the same two:\nteamquality &lt;- gamelogs |&gt; \n  select(Conference, Team, TeamFGPCT, TeamTotalRebounds, OpponentFGPCT, OpponentTotalRebounds)\nAnd since we have individual game data, we need to collapse this into one record for each team. We do that with … group by and summarize.\nteamtotals &lt;- teamquality |&gt; \n  group_by(Conference, Team) |&gt; \n  summarise(\n    FGAvg = mean(TeamFGPCT), \n    ReboundAvg = mean(TeamTotalRebounds), \n    OppFGAvg = mean(OpponentFGPCT),\n    OffRebAvg = mean(OpponentTotalRebounds)\n    ) \n\n`summarise()` has grouped output by 'Conference'. You can override using the\n`.groups` argument.\nTo calculate a z-score in R, the easiest way is to use the scale function in base R. To use it, you use scale(FieldName, center=TRUE, scale=TRUE). The center and scale indicate if you want to subtract from the mean and if you want to divide by the standard deviation, respectively. We do.\nWhen we have multiple z-scores, it’s pretty standard practice to add them together into a composite score. That’s what we’re doing at the end here with TotalZscore. Note: We have to invert OppZscore and OppRebZScore by multiplying it by a negative 1 because the lower someone’s opponent number is, the better.\nteamzscore &lt;- teamtotals |&gt; \n  mutate(\n    FGzscore = as.numeric(scale(FGAvg, center = TRUE, scale = TRUE)),\n    RebZscore = as.numeric(scale(ReboundAvg, center = TRUE, scale = TRUE)),\n    OppZscore = as.numeric(scale(OppFGAvg, center = TRUE, scale = TRUE)) * -1,\n    OppRebZScore = as.numeric(scale(OffRebAvg, center = TRUE, scale = TRUE)) * -1,\n    TotalZscore = FGzscore + RebZscore + OppZscore + OppRebZScore\n  )\nSo now we have a dataframe called teamzscore that has 360 basketball teams with Z scores. What does it look like?\nhead(teamzscore)\n\n# A tibble: 6 × 11\n# Groups:   Conference [1]\n  Conference Team         FGAvg ReboundAvg OppFGAvg OffRebAvg FGzscore RebZscore\n  &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 A-10 WBB   Davidson     0.436       31.3    0.378      27.1    0.878   -0.0939\n2 A-10 WBB   Dayton       0.400       35.7    0.437      28.7   -0.225    1.29  \n3 A-10 WBB   Duquesne     0.424       33.8    0.390      32.4    0.517    0.687 \n4 A-10 WBB   Fordham      0.403       32.5    0.408      30.5   -0.136    0.286 \n5 A-10 WBB   George Mason 0.397       34.9    0.381      32.7   -0.335    1.03  \n6 A-10 WBB   George Wash… 0.370       33.7    0.390      29.7   -1.17     0.680 \n# ℹ 3 more variables: OppZscore &lt;dbl&gt;, OppRebZScore &lt;dbl&gt;, TotalZscore &lt;dbl&gt;\nA way to read this – a team with a TotalZScore of 0 is precisely average. The larger the positive number, the more exceptional they are. The larger the negative number, the more truly terrible they are.\nSo who are the best teams in the country?\nteamzscore |&gt; arrange(desc(TotalZscore))\n\n# A tibble: 359 × 11\n# Groups:   Conference [33]\n   Conference   Team      FGAvg ReboundAvg OppFGAvg OffRebAvg FGzscore RebZscore\n   &lt;chr&gt;        &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 SEC WBB      South Ca… 0.495       42.2    0.324      28.3    2.23      1.79 \n 2 MAAC WBB     Fairfield 0.464       33.1    0.361      27.4    2.26      1.26 \n 3 Summit WBB   South Da… 0.476       35.1    0.370      26.5    2.05      0.993\n 4 Sun Belt WBB James Ma… 0.424       42.4    0.363      31.0    1.18      1.60 \n 5 WCC WBB      Gonzaga   0.482       34.2    0.403      25.4    1.82      1.05 \n 6 SWAC WBB     Jackson … 0.397       38.5    0.341      29.9    0.815     1.54 \n 7 Big East WBB Connecti… 0.498       33.5    0.356      27.8    2.26      0.848\n 8 OVC WBB      Southern… 0.452       35.6    0.359      31.1    2.02      1.13 \n 9 MWC WBB      Nevada-L… 0.463       37.6    0.381      28.2    1.69      1.84 \n10 Big 12 WBB   Texas     0.490       36.2    0.384      24.4    1.66      0.793\n# ℹ 349 more rows\n# ℹ 3 more variables: OppZscore &lt;dbl&gt;, OppRebZScore &lt;dbl&gt;, TotalZscore &lt;dbl&gt;\nDon’t sleep on Fairfield! If we look for Power Five schools, UConn and South Carolina are at the top, which checks out.\nBut closer to home, how is Maryland doing?\nteamzscore |&gt; \n  filter(Conference == \"Big Ten WBB\") |&gt; \n  arrange(desc(TotalZscore)) |&gt;\n  select(Team, TotalZscore)\n\nAdding missing grouping variables: `Conference`\n\n\n# A tibble: 14 × 3\n# Groups:   Conference [1]\n   Conference  Team           TotalZscore\n   &lt;chr&gt;       &lt;chr&gt;                &lt;dbl&gt;\n 1 Big Ten WBB Iowa                 4.41 \n 2 Big Ten WBB Indiana              3.84 \n 3 Big Ten WBB Nebraska             3.15 \n 4 Big Ten WBB Penn State           1.88 \n 5 Big Ten WBB Illinois             1.20 \n 6 Big Ten WBB Michigan             0.396\n 7 Big Ten WBB Maryland            -0.651\n 8 Big Ten WBB Ohio State          -0.662\n 9 Big Ten WBB Minnesota           -0.856\n10 Big Ten WBB Wisconsin           -0.905\n11 Big Ten WBB Michigan State      -1.17 \n12 Big Ten WBB Rutgers             -1.91 \n13 Big Ten WBB Purdue              -2.54 \n14 Big Ten WBB Northwestern        -6.20\nSo, as we can see, with our composite Z Score, Maryland is below average; not great. But better than Ohio State. Notice how, by this measure, Indiana and Iowa are far ahead of most of the conference, with Nebraska a somewhat surprising third.\nWe can limit our results to just Power Five conferences plus the Big East:\npowerfive_plus_one &lt;- c(\"SEC WBB\", \"Big Ten WBB\", \"Pac-12 WBB\", \"Big 12 WBB\", \"ACC WBB\", \"Big East WBB\")\nteamzscore |&gt; \n  filter(Conference %in% powerfive_plus_one) |&gt; \n  arrange(desc(TotalZscore)) |&gt;\n  select(Team, TotalZscore)\n\nAdding missing grouping variables: `Conference`\n\n\n# A tibble: 80 × 3\n# Groups:   Conference [6]\n   Conference   Team            TotalZscore\n   &lt;chr&gt;        &lt;chr&gt;                 &lt;dbl&gt;\n 1 SEC WBB      South Carolina         7.55\n 2 Big East WBB Connecticut            5.79\n 3 Big 12 WBB   Texas                  5.55\n 4 Pac-12 WBB   Stanford               5.12\n 5 SEC WBB      Louisiana State        5.09\n 6 Pac-12 WBB   UCLA                   4.73\n 7 ACC WBB      Virginia Tech          4.53\n 8 Big Ten WBB  Iowa                   4.41\n 9 Big Ten WBB  Indiana                3.84\n10 ACC WBB      Duke                   3.65\n# ℹ 70 more rows\nThis makes a certain amount of sense: three of the Final Four teams - South Carolina, UConn and Iowa are in the top 10. N.C. State, the fourth team, ranks 16th. Duke is an interesting #10 here. It doesn’t necessarily mean they were the ninth-best team, but given their competition they shot the ball and rebounded the ball very well.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Z-scores</span>"
    ]
  },
  {
    "objectID": "zscores.html#calculating-a-z-score-in-r",
    "href": "zscores.html#calculating-a-z-score-in-r",
    "title": "12  Z-scores",
    "section": "",
    "text": "For this walkthrough:\n   Download csv file",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Z-scores</span>"
    ]
  },
  {
    "objectID": "zscores.html#writing-about-z-scores",
    "href": "zscores.html#writing-about-z-scores",
    "title": "12  Z-scores",
    "section": "12.2 Writing about z-scores",
    "text": "12.2 Writing about z-scores\nThe great thing about z-scores is that they make it very easy for you, the sports analyst, to create your own measures of who is better than who. The downside: Only a small handful of sports fans know what the hell a z-score is.\nAs such, you should try as hard as you can to avoid writing about them.\nIf the word z-score appears in your story or in a chart, you need to explain what it is. “The ranking uses a statistical measure of the distance from the mean called a z-score” is a good way to go about it. You don’t need a full stats textbook definition, just a quick explanation. And keep it simple.\nNever use z-score in a headline. Write around it. Away from it. Z-score in a headline is attention repellent. You won’t get anyone to look at it. So “Tottenham tops in z-score” bad, “Tottenham tops in the Premiere League” good.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Z-scores</span>"
    ]
  },
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "13  Clustering",
    "section": "",
    "text": "13.1 Advanced metrics\nOne common effort in sports is to classify teams and players – who are this players peers? What teams are like this one? Who should we compare a player to? Truth is, most sports commentators use nothing more sophisticated that looking at a couple of stats or use the “eye test” to say a player is like this or that.\nThere are better ways.\nIn this chapter, we’re going to use a method that sounds advanced but it really quite simple called k-means clustering. It’s based on the concept of the k-nearest neighbor algorithm. You’re probably already scared. Don’t be.\nImagine two dots on a scatterplot. If you took a ruler out and measured the distance between those dots, you’d know how far apart they are. In math, that’s called the Euclidean distance. It’s just the space between them in numbers. Where k-nearest neighbor comes in, you have lots of dots and you want measure the distance between all of them. What does k-means clustering do? It lumps them into groups based on the average distance between them. Players who are good on offense but bad on defense are over here, good offense good defense are over here. And using the Euclidean distance between them, we can decide who is in and who is out of those groups.\nFor this exercise, I want to look at Derik Queen, who played one season at Maryland before decamping for the NBA. Had he stayed, he might have been among the all-time Terp greats. So who does Derik Queen compare to?\nTo answer this, we’ll use k-means clustering.\nFirst thing we do is load some libraries and set a seed, so if we run this repeatedly, our random numbers are generated from the same base. If you don’t have the cluster library, just add it on the console with install.packages(\"cluster\")\nI’ve gone and scraped stats for every player last season.\nNow load that data.\nTo cluster this data properly, we have some work to do.\nFirst, it won’t do to have players who haven’t played, so we can use filter to find anyone with greater than 0 minutes played. Next, Derik Queen is listed as a center, so let’s just look at centers. Third, we want to limit the data to things that make sense to look at for Queen – things like shooting, rebounds, blocks, turnovers and points.\nNow, k-means clustering doesn’t work as well with data that can be on different scales. So comparing a percentage to a count metric – shooting percentage to points – would create chaos because shooting percentages are a fraction of 1 and points, depending on when they are in the season, could be quite large. So we have to scale each metric – put them on a similar basis using the distance from the max value as our guide. Also, k-means clustering won’t work with text data, so we need to create a dataframe that’s just the numbers, but scaled. We can do that with another select, and using mutate_all with the scale function. The na.omit() means get rid of any blanks, because they too will cause errors.\nWith k-means clustering, we decide how many clusters we want. Most often, researchers will try a handful of different cluster numbers and see what works. But there are methods for finding the optimal number. One method is called the Elbow method. One implementation of this, borrowed from the University of Cincinnati’s Business Analytics program, does this quite nicely with a graph that will help you decide for yourself.\nAll you need to do in this code is change out the data frame – playersscaled in this case – and run it.\nThe Elbow method – so named because you’re looking for the “elbow” where the line flattens out. In this case, it looks like a K of 8 is ideal. So let’s try that. We’re going to use the kmeans function, saving it to an object called k5. We just need to tell it our dataframe name, how many centers (k) we want, and we’ll use a sensible default for how many different configurations to try.\nLet’s look at what we get.\nInterpreting this output, the very first thing you need to know is that the cluster numbers are meaningless. They aren’t ranks. They aren’t anything. After you have taken that on board, look at the cluster sizes at the top. Clusters 2 and 8 are pretty large compared to others. That’s notable. Then we can look at the cluster means. For reference, 0 is going to be average. So group 5 is above average on minutes played. Groups 2 is slightly below, group 1 is well above.\nSo which group is Derik Queen in? Well, first we have to put our data back together again. In K8, there is a list of cluster assignments in the same order we put them in, but recall we have no names. So we need to re-combine them with our original data. We can do that with the following:\nNow we have a dataframe called playercluster that has our player names and what cluster they are in. The fastest way to find Derik Queen is to double click on the playercluster table in the environment and use the search in the top right of the table. Because this is based on some random selections of points to start the groupings, these may change from person to person, but Smith is in Group 2 in my data.\nWe now have a dataset and can plot it like anything else. Let’s get Derik Queen and then plot him against the rest of college basketball on rebounds versus minutes played.\nSo Derik’s in cluster 1, which if you look at our clusters, puts him in the cluster with nearly all above average metrics. What does that look like? We know Derik was a rebounding machine, so where do group 1 people grade out on rebounds?\nNot bad, not bad. But who are Derik Queen’s peers? If we look at the numbers in Group 1, there’s 8 of them.\nSo here are the 7 centers most like Derik Queen last season. Were they the best centers in the country?\nHow much does this change if we change the metrics? I used pretty standard box score metrics above. What if we did it using Player Efficiency Rating, True Shooting Percentage, Point Production, Assist Percentage, Win Shares Per 40 Minutes and Box Plus Minus (you can get definitions of all of them by hovering over the stats on Maryland’s stats page).\nWe’ll repeat the process. Filter out players who don’t play, players with stats missing, and just focus on those stats listed above.\nplayersadvanced &lt;- players |&gt;\n  filter(MP&gt;0) |&gt;\n  filter(Pos == \"C\") |&gt;\n  select(Player, Team, Pos, PER, `TS%`, PProd, `AST%`, `WS/40`, BPM) |&gt;\n  na.omit()\nNow to scale them.\nplayersadvscaled &lt;- playersadvanced |&gt;\n  select(PER, `TS%`, PProd, `AST%`, `WS/40`, BPM) |&gt;\n  mutate_all(scale) |&gt;\n  na.omit()\nLet’s find the optimal number of clusters.\n# function to compute total within-cluster sum of square\nwss &lt;- function(k) {\n  kmeans(playersadvscaled, k, nstart = 10 )$tot.withinss\n}\n\n# Compute and plot wss for k = 1 to k = 15\nk.values &lt;- 1:15\n\n# extract wss for 2-15 clusters\nwss_values &lt;- map_dbl(k.values, wss)\n\nplot(k.values, wss_values,\n       type=\"b\", pch = 19, frame = FALSE,\n       xlab=\"Number of clusters K\",\n       ylab=\"Total within-clusters sum of squares\")\nLooks like 8 again.\nadvk8 &lt;- kmeans(playersadvscaled, centers = 8, nstart = 25)\nWhat do we have here?\nadvk8\n\nK-means clustering with 8 clusters of sizes 10, 54, 2, 25, 56, 63, 31, 17\n\nCluster means:\n          PER         TS%      PProd       AST%        WS/40         BPM\n1 -2.80397873 -3.32576860 -0.9757080 -1.1460236 -2.779002133 -2.44465692\n2  0.58549934  0.46575451  0.9259213 -0.1446943  0.564620860  0.56788941\n3  3.91910490  2.61177952 -0.9430676 -1.3207832  3.141851512  3.45044891\n4  1.03333914  0.26289465  2.0174109  1.7580590  0.882444035  1.09048724\n5 -0.80660282 -0.61517254 -0.7087302 -0.3516492 -0.770575283 -0.81273103\n6 -0.06088192  0.22607412 -0.4488703 -0.5876646 -0.008179258  0.02469389\n7 -0.14080517 -0.02569281 -0.3445817  0.9967488 -0.156052595 -0.28053836\n8  0.94831844  1.01849923 -0.5965913  0.2223432  1.027107675  0.72184555\n\nClustering vector:\n  [1] 2 2 6 1 4 6 6 2 5 6 6 5 2 7 6 5 2 5 5 6 6 2 6 4 5 5 7 6 4 7 2 4 2 5 7 2 6\n [38] 4 6 6 1 6 7 7 2 5 2 6 6 2 6 6 3 8 1 7 5 1 2 6 6 7 3 8 2 6 8 5 6 7 6 2 8 7\n [75] 5 6 4 6 5 6 2 4 2 8 4 6 5 2 6 4 2 2 7 8 8 7 6 2 7 7 5 5 6 6 6 2 4 5 5 1 7\n[112] 7 7 6 6 5 2 6 5 2 6 4 6 5 6 2 5 4 7 6 5 5 1 5 5 7 6 5 5 6 6 2 6 5 5 6 2 6\n[149] 5 4 2 2 7 2 8 2 4 8 1 6 4 5 5 7 5 2 7 4 7 4 2 4 2 2 2 5 5 2 6 8 6 4 2 5 4\n[186] 6 5 5 5 7 8 5 6 2 5 5 7 7 2 6 2 5 2 8 1 7 5 2 6 6 5 5 6 6 2 8 1 6 6 8 4 8\n[223] 6 2 4 2 6 1 2 5 8 7 2 2 2 7 5 2 7 5 6 4 2 5 6 4 6 5 4 8 2 6 5 5 5 2 7 5\n\nWithin cluster sum of squares by cluster:\n[1] 61.68774 54.61697  3.33234 59.95049 83.85300 64.48091 53.46295 43.85311\n (between_SS / total_SS =  72.4 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"\nLooks like this time, cluster 1 is all below average and cluster 4 is all above. Which cluster is Derik Queen in?\nplayeradvcluster &lt;- data.frame(playersadvanced, advk8$cluster)\ndqadv &lt;- playeradvcluster |&gt; filter(Player == \"Derik Queen\")\n\ndqadv\n\n       Player                     Team Pos  PER   TS. PProd AST. WS.40 BPM\n1 Derik Queen Maryland Terrapins Men's   C 24.7 0.591   584 11.6 0.216 9.2\n  advk8.cluster\n1             4\nCluster 4 on my dataset. So in this season, we can say he’s in a group of players who are all above average on these advanced metrics.\nNow who are his peers?\nplayeradvcluster |&gt;\n  filter(advk8.cluster == 4) |&gt;\n  arrange(desc(PProd))\n\n               Player                                Team Pos  PER   TS. PProd\n1    Ryan Kalkbrenner            Creighton Bluejays Men's   C 29.1 0.684   589\n2         Derik Queen            Maryland Terrapins Men's   C 24.7 0.591   584\n3       Bent Leuchten           UC-Irvine Anteaters Men's   C 28.7 0.648   582\n4    Hunter Dickinson               Kansas Jayhawks Men's   C 25.8 0.571   558\n5        Robbie Avila         Saint Louis Billikens Men's   C 21.0 0.605   542\n6     Noah Williamson                Bucknell Bison Men's   C 23.5 0.594   515\n7       Nathan Bittle                  Oregon Ducks Men's   C 25.5 0.598   476\n8       Essam Mostafa Middle Tennessee Blue Raiders Men's   C 29.0 0.621   468\n9      Kyler Filewich              Wofford Terriers Men's   C 24.6 0.529   452\n10        Oumar Ballo              Indiana Hoosiers Men's   C 23.7 0.618   404\n11      Payton Sparks          Ball State Cardinals Men's   C 26.1 0.618   404\n12     Mitchell Saxen            Saint Mary's Gaels Men's   C 22.3 0.547   400\n13    Tomislav Ivisic      Illinois Fighting Illini Men's   C 22.0 0.597   400\n14     Carter Welling        Utah Valley Wolverines Men's   C 21.0 0.560   388\n15      Jeff Woodward               Colgate Raiders Men's   C 27.9 0.655   376\n16    Amar Kuljuhovic   North Dakota Fighting Hawks Men's   C 19.4 0.572   373\n17       Jacob Hutson        Northern Iowa Panthers Men's   C 19.5 0.557   370\n18    Christoph Tilly           Santa Clara Broncos Men's   C 23.5 0.623   367\n19 Justin Vander Baan            Lafayette Leopards Men's   C 23.0 0.547   364\n20        Miles Rubin          Loyola (IL) Ramblers Men's   C 22.1 0.683   322\n21       Michael Walz              Richmond Spiders Men's   C 19.5 0.587   269\n22    Lawson Lovering                     Utah Utes Men's   C 15.5 0.505   262\n23     Dylan Cardwell                 Auburn Tigers Men's   C 20.3 0.627   240\n24   Brandon Garrison             Kentucky Wildcats Men's   C 16.6 0.547   231\n25          Aday Mara                   UCLA Bruins Men's   C 28.8 0.594   216\n   AST. WS.40  BPM advk8.cluster\n1  10.0 0.236 12.2             4\n2  11.6 0.216  9.2             4\n3  12.5 0.277  9.0             4\n4  14.4 0.197 10.3             4\n5  24.1 0.158  4.7             4\n6  12.7 0.154  2.2             4\n7  14.4 0.203 11.4             4\n8   9.7 0.237  6.3             4\n9  24.1 0.168  3.5             4\n10 14.7 0.174  8.8             4\n11 13.6 0.197  2.2             4\n12 11.2 0.196  8.5             4\n13 15.8 0.187 10.7             4\n14 12.1 0.160  2.9             4\n15 22.6 0.209  3.0             4\n16 14.0 0.111  0.2             4\n17 14.9 0.148  1.8             4\n18 16.0 0.193  6.7             4\n19 24.0 0.157  3.8             4\n20 11.7 0.167  5.1             4\n21 27.3 0.135  3.2             4\n22 22.0 0.097  3.6             4\n23 12.7 0.189 11.4             4\n24 18.0 0.116  6.3             4\n25 16.7 0.222 11.4             4\nSorting on Points Produced, Derik Queen is second out of the 25 centers who land in Cluster 4. Seems advanced metrics rate him pretty highly.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "simulations.html",
    "href": "simulations.html",
    "title": "14  Simulations",
    "section": "",
    "text": "14.1 Cold streaks\nOn Feb. 21, 2023, fans of Maryland women’s basketball got a show from Brinae Alexander. The graduate transfer guard hit 6 of 9 three-pointers in a 96-68 destruction of Iowa (Caitlin Clark scored 18, her second-lowest output of the season). It was glorious.\nBut how rare was it? Did Alexander get lucky that night?\nLuck is something that comes up a lot in sports. Is a team lucky? Or a player? One way we can get to this, we can get to that is by simulating things based on their typical percentages. Simulations work by choosing random values within a range based on a distribution. The most common distribution is the normal or binomial distribution. The normal distribution is where the most cases appear around the mean, 66 percent of cases are within one standard deviation from the mean, and the further away from the mean you get, the more rare things become.\nLet’s simulate 10 three point attempts (0-9 makes) 1000 times with Alexander’s season long shooting percentage and see if this could just be random chance or something else.\nWe do this using a base R function called rbinom or binomial distribution. So what that means is there’s a normally distributed chance that Brinae Alexander is going to shoot above and below her season three point shooting percentage. If we randomly assign values in that distribution 1000 times, how many times will it come up 6, like this example?\nFirst, we’ll load the tidyverse\nHow do we read this? The first row and the second row form a pair. The top row is the number of shots made. The number immediately under it is the number of simulations where that occurred.\nSo what we see is given her season-long shooting percentage, it’s not out of the realm of randomness that she’d make 6 of those 9 attempts. In 1000 simulations, it comes up 130 times. So more than one time in 10, Brinae Alexander will go 6-9 from deep. While it’s more likely that she’d hit 4 or 5 of those attempts, a one-in-ten chance isn’t nothing.\nDuring the final regular-season game in the 2021-22 season, Maryland’s men’s team, shooting .326 on the season from behind the arc, went 1-15 in the first half. How strange is that?\nset.seed(1234)\n\nsimulations &lt;- rbinom(n = 1000, size = 15, prob = .326)\n\nhist(simulations)\n\n\n\n\n\n\n\ntable(simulations)\n\nsimulations\n  0   1   2   3   4   5   6   7   8   9  10  11 \n  5  17  61 135 204 222 164  93  60  33   3   3\nShort answer: Pretty weird, but not totally unheard of. If you simulate 15 threes 1000 times, about 17 times it will result in a single made three-pointer. It’s slightly more common that the team would hit 9 threes out of 15. So going that cold is not totally out of the realm of random chance, but it’s pretty rare.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Simulations</span>"
    ]
  },
  {
    "objectID": "simulations.html#the-hottest-of-streaks",
    "href": "simulations.html#the-hottest-of-streaks",
    "title": "14  Simulations",
    "section": "14.2 The hottest of streaks",
    "text": "14.2 The hottest of streaks\nTwo years ago, Terps baseball player Sam Hojnar hit two home runs in consecutive games. He hit 16 HRs for the season in 233 at-bats, so his home run probability per at-bat was just under seven percent. We’ll use that and the number of games (56) to calculate the odds that he’d hit two home runs in each of two consecutive games.\n\n# Hojnar's statistics\nhome_runs &lt;- 16\ngames &lt;- 56\nat_bats &lt;- 233\n\n# Calculate probabilities\nhome_run_prob_per_at_bat &lt;- home_runs / at_bats\navg_at_bats_per_game &lt;- at_bats / games\n\n# Set simulation parameters\nnum_simulations &lt;- 100000\n\n# Run simulation\nset.seed(1234)  # For reproducibility\n\nsimulation_results &lt;- tibble(\n  sim = 1:num_simulations,\n  game1 = rbinom(num_simulations, round(avg_at_bats_per_game), home_run_prob_per_at_bat),\n  game2 = rbinom(num_simulations, round(avg_at_bats_per_game), home_run_prob_per_at_bat)\n) |&gt;\n  mutate(two_hr_each_game = game1 == 2 & game2 == 2)\n\n# Calculate probability\nprobability &lt;- simulation_results |&gt;\n  summarise(prob = mean(two_hr_each_game)) |&gt;\n  pull(prob)\n\n# Print results\ncat(\"Estimated probability of hitting exactly two home runs in each of two consecutive games:\", format(round(probability, 6), scientific = FALSE))\n\nEstimated probability of hitting exactly two home runs in each of two consecutive games: 0.0007\n\n\nLet’s parse that code. We set up our simulations as usual, with one change: because this already seems like a pretty rare event, we’re running 100,000 simulations, and we have to calculate the odds for two games, not one. Then we’re looking for results where the number of home runs in both games is two. The simulation_results dataframe shows that 70 times out of 100,000 Hojnar would hit two home runs in consecutive games, based on his own performance. That’s very, very, very unlikely, and quite a hot streak.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Simulations</span>"
    ]
  },
  {
    "objectID": "barcharts.html",
    "href": "barcharts.html",
    "title": "15  Intro to ggplot with bar charts",
    "section": "",
    "text": "15.1 The bar chart\nWith ggplot2, we dive into the world of programmatic data visualization. The ggplot2 library implements something called the grammar of graphics. The main concepts are:\nHadley Wickham, who is behind all of the libraries we have used in this course to date, wrote about his layered grammar of graphics in this 2009 paper that is worth your time to read.\nHere are some ggplot2 resources you’ll want to keep handy:\nLet’s dive in using data we’ve already seen before – football attendance. This workflow will represent a clear picture of what your work in this class will be like for much of the rest of the semester. One way to think of this workflow is that your R Notebook is now your digital sketchbook, where you will try different types of visualizations to find ones that work. Then, you will either write the code that adds necessary and required parts to finish it, or you’ll export your work into a program like Illustrator to finish the work.\nTo begin, we’ll use data we’ve seen before: college football attendance.\nNow load the tidyverse.\nAnd the data.\nFirst, let’s get a top 10 list by announced attendance in the most recent season we have data. We’ll use the same tricks we used in the filtering assignment.\nThat looks good, so let’s save it to a new data frame and use that data frame instead going forward.\nThe easiest thing we can do is create a simple bar chart of our data. Bar charts show magnitude. They invite you to compare how much more or less one thing is compared to others.\nWe could, for instance, create a bar chart of the total attendance. To do that, we simply tell ggplot2 what our dataset is, what element of the data we want to make the bar chart out of (which is the aesthetic), and the geometry type (which is the geom). It looks like this:\nggplot() + geom_bar(data=top10, aes(x=Institution))\nNote: top10 is our data, aes means aesthetics, x=Institution explicitly tells ggplot2 that our x value – our horizontal value – is the Institution field from the data, and then we add on the geom_bar() as the geometry. And what do we get when we run that?\nggplot() + \n  geom_bar(\n    data=top10, \n    aes(x=Institution)\n  )\nWe get … weirdness. We expected to see bars of different sizes, but we get all with a count of 1. What gives? Well, this is the default behavior. What we have here is something called a histogram, where ggplot2 helpfully counted up the number of times the Institution appears and counted them up. Since we only have one record per Institution, the count is always 1. How do we fix this? By adding weight to our aesthetic.\nggplot() + \n  geom_bar(\n    data=top10, \n    aes(x=Institution, weight=`2024`)\n  )\nCloser. But … what order is that in? And what happened to our count numbers on the left? Why are they in scientific notation?\nLet’s deal with the ordering first. ggplot2’s default behavior is to sort the data by the x axis variable. So it’s in alphabetical order. To change that, we have to reorder it. With reorder, we first have to tell ggplot what we are reordering, and then we have to tell it HOW we are reordering it. So it’s reorder(FIELD, SORTFIELD).\nggplot() + \n  geom_bar(\n    data=top10, \n    aes(\n      x=reorder(Institution, `2024`), \n      weight=`2024`\n      )\n    )\nBetter. We can argue about if the right order is smallest to largest or largest to smallest. But this gets us close. By the way, to sort it largest to smallest, put a negative sign in front of the sort field.\nggplot() + \n  geom_bar(\n    data=top10, \n    aes(\n      x=reorder(Institution, -`2024`), \n      weight=`2024`\n      )\n    )",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Intro to ggplot with bar charts</span>"
    ]
  },
  {
    "objectID": "barcharts.html#scales",
    "href": "barcharts.html#scales",
    "title": "15  Intro to ggplot with bar charts",
    "section": "15.2 Scales",
    "text": "15.2 Scales\nTo fix the axis labels, we need try one of the other main elements of the ggplot2 library, which is transform a scale. More often that not, that means doing something like putting it on a logarithmic scale or some other kind of transformation. In this case, we’re just changing how it’s represented. The default in ggplot2 for large values is to express them as scientific notation. Rarely ever is that useful in our line of work. So we have to transform them into human readable numbers.\nThe easiest way to do this is to use a library called scales and it’s already installed.\n\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n\nTo alter the scale, we add a piece to our plot with + and we tell it which scale is getting altered and what kind of data it is. In our case, our Y axis is what is needing to be altered, and it’s continuous data (meaning it can be any number between x and y, vs discrete data which are categorical). So we need to add scale_y_continuous and the information we want to pass it is to alter the labels with a function called comma.\n\nggplot() + \n  geom_bar(\n    data=top10, \n    aes(\n      x=reorder(Institution, -`2024`), \n      weight=`2024`\n      )\n    ) + \n  scale_y_continuous(labels=comma)\n\n\n\n\n\n\n\n\nBetter.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Intro to ggplot with bar charts</span>"
    ]
  },
  {
    "objectID": "barcharts.html#styling",
    "href": "barcharts.html#styling",
    "title": "15  Intro to ggplot with bar charts",
    "section": "15.3 Styling",
    "text": "15.3 Styling\nWe are going to spend a lot more time on styling, but let’s add some simple labels to this with a new bit called labs which is short for labels.\n\nggplot() + \n  geom_bar(\n    data=top10, \n    aes(\n      x=reorder(Institution, -`2024`), \n      weight=`2024`)\n    ) + \n  scale_y_continuous(labels=comma) + \n  labs(\n    title=\"Top 10 Football Programs By Attendance\", \n    x=\"School\", \n    y=\"Attendance\"\n)\n\n\n\n\n\n\n\n\nThe library has lots and lots of ways to alter the styling – we can programmatically control nearly every part of the look and feel of the chart. One simple way is to apply themes in the library already. We do that the same way we’ve done other things – we add them. Here’s the light theme.\n\nggplot() + \n  geom_bar(\n    data=top10, \n    aes(x=reorder(Institution, -`2024`),\n        weight=`2024`)) + \n  scale_y_continuous(labels=comma) + \n  labs(\n    title=\"Top 10 Football Programs By Attendance\", \n    x=\"School\", \n    y=\"Attendance\") + \n  theme_light()\n\n\n\n\n\n\n\n\nOr the minimal theme:\n\nggplot() + \n  geom_bar(\n    data=top10, \n    aes(x=reorder(Institution, -`2024`),\n        weight=`2024`)) + \n  scale_y_continuous(labels=comma) + \n  labs(\n    title=\"Top 10 Football Programs By Attendance\", \n    x=\"School\", \n    y=\"Attendance\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\nIf you like, you can write your own themes. For now, the built in ones will get us closer to something that looks good.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Intro to ggplot with bar charts</span>"
    ]
  },
  {
    "objectID": "barcharts.html#one-last-trick-coord-flip",
    "href": "barcharts.html#one-last-trick-coord-flip",
    "title": "15  Intro to ggplot with bar charts",
    "section": "15.4 One last trick: coord flip",
    "text": "15.4 One last trick: coord flip\nSometimes, we don’t want vertical bars. Maybe we think this would look better horizontal. How do we do that? By adding coord_flip() to our code. It does what it says – it inverts the coordinates of the figures.\n\nggplot() + \n  geom_bar(\n    data=top10, \n    aes(x=reorder(Institution, -`2024`),\n        weight=`2024`)) + \n  scale_y_continuous(labels=comma) + \n  labs(\n    title=\"Top 10 Football Programs By Attendance\", \n    x=\"School\", \n    y=\"Attendance\") + \n  theme_minimal() + \n  coord_flip()",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Intro to ggplot with bar charts</span>"
    ]
  },
  {
    "objectID": "stackedbars.html",
    "href": "stackedbars.html",
    "title": "16  Stacked bar charts",
    "section": "",
    "text": "One of the elements of data visualization excellence is inviting comparison. Often that comes in showing what proportion a thing is in relation to the whole thing. With bar charts, we’re showing magnitude of the whole thing. If we have information about the parts of the whole, we can stack them on top of each other to compare them, showing both the whole and the components. And it’s a simple change to what we’ve already done.\nWe’re going to use a dataset of college basketball games from this past season.\nFor this walkthrough:\n   Download csv file\n\nLoad the tidyverse.\n\nlibrary(tidyverse)\n\nAnd the data.\n\ngames &lt;- read_csv(\"data/logs25.csv\")\n\nRows: 11962 Columns: 60\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (10): Season, GameType, TeamFullName, Opponent, HomeAway, W_L, OT, URL,...\ndbl  (49): Game, TeamScore, OpponentScore, TeamFG, TeamFGA, TeamFGPCT, Team3...\ndate  (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWhat we have here is every game in college basketball this past season. The question we want to answer is this: Who were the best rebounders in the Big Ten? And what role did offensive and defensive rebounds play in making that happen?\nSo to make this chart, we have to just add one thing to a bar chart like we did in the previous chapter. However, it’s not that simple.\nWe have game data, and we need season data. To get that, we need to do some group by and sum work. And since we’re only interested in the Big Ten, we have some filtering to do too. For this, we’re going to measure offensive rebounds and total rebounds, and then we can calculate defensive rebounds. So if we have all the games a team played, and the offensive rebounds and total rebounds for each of those games, what we need to do to get the season totals is just add them up.\n\ngames |&gt; \n  filter(!is.na(TeamTotalRebounds)) |&gt; \n  group_by(Conference, Team) |&gt; \n  summarise(\n    SeasonOffRebounds = sum(TeamOffRebounds),\n    SeasonTotalRebounds = sum(TeamTotalRebounds)\n  ) |&gt;\n  mutate(\n    SeasonDefRebounds = SeasonTotalRebounds - SeasonOffRebounds\n  ) |&gt; \n  select(\n    -SeasonTotalRebounds\n  ) |&gt; \n  filter(Conference == \"Big Ten MBB\")\n\n# A tibble: 18 × 4\n# Groups:   Conference [1]\n   Conference  Team                SeasonOffRebounds SeasonDefRebounds\n   &lt;chr&gt;       &lt;chr&gt;                           &lt;dbl&gt;             &lt;dbl&gt;\n 1 Big Ten MBB Illinois                          408               982\n 2 Big Ten MBB Indiana                           292               754\n 3 Big Ten MBB Iowa                              250               731\n 4 Big Ten MBB Maryland                          321               881\n 5 Big Ten MBB Michigan                          346               950\n 6 Big Ten MBB Michigan State                    386               972\n 7 Big Ten MBB Minnesota                         271               702\n 8 Big Ten MBB Nebraska                          270               876\n 9 Big Ten MBB Northwestern                      302               709\n10 Big Ten MBB Ohio State                        252               716\n11 Big Ten MBB Oregon                            285               829\n12 Big Ten MBB Penn State                        242               711\n13 Big Ten MBB Purdue                            302               776\n14 Big Ten MBB Rutgers                           304               720\n15 Big Ten MBB Southern California               262               759\n16 Big Ten MBB UCLA                              325               692\n17 Big Ten MBB Washington                        257               672\n18 Big Ten MBB Wisconsin                         279               945\n\n\nBy looking at this, we can see we got what we needed. We have 14 teams and numbers that look like season totals for two types of rebounds. Save that to a new dataframe.\n\nrebounds &lt;- games |&gt; \n  filter(!is.na(TeamTotalRebounds)) |&gt; \n  group_by(Conference, Team) |&gt; \n  summarise(\n    SeasonOffRebounds = sum(TeamOffRebounds),\n    SeasonTotalRebounds = sum(TeamTotalRebounds)\n  ) |&gt;\n  mutate(\n    SeasonDefRebounds = SeasonTotalRebounds - SeasonOffRebounds\n  ) |&gt; \n  select(\n    -SeasonTotalRebounds\n  ) |&gt; \n  filter(Conference == \"Big Ten MBB\")\n\nNow, the problem we have is that ggplot wants long data and this data is wide. So we need to use tidyr to make it long, just like we did in the transforming data chapter.\n\nrebounds |&gt; \n  pivot_longer(\n    cols=starts_with(\"Season\"), \n    names_to=\"Type\", \n    values_to=\"Rebounds\")\n\n# A tibble: 36 × 4\n# Groups:   Conference [1]\n   Conference  Team     Type              Rebounds\n   &lt;chr&gt;       &lt;chr&gt;    &lt;chr&gt;                &lt;dbl&gt;\n 1 Big Ten MBB Illinois SeasonOffRebounds      408\n 2 Big Ten MBB Illinois SeasonDefRebounds      982\n 3 Big Ten MBB Indiana  SeasonOffRebounds      292\n 4 Big Ten MBB Indiana  SeasonDefRebounds      754\n 5 Big Ten MBB Iowa     SeasonOffRebounds      250\n 6 Big Ten MBB Iowa     SeasonDefRebounds      731\n 7 Big Ten MBB Maryland SeasonOffRebounds      321\n 8 Big Ten MBB Maryland SeasonDefRebounds      881\n 9 Big Ten MBB Michigan SeasonOffRebounds      346\n10 Big Ten MBB Michigan SeasonDefRebounds      950\n# ℹ 26 more rows\n\n\nWhat you can see now is that we have two rows for each team: one for offensive rebounds, one for defensive rebounds. This is what ggplot needs. Save it to a new dataframe.\n\nreboundslong &lt;- rebounds |&gt; \n  pivot_longer(\n    cols=starts_with(\"Season\"), \n    names_to=\"Type\", \n    values_to=\"Rebounds\")\n\nBuilding on what we learned in the last chapter, we know we can turn this into a bar chart with an x value, a weight and a geom_bar. What we are going to add is a fill. The fill will stack bars on each other based on which element it is. In this case, we can fill the bar by Type, which means it will stack the number of offensive rebounds on top of defensive rebounds and we can see how they compare.\n\nggplot() + \n  geom_bar(\n    data=reboundslong, \n    aes(x=Team, weight=Rebounds, fill=Type)) + \n  coord_flip()\n\n\n\n\n\n\n\n\nWhat’s the problem with this chart?\nThere’s a couple of things, one of which we’ll deal with now: The ordering is alphabetical (from the bottom up). So let’s reorder the teams by Rebounds.\n\nggplot() + \n  geom_bar(\n    data=reboundslong, \n    aes(x=reorder(Team, Rebounds), \n        weight=Rebounds, \n        fill=Type)) + \n  coord_flip()\n\n\n\n\n\n\n\n\nAnd just like that … Michigan State, the team with the best record in the league, comes out #2 behind Illinois. Maryland is fifth, which seems about right.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Stacked bar charts</span>"
    ]
  },
  {
    "objectID": "circularbarcharts.html",
    "href": "circularbarcharts.html",
    "title": "17  Circular bar plots",
    "section": "",
    "text": "17.1 Does November basketball matter?\nDoes November basketball really not matter? Are games played early in the season, before teams have had a chance to learn how to play together and when many teams feast on cupcake schedules, meaningful come March?\nLet’s look, using a new form of chart called a circular bar plot. It’s a chart type that combines several forms we’ve used before: bar charts to show magnitude, stacked bar charts to show proportion, but we’re going to add bending the chart around a circle to add some visual interesting-ness to it. We’re also going to use time as an x-axis value to make a not subtle circle of time reference – a common technique with circular bar charts.\nWe’ll use a dataset of every women’s college basketball game last season.\nLoad your libraries.\nAnd load your data.\nSo let’s test the notion of November Basketball Doesn’t Matter. What matters in basketball? Let’s start simple: Wins.\nSports Reference’s win columns are weird, so we need to scan through them and find W and L and we’ll give them numbers using case_when. I’ll also filter out post-season tournament games.\nwinlosslogs &lt;- logs |&gt; \n  filter(Date &lt; '2025-03-15') |&gt; \n  mutate(winloss = case_when(\n    grepl(\"W\", W_L) ~ 1, \n    grepl(\"L\", W_L) ~ 0)\n)\nWe can group by date and conference and sum up the wins. How many wins by day does each conference get?\ndates &lt;- winlosslogs |&gt; group_by(Date, Conference) |&gt; summarise(wins = sum(winloss))\n\n`summarise()` has grouped output by 'Date'. You can override using the\n`.groups` argument.\nEarlier, we did stacked bar charts. We have what we need to do that now.\nggplot() + geom_bar(data=dates, aes(x=Date, weight=wins, fill=Conference)) + theme_minimal()\nEeek. This is already looking not great. But to make it a circular bar chart, we add coord_polar() to our chart.\nggplot() + geom_bar(data=dates, aes(x=Date, weight=wins, fill=Conference)) + theme_minimal() + coord_polar()\nBased on that, the day is probably too thin a slice, and there’s way too many conferences in college basketball. Let’s group this by months and filter out all but the power five conferences.\np5 &lt;- c(\"SEC WBB\", \"Big Ten WBB\", \"Pac-12 WBB\", \"Big 12 WBB\", \"ACC WBB\")\nTo get months, we’re going to use a function in the library lubridate called floor_date, which combine with mutate will give us a field of just months.\nwins &lt;- winlosslogs |&gt; mutate(month = floor_date(Date, unit=\"months\")) |&gt; group_by(month, Conference) |&gt; summarise(wins=sum(winloss)) |&gt; filter(Conference %in% p5) \n\n`summarise()` has grouped output by 'month'. You can override using the\n`.groups` argument.\nNow we can use wins to make our circular bar chart of wins by month in the Power Five.\nggplot() + geom_bar(data=wins, aes(x=month, weight=wins, fill=Conference)) + theme_minimal() + coord_polar()\nYikes. That looks a lot like a broken pie chart. So months are too thick of a slice. Let’s use weeks in our floor date to see what that gives us.\nwins &lt;- winlosslogs |&gt; mutate(week = floor_date(Date, unit=\"weeks\")) |&gt; group_by(week, Conference) |&gt; summarise(wins=sum(winloss)) |&gt; filter(Conference %in% p5) \n\n`summarise()` has grouped output by 'week'. You can override using the\n`.groups` argument.\nggplot() + geom_bar(data=wins, aes(x=week, weight=wins, fill=Conference)) + theme_minimal() + coord_polar()\nThat looks better. But what does it say? Does November basketball matter? What this is saying is … yeah, it kinda does. The reason? Lots of wins get piled up in November and December, during non-conference play. So if you are a team with NCAA tournament dreams, you need to win games in November to make sure your tournament resume is where it needs to be come March. Does an individual win or loss matter? Probably not. But your record in November does.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Circular bar plots</span>"
    ]
  },
  {
    "objectID": "circularbarcharts.html#does-it-show-you-where-you-are",
    "href": "circularbarcharts.html#does-it-show-you-where-you-are",
    "title": "17  Circular bar plots",
    "section": "17.2 Does it show you where you are?",
    "text": "17.2 Does it show you where you are?\nSo here is the problem we have:\n\nWe have data for every game. In the past, we were able to calculate the team wins and losses because the way the data records them is team is the main team, and they win or lose. The opponent is recorded, but not in its own column of that name. In addition, the opponent has the mirror image of this game as well, where they are team. So essentially every game is in here twice – one for each team that plays in the game.\nWe need to attach the opponent’s winning percentage to each game so we can decide if it’s a quality win for team.\n\nFirst we need to populate opponent based on the whatever is not the team. Then what we have to do is invert the process that we’ve done before. We need to group by the opponent and we need to invert the wins and losses. A win in the win column is a win for the team. That means each loss in the win column is a WIN for the opponent.\nOnce we invert, the data looks very similar to what we’ve done before. One other thing: I noticed there’s some tournament games in here, so the filter at the end strips them out like we did before.\n\noppwinlosslogs &lt;- logs |&gt; \n  mutate(winloss = case_when(\n    grepl(\"W\", W_L) ~ 0, \n    grepl(\"L\", W_L) ~ 1)\n) |&gt; \n  filter(Date &lt; \"2025-03-15\")\n\nSo now we have a dataframe called oppwinlosslogs that has an inverted winloss column. So now we can group by the Opponent and sum the wins and it will tell us how many games the Opponent won. We can also count the wins and get a winning percentage.\n\noppwinlosslogs |&gt; group_by(Opponent) |&gt; summarise(games=n(), wins=sum(winloss)) |&gt; mutate(winpct = wins/games) -&gt; opprecord\n\nNow we have a dataframe of 614 opponent winning records. Wait, what? There’s like ~350 teams in major college basketball, so why 614? If you look through it, there’s a bunch of teams playing lower level teams. Given that they are lower level, they’re likely cannon fodder and will lose the game, and we’re going to filter them out in a minute.\nNow we can join the opponent winning percentage to our winlosslogs data so we can answer our question about quality wins.\n\nwinlosslogs &lt;- logs |&gt; \n  mutate(winloss = case_when(\n    grepl(\"W\", W_L) ~ 1, \n    grepl(\"L\", W_L) ~ 0)\n) |&gt; \n  filter(Date &lt; \"2025-03-15\")\n\n\nwinlosslogs |&gt; left_join(opprecord, by=(\"Opponent\")) -&gt; winswithopppct\n\nNow that we have a table called winswithopppct, we can filter out non-power 5 teams and teams that won less than 60 percent of their games and run the same calculations in the book.\n\np5 &lt;- c(\"SEC WBB\", \"Big Ten WBB\", \"Pac-12 WBB\", \"Big 12 WBB\", \"ACC WBB\")\n\n\nwinswithopppct |&gt; filter(winpct &gt; .6) |&gt; mutate(week = floor_date(Date, unit=\"weeks\")) |&gt; group_by(week, Conference) |&gt; summarise(wins=sum(winloss)) |&gt; filter(Conference %in% p5) -&gt; qualitywins\n\n`summarise()` has grouped output by 'week'. You can override using the\n`.groups` argument.\n\n\nNow with our dataframe called qualitywins, we can chart it again.\n\nggplot() + geom_bar(data=qualitywins, aes(x=week, weight=wins, fill=Conference)) + theme_minimal() + coord_polar()\n\n\n\n\n\n\n\n\nLook at this chart and compare it to the first one.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Circular bar plots</span>"
    ]
  },
  {
    "objectID": "wafflecharts.html",
    "href": "wafflecharts.html",
    "title": "18  Waffle charts",
    "section": "",
    "text": "18.1 Making waffles with vectors\nPie charts are the devil. They should be an instant F in any data visualization class. The problem? How carefully can you evaluate angles and area? Unless they are blindingly obvious and only a few categories, not well. If you’ve got 25 categories, how can you tell the difference between 7 and 9 percent? You can’t.\nSo let’s introduce a better way: The Waffle Chart. Some call it a square pie chart. I personally hate that. Waffles it is.\nA waffle chart is designed to show you parts of the whole – proportionality. How many yards on offense come from rushing or passing. How many singles, doubles, triples and home runs make up a teams hits. How many shots a basketball team takes are two pointers versus three pointers.\nFirst, install the library in the console. We want a newer version of the waffle library than is in CRAN – where you normally get libraries from – so copy and paste this into your console:\ninstall.packages(\"waffle\")\nNow load it:\nLet’s look at Maryland’s football game against Michigan State this season. Here’s the box score, which we’ll use for this part of the walkthrough.\nMaybe the easiest way to do waffle charts, at least at first, is to make vectors of your data and plug them in. To make a vector, we use the c or concatenate function.\nSo let’s look at offense. Net rushing vs passing.\nmd &lt;- c(\"Rushing\"=175, \"Passing\"=314)\nms &lt;- c(\"Rushing\"=100, \"Passing\"=221)\nSo what does the breakdown of the night look like?\nThe waffle library can break this down in a way that’s easier on the eyes than a pie chart. We call the library, add the data, specify the number of rows, give it a title and an x value label, and to clean up a quirk of the library, we’ve got to specify colors.\nwaffle(\n        md, \n        rows = 10, \n        title=\"Maryland's offense\", \n        xlab=\"1 square = 1 yard\", \n        colors = c(\"black\", \"red\")\n)\nOr, we could make this two teams in the same chart.\npassing &lt;- c(\"Maryland\"=314, \"Michigan State\"=221)\nwaffle(\n        passing, \n        rows = 10, \n        title=\"Maryland vs Michigan State: passing\", \n        xlab=\"1 square = 1 yard\", \n        colors = c(\"red\", \"green\")\n)\nSo what does it look like if we compare the two teams using the two vectors in the same chart? To do that – and I am not making this up – you have to create a waffle iron. Get it? Waffle charts? Iron?\niron(\n waffle(md, \n        rows = 10, \n        title=\"Maryland's offense\", \n        xlab=\"1 square = 1 yard\", \n        colors = c(\"black\", \"red\")\n        ),\n waffle(ms, \n        rows = 10, \n        title=\"Michigan State's offense\", \n        xlab=\"1 square = 1 yard\", \n        colors = c(\"black\", \"green\")\n        )\n)\nWhat do you notice about this chart? Notice how the squares aren’t the same size? Well, Maryland out-gained Michigan State by a long way. So the squares aren’t the same size because the numbers aren’t the same. We can fix that by adding an unnamed padding number so the number of yards add up to the same thing. Let’s make the total for everyone be 489, Maryland’s total yards of offense. So to do that, we need to add a padding of 168 to Michigan State. REMEMBER: Don’t name it or it’ll show up in the legend.\nmd &lt;- c(\"Rushing\"=175, \"Passing\"=314)\nms &lt;- c(\"Rushing\"=100, \"Passing\"=221, 168)\nNow, in our waffle iron, if we don’t give that padding a color, we’ll get an error. So we need to make it white. Which, given our white background, means it will disappear.\niron(\n waffle(md, \n        rows = 10, \n        title=\"Maryland's offense\", \n        xlab=\"1 square = 1 yard\", \n        colors = c(\"black\", \"red\")\n        ),\n waffle(ms, \n        rows = 10, \n        title=\"Michigan State's offense\", \n        xlab=\"1 square = 1 yard\",\n        colors = c(\"black\", \"green\", \"white\")\n        )\n)\nOne last thing we can do is change the 1 square = 1 yard bit – which makes the squares really small in this case – by dividing our vector. Look, it’s math on vectors!\niron(\n waffle(md/2, \n        rows = 10, \n        title=\"Maryland's offense\", \n        xlab=\"1 square = 1 yard\", \n        colors = c(\"black\", \"red\")\n        ),\n waffle(ms/2, \n        rows = 10, \n        title=\"Michigan State's offense\", \n        xlab=\"1 square = 1 yard\",\n        colors = c(\"black\", \"green\", \"white\")\n        )\n)\nNews flash: Michigan State is changing its fight song to “Everybody Hurts” by REM.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Waffle charts</span>"
    ]
  },
  {
    "objectID": "linecharts.html",
    "href": "linecharts.html",
    "title": "19  Line charts",
    "section": "",
    "text": "19.1 This is too simple.\nSo far, we’ve talked about bar charts – stacked or otherwise – are good for showing relative size of a thing compared to another thing. Stacked Bars and Waffle charts are good at showing proportions of a whole.\nLine charts are good for showing change over time.\nLet’s look at how we can answer this question: How did Maryland men’s basketball team\nWe’ll need the logs of every game in college basketball for this.\nLet’s start getting all that we need. We can use the tidyverse shortcut.\nAnd now load the data.\nThis data has every game from every team in it, so we need to use filtering to limit it, because we just want to look at Maryland. If you don’t remember, flip back to chapter 6.\nBecause this data has just Maryland data in it, the dates are formatted correctly, and the data is long data (instead of wide), we have what we need to make line charts.\nLine charts, unlike bar charts, do have a y-axis. So in our ggplot step, we have to define what our x and y axes are. In this case, the x axis is our Date – the most common x axis in line charts is going to be a date of some variety – and y in this case is up to us. Three-point shooting has been an increasingly important feature of basketball, so let’s chart that.\nThe problem here is that the Y axis doesn’t start with zero. That makes this look more dramatic than it is. To make the axis what you want, you can use scale_x_continuous or scale_y_continuous and pass in a list with the bottom and top value you want. You do that like this:\nNote also that our X axis labels are automated. It knows it’s a date and it just labels it by month.\nWith datasets, we want to invite comparison. So let’s answer the question visually. Let’s put two lines on the same chart. How does Maryland compare to conference leader Michigan State, for example?\nmsu &lt;- logs |&gt; filter(Team == \"Michigan State\")\nIn this case, because we have two different datasets, we’re going to put everything in the geom instead of the ggplot step. We also have to explicitly state what dataset we’re using by saying data= in the geom step.\nFirst, let’s chart Maryland. Read carefully. First we set the data. Then we set our aesthetic. Unlike bars, we need an X and a Y variable. In this case, our X is the date of the game, Y is the thing we want the lines to move with. In this case, the Team Field Goal Percentage – TeamFGPCT.\nggplot() + geom_line(data=umd, aes(x=Date, y=Team3PPCT), color=\"red\")\nNow, by using +, we can add Michigan State to it. REMEMBER COPY AND PASTE IS A THING. Nothing changes except what data you are using.\nggplot() + \n  geom_line(data=umd, aes(x=Date, y=Team3PPCT), color=\"red\") + \n  geom_line(data=msu, aes(x=Date, y=Team3PPCT), color=\"green\")\nLet’s flatten our lines out by zeroing the Y axis. We’ll set the upper limit of the y-axis to 0.70 because Illinois shot fantastically well in one December game.\nggplot() + \n  geom_line(data=umd, aes(x=Date, y=Team3PPCT), color=\"red\") + \n  geom_line(data=msu, aes(x=Date, y=Team3PPCT), color=\"green\")\n\n\n\n\n\n\n\n  scale_y_continuous(limits = c(0, .70))\n\n&lt;ScaleContinuousPosition&gt;\n Range:  \n Limits:    0 --  0.7\nSo visually speaking, the difference between Maryland and Michigan State’s seasons isn’t that great - they both had some variation in shooting from three, but mostly that smoothed out a bit after the end of 2024.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Line charts</span>"
    ]
  },
  {
    "objectID": "linecharts.html#but-what-if-i-wanted-to-add-a-lot-of-lines.",
    "href": "linecharts.html#but-what-if-i-wanted-to-add-a-lot-of-lines.",
    "title": "19  Line charts",
    "section": "19.2 But what if I wanted to add a lot of lines.",
    "text": "19.2 But what if I wanted to add a lot of lines.\nFine. How about all Power Five Schools? This data for example purposes. You don’t have to do it.\n\npowerfive &lt;- c(\"SEC MBB\", \"Big Ten MBB\", \"Pac-12 MBB\", \"Big 12 MBB\", \"ACC MBB\")\n\np5conf &lt;- logs |&gt; filter(Conference %in% powerfive)\n\nI can keep layering on layers all day if I want. And if my dataset has more than one team in it, I need to use the group command. And, the layering comes in order – so if you’re going to layer a bunch of lines with a smaller group of lines, you want the bunch on the bottom. So to do that, your code stacks from the bottom. The first geom in the code gets rendered first. The second gets layered on top of that. The third gets layered on that and so on.\n\nggplot() + \n  geom_line(data=p5conf, aes(x=Date, y=Team3PPCT, group=Team), color=\"grey\") + \n  geom_line(data=umd, aes(x=Date, y=Team3PPCT), color=\"red\") + \n  geom_line(data=msu, aes(x=Date, y=Team3PPCT), color=\"green\")\n\n\n\n\n\n\n\n  scale_y_continuous(limits = c(0, .65))\n\n&lt;ScaleContinuousPosition&gt;\n Range:  \n Limits:    0 -- 0.65\n\n\nWhat do we see here? How have Maryland’s and Michigan State’s seasons evolved against all the rest of the teams in major college basketball?\nBut how does that compare to the average? We can add that pretty easily by creating a new dataframe with it and add another geom_line.\n\naverage &lt;- logs |&gt; group_by(Date) |&gt; summarise(mean_shooting=mean(Team3PPCT))\n\n\nggplot() + \n  geom_line(data=p5conf, aes(x=Date, y=Team3PPCT, group=Team), color=\"grey\") + \n  geom_line(data=umd, aes(x=Date, y=Team3PPCT), color=\"red\") + \n  geom_line(data=msu, aes(x=Date, y=Team3PPCT), color=\"green\") +\n  geom_line(data=average, aes(x=Date, y=mean_shooting), color=\"black\") + \n  scale_y_continuous(limits = c(0, .70))",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Line charts</span>"
    ]
  },
  {
    "objectID": "stepcharts.html",
    "href": "stepcharts.html",
    "title": "20  Step charts",
    "section": "",
    "text": "Step charts are a method of showing progress toward something. They combine showing change over time – cumulative change over time – with magnitude. They’re good at inviting comparison.\nThere’s great examples out there. First is the Washington Post looking at Lebron passing Jordan’s career point total. Another is John Burn-Murdoch’s work at the Financial Times (which is paywalled) about soccer stars. Here’s an example of his work outside the paywall.\nTo replicate this, we need cumulative data – data that is the running total of data at a given point. So think of it this way – Maryland scores 50 points in a basketball game and then 50 more the next, their cumulative total at two games is 100 points.\nStep charts can be used for all kinds of things – showing how a player’s career has evolved over time, how a team fares over a season, or franchise history. Let’s walk through an example.\nLet’s look at Maryland’s women basketball team last season.\nFor this walkthrough:\n   Download csv file\n\nWe’ll need the tidyverse.\n\nlibrary(tidyverse)\n\nAnd we need to load our logs data we just downloaded.\n\nlogs &lt;- read_csv(\"data/wbblogs25.csv\")\n\nRows: 11467 Columns: 60\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (10): Season, GameType, TeamFullName, Opponent, HomeAway, W_L, OT, URL,...\ndbl  (49): Game, TeamScore, OpponentScore, TeamFG, TeamFGA, TeamFGPCT, Team3...\ndate  (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nHere we’re going to look at the scoring differential of teams. If you score more than your opponent, you win. So it stands to reason that if you score a lot more than your opponent over the course of a season, you should be very good, right? Let’s see.\nThe first thing we’re going to do is calculate that differential. Then, we’ll group it by the team. After that, we’re going to summarize using a new function called cumsum or cumulative sum – the sum for each game as we go forward. So game 1’s cumsum is the differential of that game. Game 2’s cumsum is Game 1 + Game 2. Game 3 is Game 1 + 2 + 3 and so on.\n\ndifflogs &lt;- logs |&gt; \n  mutate(Differential = TeamScore - OpponentScore) |&gt; \n  group_by(TeamFullName) |&gt; \n  mutate(CumDiff = cumsum(Differential))\n\nNow that we have the cumulative sum for each, let’s filter it down to just Big Ten teams.\n\nbigdiff &lt;- difflogs |&gt; filter(Conference == \"Big Ten WBB\")\n\nThe step chart is it’s own geom, so we can employ it just like we have the others. It works almost exactly the same as a line chart, but it uses the cumulative sum instead of a regular value and, as the name implies, creates a step like shape to the line instead of a curve.\n\nggplot() + geom_step(data=bigdiff, aes(x=Date, y=CumDiff, group=Team))\n\n\n\n\n\n\n\n\nLet’s try a different element of the aesthetic: color, but this time inside the aesthetic. Last time, we did the color outside. When you put it inside, you pass it a column name and ggplot will color each line based on what thing that is, and it will create a legend that labels each line that thing.\n\nggplot() + geom_step(data=bigdiff, aes(x=Date, y=CumDiff, group=Team, color=Team))\n\n\n\n\n\n\n\n\nFrom this, we can see a handful of teams in the Big Ten had negative point differentials last season. But which is which? And which one is Maryland? Too many colors and it’s too hard to tell. How to sort that out? Let’s add some helpers beyond layering.\nLet’s look at Maryland, plus another team: Illinois\n\numd &lt;- bigdiff |&gt; filter(Team == \"Maryland\")\nill &lt;- bigdiff |&gt; filter(Team == \"Illinois\")\n\nLet’s introduce a couple of new things here. First, note when I take the color OUT of the aesthetic, the legend disappears.\nThe second thing I’m going to add is the annotation layer. In this case, I am adding a text annotation layer, and I can specify where by adding in a x and a y value where I want to put it. This takes some finesse. After that, I’m going to add labels and a theme.\n\nggplot() + \n  geom_step(data=bigdiff, aes(x=Date, y=CumDiff, group=Team), color=\"light grey\") +\n  geom_step(data=umd, aes(x=Date, y=CumDiff, group=Team), color=\"red\") + \n  geom_step(data=ill, aes(x=Date, y=CumDiff, group=Team), color=\"orange\") +\n  annotate(\"text\", x=(as.Date(\"2024-12-10\")), y=300, label=\"Maryland\") +\n  annotate(\"text\", x=(as.Date(\"2025-02-01\")), y=175, label=\"Illinois\") +\n  labs(\n    x=\"Date\", \n    y=\"Cumulative Point Differential\", \n    title=\"Maryland and Illinois Had Similar Seasons\", \n    subtitle=\"The Terps were middle of the pack in cumulative point differential.\", \n    caption=\"Source: Sports-Reference.com | By Derek Willis\") +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Step charts</span>"
    ]
  },
  {
    "objectID": "dumbbellcharts.html",
    "href": "dumbbellcharts.html",
    "title": "21  Dumbbell and lollipop charts",
    "section": "",
    "text": "21.1 Dumbbell plots\nSecond to my love of waffle charts because I’m always hungry, dumbbell charts are an excellently named way of showing the difference between two things on a number line – a start and a finish, for instance. Or the difference between two related things. Say, turnovers and assists.\nLollipop charts – another excellent name – are a variation on bar charts. They do a good job of showing magnitude and difference between things.\nTo use both of them, you need to add a new library:\ninstall.packages(\"ggalt\")\nLet’s give it a whirl.\nFor this, let’s use college volleyball game logs from this season.\nAnd load it.\nlogs &lt;- read_csv(\"data/ncaa_womens_volleyball_matchstats_2024.csv\")\n\nRows: 10149 Columns: 40\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (4): team, opponent, home_away, result\ndbl  (35): ncaa_id, team_score, opponent_score, s.x, kills, errors, total_at...\ndate  (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nFor the first example, let’s look at the difference between a team’s errors versus their opponents’ errors. To get this, we’re going to add up all errors and opponent errors for a team in a season and take a look at where they come out. To make this readable, I’m going to focus on the Big Ten.\nbig10 &lt;- c(\"Nebraska\", \"Iowa\", \"Minnesota\", \"Illinois\", \"Northwestern\", \"Wisconsin\", \"Indiana\", \"Purdue\", \"Ohio St.\", \"Michigan\", \"Michigan St.\", \"Penn St.\", \"Rutgers\", \"Maryland\", \"Southern California\", \"UCLA\", \"Washington\", \"Oregon\")\n\nerrors &lt;- logs |&gt;\n  filter(team %in% big10) |&gt; \n  group_by(team) |&gt; \n  summarise(\n    total_errors = sum(errors), \n    opp_errors = sum(defensive_errors))\nNow, the way that the geom_dumbbell works is pretty simple when viewed through what we’ve done before. There’s just some tweaks.\nFirst: We start with the y axis. The reason is we want our dumbbells going left and right, so the label is going to be on the y axis.\nSecond: Our x is actually two things: x and xend. What you put in there will decide where on the line the dot appears.\nggplot() + \n  geom_dumbbell(\n    data=errors, \n    aes(y=team, x=total_errors, xend=opp_errors)\n  )\n\nWarning: Using the `size` aesthetic with geom_segment was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\nWell, that’s a chart alright, but what dot is the team errors and what are the opponent errors? To fix this, we’ll add colors.\nSo our choice of colors here is important. We want team errors to be seen as bad and opponent errors to be seen as good. So lets try red for team errors and green for opponent errors To make this work, we’ll need to do three things: first, use the English spelling of color, so colour. The, uh, colour is the bar between the dots, the x_colour is the color of the x value dot and the xend_colour is the color of the xend dot. So in our setup, defensive errors are x, they’re good, so they’re green.\nggplot() + \n  geom_dumbbell(\n    data=errors, \n    aes(y=team, x=opp_errors, xend=total_errors),\n    colour = \"grey\",\n    colour_x = \"green\",\n    colour_xend = \"red\")\nBetter. Let’s make two more tweaks. First, let’s make the lines bigger with a linewidth element. And let’s add theme_minimal to clean out some cruft.\nggplot() + \n  geom_dumbbell(\n    data=errors, \n    aes(y=team, x=opp_errors, xend=total_errors),\n    linewidth = 1,\n    color = \"grey\",\n    colour_x = \"green\",\n    colour_xend = \"red\") + \n  theme_minimal()\n\nWarning in geom_dumbbell(data = errors, aes(y = team, x = opp_errors, xend =\ntotal_errors), : Ignoring unknown parameters: `linewidth`\nAnd now we have a chart that tells a story – got green on the right? That’s good. A long distance between green and red? Better. But what if we sort it by good errors?\nggplot() + \n  geom_dumbbell(\n    data=errors, \n    aes(y=reorder(team, opp_errors), x=opp_errors, xend=total_errors),\n    linewidth = 1,\n    color = \"grey\",\n    colour_x = \"green\",\n    colour_xend = \"red\") + \n  theme_minimal()\n\nWarning in geom_dumbbell(data = errors, aes(y = reorder(team, opp_errors), :\nIgnoring unknown parameters: `linewidth`\nMaryland’s opponents have committed some errors - that’s good news for the Terps - but there’s a clear gap between Maryland and the top teams in the conference when it comes to committing errors. And the gap between “good” errors and “bad” errors for the Terps is tiny.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Dumbbell and lollipop charts</span>"
    ]
  },
  {
    "objectID": "dumbbellcharts.html#dumbbell-plots",
    "href": "dumbbellcharts.html#dumbbell-plots",
    "title": "21  Dumbbell and lollipop charts",
    "section": "",
    "text": "For this walkthrough:\n   Download csv file",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Dumbbell and lollipop charts</span>"
    ]
  },
  {
    "objectID": "dumbbellcharts.html#lollipop-charts",
    "href": "dumbbellcharts.html#lollipop-charts",
    "title": "21  Dumbbell and lollipop charts",
    "section": "21.2 Lollipop charts",
    "text": "21.2 Lollipop charts\nSticking with takeaways, lollipops are similar to bar charts in that they show magnitude. And like dumbbells, they are similar in that we start with a y – the traditional lollipop chart is on its side – and we only need one x. The only additional thing we need to add is that we need to tell it that it is a horizontal chart.\n\nggplot() + \n  geom_lollipop(\n    data=errors, \n    aes(y=team, x=opp_errors), \n    horizontal = TRUE\n    )\n\n\n\n\n\n\n\n\nWe can do better than this with a simple theme_minimal and some better labels.\n\nggplot() + \n  geom_lollipop(\n    data=errors, \n    aes(y=reorder(team, opp_errors), x=opp_errors), \n    horizontal = TRUE\n    ) + theme_minimal() + \n  labs(title = \"Nebraska, Penn State force more errors\", y=\"team\")\n\n\n\n\n\n\n\n\nHow about some layering?\n\nmd &lt;- errors |&gt; filter(team == \"Maryland\")\n\n\nggplot() + \n  geom_lollipop(\n    data=errors, \n    aes(y=reorder(team, opp_errors), x=opp_errors), \n    horizontal = TRUE\n    ) + \n  geom_lollipop(\n    data=md,\n    aes(y=team, x=opp_errors),\n    horizontal = TRUE,\n    color = \"red\"\n  ) + \n  theme_minimal() + \n  labs(title = \"Maryland near the bottom of Big Ten teams\", y=\"team\")\n\n\n\n\n\n\n\n\nThe headline says it all.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Dumbbell and lollipop charts</span>"
    ]
  },
  {
    "objectID": "scatterplots.html",
    "href": "scatterplots.html",
    "title": "22  Scatterplots",
    "section": "",
    "text": "22.1 Let’s see it fail\nWith the exception of those curmudgeons who love defense, everybody loves scoring. We enjoy blowout games (when our teams win) and bemoan low-scoring affairs. But does how you score matter? Does it make a difference if you rely on a single star to go one-on-one versus if you move the ball around? While scoring is a necessary condition for winning, it’s not the only one. Plenty of bad teams score a lot - they just happen to give up more runs or points or goals.\nSo how do we tell if good passing makes a difference? How can we test the ingredients of success and determine what’s a significant factor and what’s a hot take?\nThis is what we’re going to start to answer today. And we’ll do it with scatterplots and regressions. Scatterplots are very good at showing relationships between two numbers.\nTo demonstrate this, we’ll look at women’s college lacrosse from the 2025 season, and we’ll see how assists and wins are related.\nFirst, we need libraries and every college lacrosse game from the last season. What we’re interested in is less about a specific team and more about a general point: Are these numbers related and by how much? What can they tell you about your team in general?\nLoad the tidyverse.\nAnd the data.\nWe’ve got columns for overall_wins and assists. Now let’s look at the scatterplot. With a scatterplot, we put what predicts the thing on the X axis, and the thing being predicted on the Y axis. In this case, X is our assists, y is our wins.\nLet’s talk about this. Ok, there’s really a clear pattern here - as assists increase, so do wins, generally. But can we get a better sense of this? Yes, by adding another geom – geom_smooth. It’s identical to our geom_point, but we add a method to the end, which in this case we’re using the linear method or lm.\nA line climbing from left to right is good. It means there’s a solid positive relationship here. The numbers don’t suggest anything. Still, it’s worth asking: can we know exactly how strong of a relationship is this? How much can assists scored explain wins? Can we put some numbers to this?\nOf course we can. We can apply a linear model to this – remember Chapter 9? We’re going to create an object called fit, and then we’re going to put into that object a linear model – lm – and the way to read this is “wins are predicted by opponent threes”. Then we just want the summary of that model.\nRemember from Chapter 9: There’s just a few things you really need.\nThe first thing: R-squared. In this case, the Adjusted R-squared value is 0.7257, which we can interpret as the number of assists a team records predicts about 72 percent of the variance in wins. Pretty good!\nSecond: The P-value. We want anything less than .05. If it’s above .05, the difference between them is not statistically significant – it’s probably explained by random chance. In our case, we have 0.00000000000000022, so this isn’t random chance. Which makes sense, because it’s harder to win when you don’t score.\nNormally, we’d stop here, but let’s look at the third element: The coefficient. In this case, the coefficient for assists is 0.103026. What this model predicts, given that and the intercept of -1.141803, is this: For every assist you tally, you add about .10 towards your wins total. So if you have 50 assists in a season, you’ll be a 5-win team. Notch 150, you’re closer to a 15-win team, and so on. How am I doing that? Remember your algebra and y = mx + b. In this case, y is the wins, m is the coefficient, x is the number of goals and b is the intercept.\nLet’s use Maryland as an example. They recorded 118 assists last season.\ny = 0.103026 * 118 + -1.141803 or 11 wins\nHow many wins did Maryland have? 15.\nWhat does that mean? It means that Maryland over-performed, according to this model. Seems like assists is an ok predictor for Maryland, but maybe not a great one. Perhaps the Terps relied on one-on-one scoring opportunities instead of passing. Where is Maryland on the plot? We know we can use layering for that.\nMaryland’s not the most interesting team on this plot, though. Who is?\nScatterplots also are useful for shooting down the hottest of takes. What about green cards? Are they related to wins in lacrosse?\nggplot() + \n  geom_point(data=teams, aes(x=green_cards, y=overall_wins)) +\n  geom_smooth(data=teams, aes(x=green_cards, y=overall_wins), method=\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\nThis is basically a flat line and a lot of dots spread all over the plot. There appears to be absolutely no relationship, but let’s test that out just to be sure.\nLet’s get our linear regression stats.\ngcfit &lt;- lm(overall_wins ~ green_cards, data = teams)\nsummary(gcfit)\n\n\nCall:\nlm(formula = overall_wins ~ green_cards, data = teams)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.6510 -2.6487 -0.6428  2.3527 13.3531 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 8.6319063  1.1930828   7.235 3.69e-11 ***\ngreen_cards 0.0004536  0.0303506   0.015    0.988    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.241 on 129 degrees of freedom\nMultiple R-squared:  1.731e-06, Adjusted R-squared:  -0.00775 \nF-statistic: 0.0002233 on 1 and 129 DF,  p-value: 0.9881\nThe p-value is way above 0.05, so the impact of green cards to wins (or losses) could just be random. The adjusted R-squared is all of -0.00775 percent. We’ve do not have something here. Let’s use our coefficients to look at Maryland’s 2024-25 season.\n(0.0004536 * 31) + 8.6319063\n\n[1] 8.645968\nThis model says that based only on Maryland’s green cards, they should have won 8, maybe 9 games. They won 15. Not a great model for many teams.\nThe power in combining scatterplots + regression is that we can answer two questions: is there a relationship, and how meaningful is it?",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Scatterplots</span>"
    ]
  },
  {
    "objectID": "bubblecharts.html",
    "href": "bubblecharts.html",
    "title": "23  Bubble charts",
    "section": "",
    "text": "Here is the real talk: Bubble charts are hard. The reason they are hard is not because of the code, or the complexity or anything like that. They’re a scatterplot with magnitude added – the size of the dot in the scatterplot has meaning. The hard part is seeing when a bubble chart works and when it doesn’t.\nIf you want to see it work spectacularly well, watch a semi-famous Ted Talk by Hans Rosling from 2006 where bubble charts were the centerpiece. It’s worth watching. It’ll change your perspective on the world. No seriously. It will.\nAnd since then, people have wanted bubble charts. And we’re back to the original problem: They’re hard. There’s a finite set of circumstances where they work.\nHere’s one: placing players or teams into groups based on performance.\nI’m going to load up my libraries.\n\nlibrary(tidyverse)\n\nSo for this example, I want to look at where Big Ten teams compare to the rest of college football last season. Is the Big Ten’s reputation for tough games and defenses earned? Can we see patterns in good team vs bad teams?\nI’m going to create a scatterplot with offensive yards per play on the X axis and defensive yards per play on the y axis. We can then divide the grid into four quadrants. Teams with high yards per offensive play and low defensive yards per play are teams with good offenses and good defenses. The opposite means bad defense, bad offense. Then, to drive the point home, I’m going to make the dot the size of the total wins on the season – the bubble in my bubble charts.\nWe’ll use last season’s college football games.\nFor this walkthrough:\n   Download csv file\n\nAnd load it.\n\nlogs &lt;- read_csv(\"data/footballlogs24.csv\")\n\nRows: 1344 Columns: 54\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (8): HomeAway, Opponent, Result, TeamFull, TeamURL, Outcome, Team, Con...\ndbl  (45): Game, PassingCmp, PassingAtt, PassingPct, PassingYds, PassingTD, ...\ndate  (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nTo do this, I’ve got some work to do. First, I need to mutate the outcomes of the games into 1s and 0s so I can add up the wins. We’ve done this before, so this won’t be new to you, just adjusted slightly from basketball data.\n\nwinlosslogs &lt;- logs |&gt; \n  mutate(\n    wins = case_when(\n      grepl(\"W\", Outcome) ~ 1, \n      grepl(\"L\", Outcome) ~ 0)\n)\n\nNow I have some more work to do. My football logs data has the yards per play of each game, and I could average those together and get something very close to what I’m going to do, but averaging each games yards per play is not the same thing as calculating it, so we’re going to calculate it.\n\nwinlosslogs |&gt; \n  group_by(Team, Conference) |&gt; \n  summarise(\n    TotalPlays = sum(OffensivePlays), \n    TotalYards = sum(OffensiveYards), \n    DefensivePlays = sum(DefPlays), \n    DefensiveYards = sum(DefYards), \n    TotalWins = sum(wins)) |&gt; \n  mutate(\n    OffensiveYPP = TotalYards/TotalPlays, \n    DefensiveYPP = DefensiveYards/DefensivePlays) -&gt; ypp\n\n`summarise()` has grouped output by 'Team'. You can override using the\n`.groups` argument.\n\n\nA bubble chart is just a scatterplot with one additional element in the aesthetic – a size. Here’s the scatterplot version.\n\nggplot() + \n  geom_point(\n    data=ypp, aes(x=OffensiveYPP, y=DefensiveYPP)\n    )\n\n\n\n\n\n\n\n\nLooks kind of random, eh? In this case, that’s not that bad because we’re not claiming a relationship. We’re saying the location on the chart has meaning. So, do teams on the bottom right – good offense, good defense – win more games?\nLet’s add the size element.\n\nggplot() + \n  geom_point(\n    data=ypp, \n    aes(x=OffensiveYPP, y=DefensiveYPP, size=TotalWins)\n    )\n\n\n\n\n\n\n\n\nWhat does this chart tell you? We can see a general pattern that there are more big dots on the bottom right than the upper left. But we can make this more readable by adding an alpha element outside the aesthetic – alpha in this case is transparency – and we can manually change the size of the dots by adding scale_size and a range.\n\nggplot() + \n  geom_point(\n    data=ypp, \n    aes(x=OffensiveYPP, y=DefensiveYPP, size=TotalWins),\n    alpha = .3) + \n  scale_size(range = c(3, 8), name=\"Wins\")\n\n\n\n\n\n\n\n\nAnd by now, you now know to add in the Big Ten as a layer, I would hope.\n\nbigten &lt;- ypp |&gt; filter(Conference == \"Big Ten Conference\")\n\n\nggplot() + \n  geom_point(\n    data=ypp, \n    aes(x=OffensiveYPP, y=DefensiveYPP, size=TotalWins), \n    color=\"grey\", \n    alpha=.5) + \n  geom_point(\n    data=bigten, \n    aes(x=OffensiveYPP, y=DefensiveYPP, size=TotalWins), \n    color=\"red\")\n\n\n\n\n\n\n\n\nLet’s add some things to this chart to help us out. First, let’s add lines that show us the average of all teams for those two metrics. So first, we need to calculate those. Because I have grouped data, it’s going to require me to ungroup it so I can get just the total average of those two numbers.\n\nypp |&gt; \n  ungroup() |&gt; \n  summarise(\n    offense = mean(OffensiveYPP), \n    defense = mean(DefensiveYPP)\n    )\n\n# A tibble: 1 × 2\n  offense defense\n    &lt;dbl&gt;   &lt;dbl&gt;\n1    5.81    5.56\n\n\nNow we can use those averages to add two more geoms – geom_vline and geom_hline, for vertical lines and horizontal lines.\n\nggplot() + \n  geom_point(\n    data=ypp, \n    aes(x=OffensiveYPP, y=DefensiveYPP, size=TotalWins), \n    color=\"grey\", \n    alpha=.5) + \n  geom_point(\n    data=bigten, \n    aes(x=OffensiveYPP, y=DefensiveYPP, size=TotalWins), \n    color=\"red\") + \n  geom_vline(xintercept = 5.811255) + \n  geom_hline(yintercept = 5.562701)\n\n\n\n\n\n\n\n\nNow, let’s add another new geom for us, using a new library called ggrepel, which will help us label the dots without overwriting other labels. So we’ll have to install that in the console:\n`install.packages(“ggrepel”)\n\nlibrary(ggrepel)\n\nAnd with that, we can add labels to the dots. The geom_text_repel is pretty much the exact same thing as your Big Ten geom point, but instead of a size, you include a label.\n\nggplot() + \n  geom_point(\n    data=ypp, \n    aes(x=OffensiveYPP, y=DefensiveYPP, size=TotalWins), \n    color=\"grey\", \n    alpha=.5) + \n  geom_point(\n    data=bigten, \n    aes(x=OffensiveYPP, y=DefensiveYPP, size=TotalWins), \n    color=\"red\") + \n  geom_vline(xintercept = 5.811255) + \n  geom_hline(yintercept = 5.562701) +\n  geom_text_repel(\n    data=bigten, \n    aes(x=OffensiveYPP, y=DefensiveYPP, label=Team)\n  )\n\n\n\n\n\n\n\n\nWell, what do you know about that? Maryland was … in the “worst” quadrant last season.\nAll that’s left is some labels and some finishing touches.\n\nggplot() + \n  geom_point(\n    data=ypp, \n    aes(x=OffensiveYPP, y=DefensiveYPP, size=TotalWins), \n    color=\"grey\", \n    alpha=.5) + \n  geom_point(\n    data=bigten, \n    aes(x=OffensiveYPP, y=DefensiveYPP, size=TotalWins), \n    color=\"red\") + \n  geom_vline(xintercept = 5.811255) + \n  geom_hline(yintercept = 5.562701) +\n  geom_text_repel(\n    data=bigten, \n    aes(x=OffensiveYPP, y=DefensiveYPP, label=Team)\n  ) +\n  labs(title=\"A Step Back for Maryland\", subtitle=\"The Terps offense and defense puts it among the worst teams in the conference.\", caption=\"Source: NCAA | By Derek Willis\")  + theme_minimal() + \n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 8), \n    plot.subtitle = element_text(size=10), \n    panel.grid.minor = element_blank()\n    )",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Bubble charts</span>"
    ]
  },
  {
    "objectID": "beeswarmplots.html",
    "href": "beeswarmplots.html",
    "title": "24  Beeswarm plots",
    "section": "",
    "text": "24.1 A few other options\nA beeswarm plot is sometimes called a column scatterplot. It’s an effective way to show how individual things – teams, players, etc. – are distributed along a numberline. The column is a grouping – say positions in basketball – and the dots are players, and the dots cluster where the numbers are more common. So think of it like a histogram mixed with a scatterplot crossed with a bar chart.\nAn example will help.\nFirst things first: Install ggbeeswarm with install.packages(\"ggbeeswarm\")\nLike ggalt and ggrepel, ggbeeswarm adds a couple new geoms to ggplot. We’ll need to load it, the tidyverse and, for later, ggrepel.\nAnother bit of setup: we need to set the seed for the random number generator. The library “jitters” the dots in the beeswarm randomly. If we don’t set the seed, we’ll get different results each time. Setting the seed means we get the same look.\nSo let’s look at last year’s women’s basketball team as a group of shooters. Maryland didn’t really have someone like Diamond Miller or Abby Meyers on the team, and some of the better scorers have graduated. Who are the shooters-in-waiting?\nFirst we’ll load our player data.\nWe know this data has a lot of players who didn’t play, so let’s get rid of them.\nIf we include all players, we’ll have too many dots. So let’s narrow it down. A decent tool for cutoffs? Field goal attempts. Let’s get a quick look at them.\nThe median number of shots is 112, but we only really care about prolific ones. So let’s use 222 attempts – the third quartile – as our cutoff.\nNow we’ve got enough for a beeswarm plot. It works very much like you would expect – the group value is the x, the number is the y. We’re going to beeswarm by position, and the dots will be true shooting percentage (ts_percent in the data):\nYou can see that there’s a lot fewer centers who have attempted more than 222 shots than guards, but then there’s a lot more guards in college basketball than anything else. In the guards column, note that fat width of the swarm is between .5 and .6. So that means most guards who shoot more than 222 shots end up in that area. They’re the average shooter at that level. You can see, some are better, some are worse.\nSo where are the Maryland players in that mix?\nWe’ll filter players on Maryland who meet our criteria.\nFive Terps took more than 222 shots. Number not on the roster this season? Three: Sarah Te-Biasu, Shyanne Sellers and Christina Dalce.\nBut how good are they as true shooters? Let’s add them to the graphic:\nWho are they?\nThis is where we can use ggrepel. Let’s add a text layer and label the dots.\nSo Sarah Te-Biasu was our best shooter by true shooting percentage, with Shyanne Sellers close behind. The other three were at or above average shooters for that volume of shooting.\nThe ggbeeswarm library has a couple of variations on the geom_beeswarm that may work better for your application. They are geom_quasirandom and geom_jitter.\nThere’s not a lot to change from our example to see what they do.\nggplot() + \n  geom_quasirandom(\n    data=shooters, \n    aes(x=pos, y=ts_percent), color=\"grey\") + \n  geom_quasirandom(\n    data=umd, \n    aes(x=pos, y=ts_percent), color=\"red\") +\n  geom_text_repel(\n    data=umd, \n    aes(x=pos, y=ts_percent, label=player))\nQuasirandom spreads out the dots you see in beeswarm using – you guessed it – quasirandom spacing.\nggplot() + \n  geom_jitter(\n    data=shooters, \n    aes(x=pos, y=ts_percent), color=\"grey\") + \n  geom_jitter(\n    data=umd, \n    aes(x=pos, y=ts_percent), color=\"red\") +\n  geom_text_repel(\n    data=umd, \n    aes(x=pos, y=ts_percent, label=player))\ngeom_jitter spreads out the dots evenly across the width of the column, randomly deciding where in the line of the true shooting percentage they appear.\nWhich one is right for you? You’re going to have to experiment and decide. This is the art in the art and a science.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Beeswarm plots</span>"
    ]
  },
  {
    "objectID": "bumpcharts.html",
    "href": "bumpcharts.html",
    "title": "25  Bump charts",
    "section": "",
    "text": "The point of a bump chart is to show how the ranking of something changed over time – you could do this with the top 25 in football or basketball. I’ve seen it done with European soccer league standings over a season.\nThe requirements are that you have a row of data for a team, in that week, with their rank.\nThis is another extension to ggplot, and you’ll install it the usual way: install.packages(\"ggbump\")\n\nlibrary(tidyverse)\nlibrary(ggbump)\n\nLet’s use the 2024-25 college football playoff rankings:\nFor this walkthrough:\n   Download csv file\n\n\nrankings &lt;- read_csv(\"data/cfbranking24.csv\")\n\nRows: 150 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Team\ndbl (2): Week, Rank\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nGiven our requirements of a row of data for a team, in that week, with their rank, take a look at the data provided. We have 5 weeks of playoff rankings, so we should see a ranking, the week of the ranking and the team at that rank. You can see the basic look of the data by using head()\n\nhead(rankings)\n\n# A tibble: 6 × 3\n   Week  Rank Team      \n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;     \n1    10     1 Oregon    \n2    10     2 Ohio State\n3    10     3 Georgia   \n4    10     4 Miami     \n5    10     5 Texas     \n6    10     6 Penn State\n\n\nSo Oregon was ranked in the first (yawn), followed by Ohio State (double yawn), Georgia (of course), Miami (!) and so on. Our data is in the form we need it to be. Now we can make a bump chart. We’ll start simple.\n\nggplot() + \n  geom_bump(\n    data=rankings, aes(x=Week, y=Rank, color=Team))\n\nWarning in compute_group(...): 'StatBump' needs at least two observations per\ngroup\nWarning in compute_group(...): 'StatBump' needs at least two observations per\ngroup\n\n\n\n\n\n\n\n\n\nWell, it’s a start.\nThe warning that you’re seeing is that there are two teams last season that made one appearance in the college football playoff rankings and disappeared. Some fans would bite your arm off for that. Alas. We should eliminate them and thin up our chart a little. Let’s just take teams that finished in the top 10. We’re going to use a neat filter trick for this that you learned earlier using %in%.\n\ntop10 &lt;- rankings |&gt; filter(Week == 15 & Rank &lt;= 10)\n\nnewrankings &lt;- rankings |&gt; filter(Team %in% top10$Team)\n\nNow you have something called newrankings that shows how teams who finished in the top 10 at the end of the season ended up there. And every team who finished in the top 10 in week 17 had been in the rankings more than once in the 5 weeks before.\n\nggplot() + \n  geom_bump(\n    data=newrankings, aes(x=Week, y=Rank, color=Team))\n\n\n\n\n\n\n\n\nFirst things first: I’m immediately annoyed by the top teams being at the bottom. I learned a neat trick from ggbump that’s been in ggplot all along – scale_y_reverse()\n\nggplot() + \n  geom_bump(\n    data=newrankings, aes(x=Week, y=Rank, color=Team)) + \n  scale_y_reverse()\n\n\n\n\n\n\n\n\nBetter. But, still not great. Let’s add a point at each week.\n\nggplot() + \n  geom_bump(data=newrankings, aes(x=Week, y=Rank, color=Team)) + \n  geom_point(data=newrankings, aes(x=Week, y=Rank, color=Team), size = 4) +\n  scale_y_reverse() \n\n\n\n\n\n\n\n\nAnother step. That makes it more subway-map like. But the colors are all wrong. To fix this, we’re going to use scale_color_manual and we’re going to Google the hex codes for each team. The legend will tell you what order your scale_color_manual needs to be.\n\nggplot() + \n  geom_bump(data=newrankings, aes(x=Week, y=Rank, color=Team)) + \n  geom_point(data=newrankings, aes(x=Week, y=Rank, color=Team), size = 4) + \n  scale_color_manual(values = c(\"HEX CODE\", \"HEX CODE\")) + # replace HEX CODE with each team's code\n  scale_y_reverse() \n\nAnother step. But the legend is annoying. And trying to find which red is Alabama vs Ohio State is hard. So what if we labeled each dot at the beginning and end? We can do that with some clever usage of geom_text and a little dplyr filtering inside the data step. We filter out the first and last weeks, then use hjust – horizontal justification – to move them left or right.\n\nggplot() + \n  geom_bump(data=newrankings, aes(x=Week, y=Rank, color=Team)) + \n  geom_point(data=newrankings, aes(x=Week, y=Rank, color=Team), size = 4) +   \n  geom_text(data = newrankings |&gt; filter(Week == min(Week)), aes(x = Week - .2, y=Rank, label = Team), size = 3, hjust = 1) +\n  geom_text(data = newrankings |&gt; filter(Week == max(Week)), aes(x = Week + .2, y=Rank, label = Team), size = 3, hjust = 0) +\n  scale_color_manual(values = c(\"HEX CODE\", \"HEX CODE\")) # replace HEX CODE with each team's code) +\n  scale_y_reverse() \n\nBetter, but the legend is still there. We can drop it in a theme directive by saying legend.position = \"none\". We’ll also throw a theme_minimal on there to drop the default grey, and we’ll add some better labeling.\n\nggplot() + \n  geom_bump(data=newrankings, aes(x=Week, y=Rank, color=Team)) + \n  geom_point(data=newrankings, aes(x=Week, y=Rank, color=Team), size = 4) +   \n  geom_text(data = newrankings |&gt; filter(Week == min(Week)), aes(x = Week - .2, y=Rank, label = Team), size = 3, hjust = 1) +\n  geom_text(data = newrankings |&gt; filter(Week == max(Week)), aes(x = Week + .2, y=Rank, label = Team), size = 3, hjust = 0) +\n  labs(title=\"The top ten was anything but boring in 2024-25\", subtitle=\"\", y= \"Rank\", x = \"Week\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"none\",\n    panel.grid.major = element_blank()\n    ) +\n  scale_color_manual(values = c(\"HEX CODE\", \"HEX CODE\")) # replace HEX CODE with each team's code) +\n  scale_y_reverse() \n\nNow let’s fix our text hierarchy.\n\nggplot() + \n  geom_bump(data=newrankings, aes(x=Week, y=Rank, color=Team)) + \n  geom_point(data=newrankings, aes(x=Week, y=Rank, color=Team), size = 4) +   \n  geom_text(data = newrankings |&gt; filter(Week == min(Week)), aes(x = Week - .2, y=Rank, label = Team), size = 3, hjust = 1) +\n  geom_text(data = newrankings |&gt; filter(Week == max(Week)), aes(x = Week + .2, y=Rank, label = Team), size = 3, hjust = 0) +\n  labs(title=\"The top ten was anything but boring in 2024-25\", subtitle=\"\", y= \"Rank\", x = \"Week\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"none\",\n    panel.grid.major = element_blank(),\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 8), \n    plot.subtitle = element_text(size=10), \n    panel.grid.minor = element_blank()\n    ) +\n  scale_color_manual(values = c(\"HEX CODE\", \"HEX CODE\")) # replace HEX CODE with each team's code) +\n  scale_y_reverse() \n\nAnd the last thing: anyone else annoyed at 7.5th place on the left? We can fix that too by specifying the breaks in scale_y_reverse. We can do that with the x axis as well, but since we haven’t reversed it, we do that in scale_x_continuous with the same breaks. Also: forgot my source and credit line.\nOne last thing: Let’s change the width of the chart to make the names fit. We can do that by adding fig.width=X in the {r} setup in your block. So something like this:\n\nggplot() + \n  geom_bump(data=newrankings, aes(x=Week, y=Rank, color=Team)) + \n  geom_point(data=newrankings, aes(x=Week, y=Rank, color=Team), size = 4) +   \n  geom_text(data = newrankings |&gt; filter(Week == min(Week)), aes(x = Week - .2, y=Rank, label = Team), size = 3, hjust = 1) +\n  geom_text(data = newrankings |&gt; filter(Week == max(Week)), aes(x = Week + .2, y=Rank, label = Team), size = 3, hjust = 0) +\n  labs(title=\"The top ten was anything but boring in 2024-25\", subtitle=\"\", y= \"Rank\", x = \"Week\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"none\",\n    panel.grid.major = element_blank(),\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 8), \n    plot.subtitle = element_text(size=10), \n    panel.grid.minor = element_blank()\n    ) +\n  scale_color_manual(values = c(\"HEX CODE\", \"HEX CODE\")) # replace HEX CODE with each team's code) +\n  scale_x_continuous(breaks=c(13,14,15,16,17)) + \n  scale_y_reverse(breaks=c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15))",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Bump charts</span>"
    ]
  },
  {
    "objectID": "tables.html",
    "href": "tables.html",
    "title": "26  Tables",
    "section": "",
    "text": "But not a table. A table with features.\nSometimes, the best way to show your data is with a table – simple rows and columns. It allows a reader to compare whatever they want to compare a little easier than a graph where you’ve chosen what to highlight. The folks that made R Studio and the tidyverse have a neat package called gt.\nFor this assignment, we’ll need gt so go over to the console and run:\ninstall.packages(\"gt\")\nSo what does all of these libraries do? Let’s gather a few and use data of every men’s basketball game between 2015-2025.\nFor this walkthrough:\n   Download csv file\n\nLoad libraries.\n\nlibrary(tidyverse)\nlibrary(gt)\n\nAnd the data.\n\nlogs &lt;- read_csv(\"data/logs2425.csv\")\n\nRows: 23899 Columns: 60\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (10): Season, GameType, TeamFullName, Opponent, HomeAway, W_L, OT, URL,...\ndbl  (49): Game, TeamScore, OpponentScore, TeamFG, TeamFGA, TeamFGPCT, Team3...\ndate  (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet’s ask this question: which college basketball team saw the greatest decrease in three point attempts per game between last season as a percentage of shots? The simplest way to calculate that is by percent change.\nWe’ve got a little work to do, putting together ideas we’ve used before. What we need to end up with is some data that looks like this:\nTeam | 2023-2024 season threes | 2024-2025 season threes | pct change\nTo get that, we’ll need to do some filtering to get the right seasons, some grouping and summarizing to get the right number, some pivoting to get it organized correctly so we can mutate the percent change.\n\nthreechange &lt;- logs |&gt;\n  filter(Season == \"2023-2024\" | Season == \"2024-2025\") |&gt;\n  group_by(Team, Season) |&gt;\n  summarise(Total3PA = sum(Team3PA)) |&gt;\n  pivot_wider(names_from=Season, values_from = Total3PA) |&gt;\n  filter(!is.na(`2024-2025`)) |&gt; \n  mutate(PercentChange = (`2024-2025`-`2023-2024`)/`2023-2024`) |&gt;\n  arrange(PercentChange) |&gt; \n  ungroup() |&gt;\n  slice_head(n=10) # just want a top 10 list, but can't use top_n!\n\n`summarise()` has grouped output by 'Team'. You can override using the\n`.groups` argument.\n\n\nWe’ve output tables to the screen a thousand times in this class with head, but gt makes them look decent with very little code.\n\nthreechange |&gt; gt()\n\n\n\n\n\n\n\nTeam\n2023-2024\n2024-2025\nPercentChange\n\n\n\n\nTemple\n931\n597\n-0.3587540\n\n\nTarleton State\n623\n408\n-0.3451043\n\n\nMorehead State\n912\n614\n-0.3267544\n\n\nFlorida International\n840\n585\n-0.3035714\n\n\nMilwaukee\n861\n613\n-0.2880372\n\n\nBoston College\n826\n597\n-0.2772397\n\n\nCollege of Charleston\n1072\n782\n-0.2705224\n\n\nFairfield\n943\n729\n-0.2269353\n\n\nArkansas-Pine Bluff\n898\n696\n-0.2249443\n\n\nGeorgia\n883\n692\n-0.2163080\n\n\n\n\n\n\n\nSo there you have it. Temple changed its team so much they took 35 percent fewer threes in 2024-25 from the season before. Where did Maryland come out? We ranked pretty low in college basketball in terms of fewer threes from the season before, because the Terps actually took 12% more.\ngt has a mountain of customization options. The good news is that it works in a very familiar pattern. We’ll start with fixing headers. What we have isn’t bad, but PercentChange isn’t good either. Let’s fix that.\n\nthreechange |&gt; \n  gt() |&gt; \n  cols_label(\n    PercentChange = \"Percent Change\"\n  )\n\n\n\n\n\n\n\nTeam\n2023-2024\n2024-2025\nPercent Change\n\n\n\n\nTemple\n931\n597\n-0.3587540\n\n\nTarleton State\n623\n408\n-0.3451043\n\n\nMorehead State\n912\n614\n-0.3267544\n\n\nFlorida International\n840\n585\n-0.3035714\n\n\nMilwaukee\n861\n613\n-0.2880372\n\n\nBoston College\n826\n597\n-0.2772397\n\n\nCollege of Charleston\n1072\n782\n-0.2705224\n\n\nFairfield\n943\n729\n-0.2269353\n\n\nArkansas-Pine Bluff\n898\n696\n-0.2249443\n\n\nGeorgia\n883\n692\n-0.2163080\n\n\n\n\n\n\n\nBetter. Note the pattern: Actual header name = “What we want to see”. So if we wanted to change Team to School, we’d do this: Team = \"School\" inside the cols_label bits.\nNow we can start working with styling. The truth is most of your code in tables is going to be dedicated to styling specific things. The first thing we need: A headline and some chatter. They’re required parts of a graphic, so they’re a good place to start. We do that with tab_header\n\nthreechange |&gt; \n  gt() |&gt; \n  cols_label(\n    PercentChange = \"Percent Change\"\n  ) |&gt;\n  tab_header(\n    title = \"Did Maryland Shoot Fewer Threes in 2024-25?\",\n    subtitle = \"No, the Terps shot more. But these 10 teams completely changed their offenses.\"\n  )\n\n\n\n\n\n\n\nDid Maryland Shoot Fewer Threes in 2024-25?\n\n\nNo, the Terps shot more. But these 10 teams completely changed their offenses.\n\n\nTeam\n2023-2024\n2024-2025\nPercent Change\n\n\n\n\nTemple\n931\n597\n-0.3587540\n\n\nTarleton State\n623\n408\n-0.3451043\n\n\nMorehead State\n912\n614\n-0.3267544\n\n\nFlorida International\n840\n585\n-0.3035714\n\n\nMilwaukee\n861\n613\n-0.2880372\n\n\nBoston College\n826\n597\n-0.2772397\n\n\nCollege of Charleston\n1072\n782\n-0.2705224\n\n\nFairfield\n943\n729\n-0.2269353\n\n\nArkansas-Pine Bluff\n898\n696\n-0.2249443\n\n\nGeorgia\n883\n692\n-0.2163080\n\n\n\n\n\n\n\nWe have a headline and some chatter, but … gross. Centered? The extra lines? No real difference in font weight? We can do better. We can style individual elements using tab_style. First, let’s make the main headline – the title – bold and left aligned.\n\nthreechange |&gt; \n  gt() |&gt; \n  cols_label(\n    PercentChange = \"Percent Change\"\n  ) |&gt;\n  tab_header(\n    title = \"Did Maryland Shoot Fewer Threes in 2024-25?\",\n    subtitle = \"No, the Terps shot more. But these 10 teams completely changed their offenses.\"\n  ) |&gt; tab_style(\n    style = cell_text(color = \"black\", weight = \"bold\", align = \"left\"),\n    locations = cells_title(\"title\")\n  )\n\n\n\n\n\n\n\nDid Maryland Shoot Fewer Threes in 2024-25?\n\n\nNo, the Terps shot more. But these 10 teams completely changed their offenses.\n\n\nTeam\n2023-2024\n2024-2025\nPercent Change\n\n\n\n\nTemple\n931\n597\n-0.3587540\n\n\nTarleton State\n623\n408\n-0.3451043\n\n\nMorehead State\n912\n614\n-0.3267544\n\n\nFlorida International\n840\n585\n-0.3035714\n\n\nMilwaukee\n861\n613\n-0.2880372\n\n\nBoston College\n826\n597\n-0.2772397\n\n\nCollege of Charleston\n1072\n782\n-0.2705224\n\n\nFairfield\n943\n729\n-0.2269353\n\n\nArkansas-Pine Bluff\n898\n696\n-0.2249443\n\n\nGeorgia\n883\n692\n-0.2163080\n\n\n\n\n\n\n\nIt’s hard to see here, but the chatter below is also centered (it doesn’t look like it because it fills the space). We can left align that too, but leave it normal weight (i.e. not bold).\n\nthreechange |&gt; \n  gt() |&gt; \n  cols_label(\n    PercentChange = \"Percent Change\"\n  ) |&gt;\n  tab_header(\n    title = \"Did Maryland Shoot Fewer Threes in 2024-25?\",\n    subtitle = \"No, the Terps shot more. But these 10 teams completely changed their offenses.\"\n  ) |&gt; tab_style(\n    style = cell_text(color = \"black\", weight = \"bold\", align = \"left\"),\n    locations = cells_title(\"title\")\n  ) |&gt; tab_style(\n    style = cell_text(color = \"black\", align = \"left\"),\n    locations = cells_title(\"subtitle\")\n  )\n\n\n\n\n\n\n\nDid Maryland Shoot Fewer Threes in 2024-25?\n\n\nNo, the Terps shot more. But these 10 teams completely changed their offenses.\n\n\nTeam\n2023-2024\n2024-2025\nPercent Change\n\n\n\n\nTemple\n931\n597\n-0.3587540\n\n\nTarleton State\n623\n408\n-0.3451043\n\n\nMorehead State\n912\n614\n-0.3267544\n\n\nFlorida International\n840\n585\n-0.3035714\n\n\nMilwaukee\n861\n613\n-0.2880372\n\n\nBoston College\n826\n597\n-0.2772397\n\n\nCollege of Charleston\n1072\n782\n-0.2705224\n\n\nFairfield\n943\n729\n-0.2269353\n\n\nArkansas-Pine Bluff\n898\n696\n-0.2249443\n\n\nGeorgia\n883\n692\n-0.2163080\n\n\n\n\n\n\n\nThe next item on the required elements list: Source and credit lines. In gt, those are called tab_source_notes and we can add them like this:\n\nthreechange |&gt; \n  gt() |&gt; \n  cols_label(\n    PercentChange = \"Percent Change\"\n  ) |&gt;\n  tab_header(\n    title = \"Did Maryland Shoot Fewer Threes in 2024-25?\",\n    subtitle = \"No, the Terps shot more. But these 10 teams completely changed their offenses.\"\n  ) |&gt; tab_style(\n    style = cell_text(color = \"black\", weight = \"bold\", align = \"left\"),\n    locations = cells_title(\"title\")\n  ) |&gt; tab_style(\n    style = cell_text(color = \"black\", align = \"left\"),\n    locations = cells_title(\"subtitle\")\n  ) |&gt;\n  tab_source_note(\n    source_note = md(\"**By:** Derek Willis  |  **Source:** [Sports Reference](https://www.sports-reference.com/cbb/seasons/)\")\n  )\n\n\n\n\n\n\n\nDid Maryland Shoot Fewer Threes in 2024-25?\n\n\nNo, the Terps shot more. But these 10 teams completely changed their offenses.\n\n\nTeam\n2023-2024\n2024-2025\nPercent Change\n\n\n\n\nTemple\n931\n597\n-0.3587540\n\n\nTarleton State\n623\n408\n-0.3451043\n\n\nMorehead State\n912\n614\n-0.3267544\n\n\nFlorida International\n840\n585\n-0.3035714\n\n\nMilwaukee\n861\n613\n-0.2880372\n\n\nBoston College\n826\n597\n-0.2772397\n\n\nCollege of Charleston\n1072\n782\n-0.2705224\n\n\nFairfield\n943\n729\n-0.2269353\n\n\nArkansas-Pine Bluff\n898\n696\n-0.2249443\n\n\nGeorgia\n883\n692\n-0.2163080\n\n\n\nBy: Derek Willis | Source: Sports Reference\n\n\n\n\n\n\n\n\nWe can do a lot with tab_style. For instance, we can make the headers bold and reduce the size a bit to reduce font congestion in the area.\n\nthreechange |&gt; \n  gt() |&gt; \n  cols_label(\n    PercentChange = \"Percent Change\"\n  ) |&gt;\n  tab_header(\n    title = \"Did Maryland Shoot Fewer Threes in 2024-25?\",\n    subtitle = \"No, the Terps shot more. But these 10 teams completely changed their offenses.\"\n  ) |&gt; \n  tab_style(\n    style = cell_text(color = \"black\", weight = \"bold\", align = \"left\"),\n    locations = cells_title(\"title\")\n  ) |&gt; \n  tab_style(\n    style = cell_text(color = \"black\", align = \"left\"),\n    locations = cells_title(\"subtitle\")\n  ) |&gt;  \n  tab_source_note(\n    source_note = md(\"**By:** Derek Willis  |  **Source:** [Sports Reference](https://www.sports-reference.com/cbb/seasons/)\")\n  ) |&gt;\n  tab_style(\n     locations = cells_column_labels(columns = everything()),\n     style = list(\n       cell_borders(sides = \"bottom\", weight = px(3)),\n       cell_text(weight = \"bold\", size=12)\n     )\n   ) \n\n\n\n\n\n\n\nDid Maryland Shoot Fewer Threes in 2024-25?\n\n\nNo, the Terps shot more. But these 10 teams completely changed their offenses.\n\n\nTeam\n2023-2024\n2024-2025\nPercent Change\n\n\n\n\nTemple\n931\n597\n-0.3587540\n\n\nTarleton State\n623\n408\n-0.3451043\n\n\nMorehead State\n912\n614\n-0.3267544\n\n\nFlorida International\n840\n585\n-0.3035714\n\n\nMilwaukee\n861\n613\n-0.2880372\n\n\nBoston College\n826\n597\n-0.2772397\n\n\nCollege of Charleston\n1072\n782\n-0.2705224\n\n\nFairfield\n943\n729\n-0.2269353\n\n\nArkansas-Pine Bluff\n898\n696\n-0.2249443\n\n\nGeorgia\n883\n692\n-0.2163080\n\n\n\nBy: Derek Willis | Source: Sports Reference\n\n\n\n\n\n\n\n\nNext up: There’s a lot of lines in this that don’t need to be there. gt has some tools to get rid of them easily and add in some other readability improvements.\n\nthreechange |&gt; \n  gt() |&gt; \n  cols_label(\n    PercentChange = \"Percent Change\"\n  ) |&gt;\n  tab_header(\n    title = \"Did Maryland Shoot Fewer Threes in 2024-25?\",\n    subtitle = \"No, the Terps shot more. But these 10 teams completely changed their offenses.\"\n  ) |&gt;  \n  tab_source_note(\n    source_note = md(\"**By:** Derek Willis  |  **Source:** [Sports Reference](https://www.sports-reference.com/cbb/seasons/)\")\n  ) |&gt; \n  tab_style(\n    style = cell_text(color = \"black\", weight = \"bold\", align = \"left\"),\n    locations = cells_title(\"title\")\n  ) |&gt; \n  tab_style(\n    style = cell_text(color = \"black\", align = \"left\"),\n    locations = cells_title(\"subtitle\")\n  ) |&gt;\n  tab_style(\n     locations = cells_column_labels(columns = everything()),\n     style = list(\n       cell_borders(sides = \"bottom\", weight = px(3)),\n       cell_text(weight = \"bold\", size=12)\n     )\n   ) |&gt;\n  opt_row_striping() |&gt; \n  opt_table_lines(\"none\")\n\n\n\n\n\n\n\nDid Maryland Shoot Fewer Threes in 2024-25?\n\n\nNo, the Terps shot more. But these 10 teams completely changed their offenses.\n\n\nTeam\n2023-2024\n2024-2025\nPercent Change\n\n\n\n\nTemple\n931\n597\n-0.3587540\n\n\nTarleton State\n623\n408\n-0.3451043\n\n\nMorehead State\n912\n614\n-0.3267544\n\n\nFlorida International\n840\n585\n-0.3035714\n\n\nMilwaukee\n861\n613\n-0.2880372\n\n\nBoston College\n826\n597\n-0.2772397\n\n\nCollege of Charleston\n1072\n782\n-0.2705224\n\n\nFairfield\n943\n729\n-0.2269353\n\n\nArkansas-Pine Bluff\n898\n696\n-0.2249443\n\n\nGeorgia\n883\n692\n-0.2163080\n\n\n\nBy: Derek Willis | Source: Sports Reference\n\n\n\n\n\n\n\n\nWe’re in pretty good shape here, but look closer. What else makes this table sub-par? How about the formatting of the percent change? We can fix that with a formatter.\n\nthreechange |&gt; \n  gt() |&gt; \n  cols_label(\n    PercentChange = \"Percent Change\"\n  ) |&gt;\n  tab_header(\n    title = \"Did Maryland Shoot Fewer Threes in 2024-25?\",\n    subtitle = \"No, the Terps shot more. But these 10 teams completely changed their offenses.\"\n  ) |&gt;  \n  tab_source_note(\n    source_note = md(\"**By:** Derek Willis  |  **Source:** [Sports Reference](https://www.sports-reference.com/cbb/seasons/)\")\n  ) |&gt; \n  tab_style(\n    style = cell_text(color = \"black\", weight = \"bold\", align = \"left\"),\n    locations = cells_title(\"title\")\n  ) |&gt; \n  tab_style(\n    style = cell_text(color = \"black\", align = \"left\"),\n    locations = cells_title(\"subtitle\")\n  ) |&gt;\n  tab_style(\n     locations = cells_column_labels(columns = everything()),\n     style = list(\n       cell_borders(sides = \"bottom\", weight = px(3)),\n       cell_text(weight = \"bold\", size=12)\n     )\n   ) |&gt;\n  opt_row_striping() |&gt; \n  opt_table_lines(\"none\") |&gt;\n    fmt_percent(\n    columns = c(PercentChange),\n    decimals = 1\n  )\n\n\n\n\n\n\n\nDid Maryland Shoot Fewer Threes in 2024-25?\n\n\nNo, the Terps shot more. But these 10 teams completely changed their offenses.\n\n\nTeam\n2023-2024\n2024-2025\nPercent Change\n\n\n\n\nTemple\n931\n597\n−35.9%\n\n\nTarleton State\n623\n408\n−34.5%\n\n\nMorehead State\n912\n614\n−32.7%\n\n\nFlorida International\n840\n585\n−30.4%\n\n\nMilwaukee\n861\n613\n−28.8%\n\n\nBoston College\n826\n597\n−27.7%\n\n\nCollege of Charleston\n1072\n782\n−27.1%\n\n\nFairfield\n943\n729\n−22.7%\n\n\nArkansas-Pine Bluff\n898\n696\n−22.5%\n\n\nGeorgia\n883\n692\n−21.6%\n\n\n\nBy: Derek Willis | Source: Sports Reference\n\n\n\n\n\n\n\n\nThroughout the semester, we’ve been using color and other signals to highlight things. Let’s pretend we’re doing a project on Boston College. With a little tab_style magic, we can change individual rows and add color. The last tab_style block here will first pass off the styles we want to use – we’re going to make the rows maroon and the text gold – and then for locations we specify where with a simple filter. What that means is that any rows we can address with logic – all rows with a value greater than X, for example – we can change the styling.\n\nthreechange |&gt; \n  gt() |&gt; \n  cols_label(\n    PercentChange = \"Percent Change\"\n  ) |&gt;\n  tab_header(\n    title = \"Did Maryland Shoot Fewer Threes in 2024-25?\",\n    subtitle = \"No, the Terps shot more. But these 10 teams completely changed their offenses.\"\n  ) |&gt;  \n  tab_source_note(\n    source_note = md(\"**By:** Derek Willis  |  **Source:** [Sports Reference](https://www.sports-reference.com/cbb/seasons/)\")\n  ) |&gt; \n  tab_style(\n    style = cell_text(color = \"black\", weight = \"bold\", align = \"left\"),\n    locations = cells_title(\"title\")\n  ) |&gt; \n  tab_style(\n    style = cell_text(color = \"black\", align = \"left\"),\n    locations = cells_title(\"subtitle\")\n  ) |&gt;\n  tab_style(\n     locations = cells_column_labels(columns = everything()),\n     style = list(\n       cell_borders(sides = \"bottom\", weight = px(3)),\n       cell_text(weight = \"bold\", size=12)\n     )\n   ) |&gt;\n  opt_row_striping() |&gt; \n  opt_table_lines(\"none\") |&gt;\n    fmt_percent(\n    columns = c(PercentChange),\n    decimals = 1\n  ) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = \"maroon\"),\n      cell_text(color = \"gold\")\n      ),\n    locations = cells_body(\n      rows = Team == \"Boston College\")\n  )\n\n\n\n\n\n\n\nDid Maryland Shoot Fewer Threes in 2024-25?\n\n\nNo, the Terps shot more. But these 10 teams completely changed their offenses.\n\n\nTeam\n2023-2024\n2024-2025\nPercent Change\n\n\n\n\nTemple\n931\n597\n−35.9%\n\n\nTarleton State\n623\n408\n−34.5%\n\n\nMorehead State\n912\n614\n−32.7%\n\n\nFlorida International\n840\n585\n−30.4%\n\n\nMilwaukee\n861\n613\n−28.8%\n\n\nBoston College\n826\n597\n−27.7%\n\n\nCollege of Charleston\n1072\n782\n−27.1%\n\n\nFairfield\n943\n729\n−22.7%\n\n\nArkansas-Pine Bluff\n898\n696\n−22.5%\n\n\nGeorgia\n883\n692\n−21.6%\n\n\n\nBy: Derek Willis | Source: Sports Reference\n\n\n\n\n\n\n\n\nTwo things here:\n\nDear God that color scheme is awful, which is fitting for a school that has a non-rhyming mascot.\nWe’ve arrived where we want to be: We’ve created a clear table that allows a reader to compare schools at will while also using color to draw attention to the thing we want to draw attention to. We’ve kept it simple so the color has impact.",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Tables</span>"
    ]
  },
  {
    "objectID": "facetwraps.html",
    "href": "facetwraps.html",
    "title": "27  Facet wraps",
    "section": "",
    "text": "27.1 Facet grid vs facet wraps\nSometimes the easiest way to spot a trend is to chart a bunch of small things side by side. Edward Tufte, one of the most well known data visualization thinkers on the planet, calls this “small multiples” where ggplot calls this a facet wrap or a facet grid, depending.\nOne thing we noticed earlier in the semester – it seems that a lot of teams shoot worse as the season goes on. Do they? We could answer this a number of ways, but the best way to show people would be visually. Let’s use Small Multiples.\nAs always, we start with libraries.\nWe’re going to use the logs of college basketball games last season.\nAnd load it.\nLet’s narrow our pile and look just at the Big Ten.\nThe first thing we can do is look at a line chart, like we have done in previous chapters.\nAnd, not surprisingly, we get a hairball. We could color certain lines, but that would limit us to focus on one team. What if we did all of them at once? We do that with a facet_wrap. The only thing we MUST pass into a facet_wrap is what thing we’re going to separate them out by. In this case, we precede that field with a tilde, so in our case we want the Team field. It looks like this:\nAnswer: Not immediately clear, but we can look at this and analyze it. We could add a piece of annotation to help us out: the average field goal percentage for the whole conference.\nWhat do you see here? How do teams compare? How do they change over time? I’m not asking you these questions because they’re an assignment – I’m asking because that’s exactly what this chart helps answer. Your brain will immediately start making those connections.\nFacet grids allow us to put teams on the same plane, versus just repeating them. And we can specify that plane as vertical or horizontal. For example, here’s our chart from above, but using facet_grid to stack them.\nggplot() + \n  geom_hline(yintercept=big10_avg, color=\"blue\") + \n  geom_line(data=big10, aes(x=Date, y=TeamFGPCT, group=Team)) + \n  scale_y_continuous(limits = c(0, .7)) + \n  facet_grid(Team ~ .)\nAnd here they are next to each other:\nggplot() + \n  geom_hline(yintercept=big10_avg, color=\"blue\") + \n  geom_line(data=big10, aes(x=Date, y=TeamFGPCT, group=Team)) + \n  scale_y_continuous(limits = c(0, .7)) + \n  facet_grid(. ~ Team)\nNote: We’d have some work to do with the labeling on this – we’ll get to that – but you can see where this is valuable comparing a group of things. One warning: Don’t go too crazy with this or it loses it’s visual power.",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Facet wraps</span>"
    ]
  },
  {
    "objectID": "facetwraps.html#other-types",
    "href": "facetwraps.html#other-types",
    "title": "27  Facet wraps",
    "section": "27.2 Other types",
    "text": "27.2 Other types\nLine charts aren’t the only things we can do. We can do any kind of chart in ggplot. Staying with shooting, where are team’s winning and losing performances coming from when we talk about team shooting and opponent shooting?\n\nggplot() + \n  geom_point(data=big10, aes(x=TeamFGPCT, y=OpponentFGPCT, color=W_L)) +\n  scale_y_continuous(limits = c(0, .7)) + \n  scale_x_continuous(limits = c(0, .7)) + \n  facet_wrap(~Team)",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Facet wraps</span>"
    ]
  },
  {
    "objectID": "cowplots.html",
    "href": "cowplots.html",
    "title": "28  Arranging multiple plots together",
    "section": "",
    "text": "Sometimes you have two or three (or more) charts that by themselves aren’t very exciting and are really just one chart that you need to merge together. It would be nice to be able to arrange them programmatically and not have to mess with it in Adobe Illustrator.\nGood news.\nThere is.\nIt’s called cowplot, and it’s pretty easy to use. First install cowplot with install.packages(\"cowplot\"). Then let’s load tidyverse and cowplot.\n\nlibrary(tidyverse)\nlibrary(cowplot)\n\nWe’ll use the college football attendance data we’ve used before.\nFor this walkthrough:\n   Download csv file\n\nAnd load it.\n\nattendance &lt;- read_csv(\"data/attendance.csv\")\n\nRows: 146 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): Institution, Conference\ndbl (12): 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nMaking a quick percent change.\n\nattendance &lt;- attendance |&gt; mutate(change = ((`2024`-`2023`)/`2023`)*100)\n\nLet’s chart the top 10 and bottom 10 of college football ticket growth … and shrinkage.\n\ntop10 &lt;- attendance |&gt; top_n(10, wt=change) |&gt; arrange(desc(change))\nbottom10 &lt;- attendance |&gt; top_n(10, wt=-change) |&gt; arrange(change)\n\nOhio State is benefiting from the extended playoffs, but hello, SMU and Indiana!\nOkay, now to do this I need to save my plots to an object. We do this the same way we save things to a dataframe – with the arrow. We’ll make two identical bar charts, one with the top 10 and one with the bottom 10.\n\nbar1 &lt;- ggplot() + \n  geom_bar(data=top10, aes(x=reorder(Institution, change), weight=change)) +\n  coord_flip()\n\n\nbar2 &lt;- ggplot() + \n  geom_bar(data=bottom10, aes(x=reorder(Institution, change), weight=change)) +\n  coord_flip()\n\nWith cowplot, we can use a function called plot_grid to arrange the charts:\n\nplot_grid(bar1, bar2) \n\n\n\n\n\n\n\n\nWe can also stack them on top of each other:\n\nplot_grid(bar1, bar2, ncol=1) \n\n\n\n\n\n\n\n\nTo make these publishable, we should add headlines, chatter, decent labels, credit lines, etc. But to do this, we’ll have to figure out which labels go on which charts, so we can make it look decent. For example – both charts don’t need x or y labels. If you don’t have a title and subtitle on both, the spacing is off, so you need to leave one blank or the other blank. You’ll just have to fiddle with it until you get it looking right.\n\nbar1 &lt;- ggplot() + \n  geom_bar(data=top10, aes(x=reorder(Institution, change), weight=change)) +\n  coord_flip() + \n  labs(title=\"College football winners...\", subtitle = \"Not every football program saw attendance shrink in 2024. But some really did.\",  x=\"\", y=\"Percent change\", caption = \"\") + \n  theme_minimal() + \n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 8), \n    plot.subtitle = element_text(size=10), \n    panel.grid.minor = element_blank()\n    )\n\n\nbar2 &lt;- ggplot() + \n  geom_bar(data=bottom10, aes(x=reorder(Institution, change), weight=change)) +\n  coord_flip() +  \n  labs(title = \"... and losers\", subtitle= \"\", x=\"\", y=\"\",  caption=\"Source: NCAA | By Derek Willis\") + \n  theme_minimal() + \n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 8), \n    plot.subtitle = element_text(size=10), \n    panel.grid.minor = element_blank()\n    )\n\n\nplot_grid(bar1, bar2) \n\n\n\n\n\n\n\n\nWhat’s missing here? Color. Our eyes aren’t drawn to anything (except maybe the top and bottom). So we need to help that. A bar chart without context or color to draw attention to something isn’t much of a bar chart. Same with a line chart – if your line chart has one line, no context, no color, it’s going to fare poorly.\n\nnw &lt;- bottom10 |&gt; filter(Institution == \"Northwestern\")\nsm &lt;- top10 |&gt; filter(Institution == \"SMU\")\n\n\nbar1 &lt;- ggplot() + \n  geom_bar(data=top10, aes(x=reorder(Institution, change), weight=change)) + \n  geom_bar(data=sm, aes(x=reorder(Institution, change), weight=change), fill=\"blue\") + \n  coord_flip() + \n  labs(title=\"College football winners...\", subtitle = \"Not every football program saw attendance shrink in 2023 But some really did.\",  x=\"\", y=\"Percent change\", caption = \"\") +\n  theme_minimal() + \n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 8), \n    plot.subtitle = element_text(size=10), \n    panel.grid.minor = element_blank()\n    )\n\n\nbar2 &lt;- ggplot() + \n  geom_bar(data=bottom10, aes(x=reorder(Institution, change), weight=change)) + \n  geom_bar(data=nw, aes(x=reorder(Institution, change), weight=change), fill=\"purple\") + \n  coord_flip() +  \n  labs(title = \"... and losers\", subtitle= \"\", x=\"\", y=\"\",  caption=\"Source: NCAA | By Derek Willis\") + \n  theme_minimal() + \n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 8), \n    plot.subtitle = element_text(size=10), \n    panel.grid.minor = element_blank()\n    )\n\n\nplot_grid(bar1, bar2)",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Arranging multiple plots together</span>"
    ]
  },
  {
    "objectID": "encirclingpoints.html",
    "href": "encirclingpoints.html",
    "title": "29  Encircling points on a scatterplot",
    "section": "",
    "text": "29.1 A different, more local example\nOne thing we’ve talked about all semester is drawing attention to the thing you want to draw attention to. We’ve used color and labels to do that so far. Let’s add another layer to it – a shape around the points you want to highlight.\nRemember: The point of all of this is to draw the eye to what you are trying to show your reader. You want people to see the story you are trying to tell.\nIt’s not hard to draw a shape in ggplot – it is a challenge to put it in the right place. But, there is a library to the rescue that makes this super easy – ggalt.\nInstall it in the console with install.packages(\"ggalt\")\nThere’s a bunch of things that ggalt does, but one of the most useful for us is the function encircle. Let’s dive in.\nLet’s say we want to highlight the top scorers in women’s college lacrosse. So let’s use our player data.\nAnd while we’re loading it, let’s filter out anyone who hasn’t played.\nWe’ve done this before, but let’s make a standard scatterplot of games and points.\nSo we can see right away that there are some dots at the very top that we’d want to highlight. Who are these scoring machines?\nLike we have done in the past, let’s make a dataframe of top scorers. We’ll set the cutoff at 120 points.\nAnd like we’ve done in the past, we can add it to the chart with another geom_point. We’ll make all the players grey, we’ll make all the top scorers black.\nAnd like that, we’re on the path to something publishable. We’ll need to label those dots with ggrepel and we’ll need to drop the default grey and add some headlines and all that. And, for the most part, we’ve got a solid chart.\nBut what if we could really draw the eye to those players. Let’s draw a circle around them. In ggalt, there is a new geom called geom_encircle, which … does what you think it does. It encircles all the dots in a dataset.\nSo let’s add geom_encircle and we’ll just copy the data and the aes from our topscorers geom_point. Then, we need to give the encirclement a shape using s_shape – which is a number between 0 and 1 – and then how far away from the dots to draw the circle using expand, which is another number between 0 and 1.\nLet’s start with s_shape 1 and expand 1.\nWhoa. That’s … not good.\nLet’s go the opposite direction.\nBetter, but … the circle cuts through multiple dots.\nThis takes a little bit of finessing, but a shape of .5 means the line will have some bend to it – it’ll look more like someone circled it with a pen. Then, the expand is better if you use hundredths instead of tenths. So .01 instead of .1. Here’s mine after fiddling with it for a bit, and I’ll add in player names as a label, use geom_text_repel to make some space for the names and extend the x-axis for more room.\nNow let’s clean this up and make it presentable. If you look at the top scorers, all four are guards. So here’s what a chart telling that story might look like.\nYou can use circling outside of the top of something. It’s a bit obvious that the previous dots were top scorers. What about when they aren’t at the top?\nWorks the same way – use layering and color smartly and tell the story with all your tools.\nLet’s grab the top three point attempt takers on the 2024-25 Maryland roster.\nplayers24 &lt;- read_csv(\"https://thescoop.org/sports-data-files/players25.csv\")\n\nRows: 5818 Columns: 57\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (10): Team, Player, Class, Height, Hometown, High School, Summary, Pos, ...\ndbl (47): #, Weight, G, GS, MP, FG, FGA, FG%, 3P, 3PA, 3P%, 2P, 2PA, 2P%, eF...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmdtop &lt;- players24 |&gt; filter(Team == \"Maryland Terrapins Men's\") |&gt; top_n(3, `3PA`)\nAnd just like above, we can plug in our players geom, our mdtop dataframe into another geom, then encircle that dataframe. Slap some headlines and annotations on it and here’s what we get:\nggplot() + \n  geom_point(data=players24, aes(x=MP, y=`3PA`), color=\"grey\") + \n  geom_point(data=mdtop, aes(x=MP, y=`3PA`), color=\"red\") + \n  geom_encircle(data=mdtop, aes(x=MP, y=`3PA`), s_shape=.02, expand=.10, colour=\"red\") +\n  geom_text_repel(data=mdtop, aes(x=MP, y=`3PA`, label=Player)) +\n  geom_text(aes(x=400, y=200, label=\"Maryland's top three shooters\")) + \n  labs(title=\"Terp Task: Replace the Big Three\", subtitle=\"None of Maryland's most prolific long-range shooters return this season\", x=\"Minutes\", y=\"Three point attempts\") + \n  theme_minimal() + \n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 8), \n    plot.subtitle = element_text(size=10), \n    panel.grid.minor = element_blank()\n    )\n\nWarning: Removed 929 rows containing missing values or values outside the scale range\n(`geom_point()`).\nNone of them return this season.",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Encircling points on a scatterplot</span>"
    ]
  },
  {
    "objectID": "usingpackages.html",
    "href": "usingpackages.html",
    "title": "38  Using packages to get data",
    "section": "",
    "text": "38.1 Using cfbfastR as a cautionary tale\nThere is a growing number of packages and repositories of sports data, largely because there’s a growing number of people who want to analyze that data. We’ve done it ourselves with simple Google Sheets tricks. Then there’s RVest, which is a method of scraping the data yourself from websites. But with these packages, someone has done the work of gathering the data for you. All you have to learn are the commands to get it.\nOne very promising collection of libraries is something called the SportsDataverse, which has a collection of packages covering specific sports, all of which are in various stages of development. Some are more complete than others, but they are all being actively worked on by developers. Packages of interest in this class are:\nNot part of the SportsDataverse, but in the same neighborhood, is nflfastR, which can provide NFL play-by-play data.\nBecause they’re all under development, not all of them can be installed with just a simple install.packages(\"something\"). Some require a little work, some require API keys.\nThe main issue for you is to read the documentation carefully.\ncfbfastR presents us a good view into the promise and peril of libraries like this.\nFirst, to make this work, follow the installation instructions and then follow how to get an API key from College Football Data and how to add that to your environment. But maybe wait to do that until you read the whole section.\nAfter installations, we can load it up.\nlibrary(tidyverse)\nlibrary(cfbfastR)\nYou might be thinking, “Oh wow, I can get play by play data for college football. Let’s look at what are the five most heartbreaking plays of last year’s Maryland season.” Because what better way to determine doom than by looking at the steepest drop-off in win probability, which is included in the data.\nGreat idea. Let’s do it. You’ll need to make sure that your API key has been added to your environment.\nThe first thing to do is read the documentation. You’ll see that you can request data for each week. For example, here’s week 1 against Buffalo.\nmaryland &lt;- cfbd_pbp_data(\n 2024,\n  week=1, \n  season_type = \"regular\",\n  team = \"Maryland\",\n  epa_wpa = TRUE,\n)\nThere’s not an easy way to get all of a single team’s games. A way to do it that’s not very pretty but it works is like this:\nwk1 &lt;- cfbd_pbp_data(2022, week=1, season_type = \"regular\", team = \"Maryland\", epa_wpa = TRUE)\nSys.sleep(2)\nwk2 &lt;- cfbd_pbp_data(2022, week=2, season_type = \"regular\", team = \"Maryland\", epa_wpa = TRUE)\nSys.sleep(2)\nwk3 &lt;- cfbd_pbp_data(2022, week=3, season_type = \"regular\", team = \"Maryland\", epa_wpa = TRUE)\nSys.sleep(2)\nwk4 &lt;- cfbd_pbp_data(2022, week=4, season_type = \"regular\", team = \"Maryland\", epa_wpa = TRUE)\nSys.sleep(2)\nwk5 &lt;- cfbd_pbp_data(2022, week=5, season_type = \"regular\", team = \"Maryland\", epa_wpa = TRUE)\nSys.sleep(2)\nwk6 &lt;- cfbd_pbp_data(2022, week=6, season_type = \"regular\", team = \"Maryland\", epa_wpa = TRUE)\nSys.sleep(2)\nwk8 &lt;- cfbd_pbp_data(2022, week=8, season_type = \"regular\", team = \"Maryland\", epa_wpa = TRUE)\nSys.sleep(2)\nwk9 &lt;- cfbd_pbp_data(2022, week=9, season_type = \"regular\", team = \"Maryland\", epa_wpa = TRUE)\nSys.sleep(2)\nwk10 &lt;- cfbd_pbp_data(2022, week=10, season_type = \"regular\", team = \"Maryland\", epa_wpa = TRUE)\nSys.sleep(2)\nwk11 &lt;- cfbd_pbp_data(2022, week=11, season_type = \"regular\", team = \"Maryland\", epa_wpa = TRUE)\nSys.sleep(2)\nwk12 &lt;- cfbd_pbp_data(2022, week=12, season_type = \"regular\", team = \"Maryland\", epa_wpa = TRUE)\n\numplays &lt;- bind_rows(wk1, wk2, wk3, wk4, wk5, wk6, wk8, wk9, wk10, wk11, wk12)\nThe sys.sleep bits just pauses for two seconds before running the next block. Since we’re requesting data from someone else’s computer, we want to be kind. Week 2 was a bye week for Maryland, so if you request it, you’ll get an empty request and a warning. The bind_rows parts puts all the dataframes into a single dataframe.\nNow you’re ready to look at heartbreak. How do we define heartbreak? How about like this: you first have to lose the game, it comes in the third or fourth quarter, it involves a play (i.e. not a timeout), and it results in the biggest drop in win probability.\numplays |&gt; \n  filter(pos_team == \"Maryland\" & wk &gt; 4 & play_type != \"Timeout\") |&gt; \n  filter(period == 3 | period == 4) |&gt; \n  mutate(HeartbreakLevel = wp_before - wp_after) |&gt; \n  arrange(desc(HeartbreakLevel)) |&gt; \n  top_n(5, wt=HeartbreakLevel) |&gt;\n  select(period, clock.minutes, def_pos_team, play_type, play_text)\nThe most heartbreaking play of the season, according to our data? A third quarter run for two yards against Northwestern. Hmm - Maryland won that game, though. The other top plays - mostly against Purdue and a blocked punt by Ohio State - seem more in line with what we want.",
    "crumbs": [
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Using packages to get data</span>"
    ]
  },
  {
    "objectID": "usingpackages.html#another-example",
    "href": "usingpackages.html#another-example",
    "title": "38  Using packages to get data",
    "section": "38.2 Another example",
    "text": "38.2 Another example\nThe wehoop package is mature enough to have a version on CRAN, so you can install it the usual way with install.packages(\"wehoop\"). Another helpful library to install is progressr with install.packages(\"progressr\")\n\nlibrary(wehoop)\n\nMany of these libraries have more than play-by-play data. For example, wehoop has box scores and player data for both the WNBA and college basketball. From personal experience, WNBA data isn’t hard to get, but women’s college basketball is a giant pain.\nSo, who is Maryland’s single season points champion over the last six seasons?\n\nprogressr::with_progress({\n  wbb_player_box &lt;- wehoop::load_wbb_player_box(2021:2025)\n})\n\nWith progressr, you’ll see a progress bar in the console, which lets you know that your command is still working, since some of these requests take minutes to complete. Player box scores is quicker – five seasons was a matter of seconds.\nIf you look at the wbb_player_box data we now have, we have each player in each game over each season – more than 300,000 records. Finding out who Maryland’s top 10 single-season scoring leaders are is a matter of grouping, summing and filtering.\n\nwbb_player_box |&gt; \n  filter(team_short_display_name == \"Maryland\", !is.na(points)) |&gt; \n  group_by(athlete_display_name, season) |&gt; \n  summarise(totalPoints = sum(as.numeric(points))) |&gt; \n  arrange(desc(totalPoints)) |&gt;\n  ungroup() |&gt;\n  top_n(10, wt=totalPoints)\n\nMaryland relied on Diamond Miller’s scoring last year more than they have any player’s in the past six seasons.",
    "crumbs": [
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Using packages to get data</span>"
    ]
  }
]